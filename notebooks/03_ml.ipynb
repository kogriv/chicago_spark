{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ac4a7a-4644-45d3-8786-944ff88f222a",
   "metadata": {},
   "source": [
    "Ноутбук по проекту chicago_spark.  \n",
    "МО"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de64cacf-4b21-4fc7-85ca-0d7b7de73885",
   "metadata": {},
   "source": [
    "## Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f9b287-4613-4f96-9125-2472424b9cfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c57728-71d4-43e9-a453-c5d23d6a57a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.signal import welch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbb9443-57d6-4aab-ad7d-c67719e98e03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from itertools import islice\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d04534c3-1092-4cb4-9ed0-2a6b99acbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c506d768-519f-49a0-9af6-b44cbcf036bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enviserv.dictan import DictAnalyzer # анализ словарей\n",
    "import pandserv as pds # форматирование небольших пандас ДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c22c0b8-1ea3-417e-8289-34ca57326d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkserv import SparkApp, Cols\n",
    "# в SparkApp упакованы функции создания спарк приложения \n",
    "# с определением IP мастер-ноды и с подключением к кластеру\n",
    "\n",
    "# Col - класс для формирования коротких псевдонимов имен столбцов\n",
    "# при этом исходные имена полей не меняются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63596c0d-9087-4e1a-80e3-a1fd7a90babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import DataFrame as pydf\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db2ec96-2997-458c-b67a-f9e5dd6aa586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# гео библиотеки\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e47d77-cebf-4472-99e9-c2e852867df5",
   "metadata": {},
   "source": [
    "## Создание сессии, загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c292a0-9c16-44d8-8aa6-e5a51b5e08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:spark_master_ip: 172.18.0.2\n",
      "INFO:spark_app:pyspark version: 3.4.1\n",
      "INFO:spark_app:starting building spark app object: pyspark-taxi-forecasting\n",
      "INFO:spark_app:Spark app object built as: <pyspark.sql.session.SparkSession object at 0x7f0948b8f3d0>\n",
      "INFO:spark_app:==================================================================\n",
      "INFO:spark_app:Spark object can be accessed as the SparkApp_object.spark property\n",
      "INFO:spark_app:==================================================================\n"
     ]
    }
   ],
   "source": [
    "spark_app = SparkApp(my_logger_create_level = 'INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4de973af-73c4-43f8-90b6-b1d422a3360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:spark_master_ip: 172.18.0.2\n"
     ]
    }
   ],
   "source": [
    "spark_master_ip = spark_app.get_spark_master_ip()\n",
    "# print(spark_master_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55a91437-60e9-4bc2-a9cb-58365f000efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = spark_app.build_spark_app(spark_master_ip=spark_master_ip)\n",
    "# spark = spark_app.spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c7d30-9eed-479c-b899-2fcd6c2d78d4",
   "metadata": {},
   "source": [
    "Для корректного завершения спарк-сессии (например, для переключения между ноутбуками) следует останавливать сессию полностью. Для этого использую метод .stop_spark_app() класса SparkApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "412582e5-44f8-44dd-a8f0-7d9fe46ad1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_app.stop_spark_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b17caf8-7f00-4c20-a83b-744c2d0ab81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f0948b8f3d0>\n"
     ]
    }
   ],
   "source": [
    "print(spark_app.spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5221d404-34f2-4fd8-8b0c-531fd6f93bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:pyspark version: 3.4.1\n",
      "INFO:spark_app:starting building spark app object: pyspark-taxi-forecasting\n",
      "INFO:spark_app:Spark app object built as: <pyspark.sql.session.SparkSession object at 0x7f0948b8f3d0>\n",
      "INFO:spark_app:==================================================================\n",
      "INFO:spark_app:Spark object can be accessed as the SparkApp_object.spark property\n",
      "INFO:spark_app:==================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d10fb5fe9c53:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://172.18.0.2:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-taxi-forecasting</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0948b8f3d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_app.build_spark_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab1b14-51a4-4d05-9ada-4217282f13ce",
   "metadata": {},
   "source": [
    "Получим стандартный объект `spark` из созданного выше объекта `spark_app`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56cdf778-5944-484e-a72f-6e151b2ec3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = spark_app.spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca259b2f-ac8d-4de8-800b-754588a3963c",
   "metadata": {},
   "source": [
    "Проверка работы спарк-объекта на кластере. Если все в порядке, то тест должен выполниться достаточно быстро и отобразить тестовый ДФ.  \n",
    "```txt\n",
    "+------------+-----------+\n",
    "|student_name|student_age|\n",
    "+------------+-----------+\n",
    "|       Alice|         10|\n",
    "|         Bob|         20|\n",
    "+------------+-----------+\n",
    "```\n",
    "\n",
    "Если исходные образы кластера собраны с ошибкой, возможно \"зависание\" работы теста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65e12fff-3cf5-4918-b8f7-d4ce0d306949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n",
      "DataFrame created successfully.\n",
      "Alias DataFrame created successfully.\n",
      "DataFrame data matches expected result.\n",
      "+------------+-----------+\n",
      "|student_name|student_age|\n",
      "+------------+-----------+\n",
      "|       Alice|         10|\n",
      "|         Bob|         20|\n",
      "+------------+-----------+\n",
      "\n",
      "DataFrame show output matches expected output.\n",
      "\n",
      "=======use======================\n",
      "*      ____              __    *\n",
      "*     / __/__  ___ _____/ /__  *\n",
      "*    _\\ \\/ _ \\/ _ `/ __/  '_/  *\n",
      "*   /__ / .__/\\_,_/_/ /_/\\_\\   *\n",
      "*      /_/                     * \n",
      "================================\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "spark_app.test_spark_functionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd529952-7b42-4dff-bf7b-d4a3d304ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = DictAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a49c4e5f-8973-485c-8714-94448bbe1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция простой рандомизированной выборки\n",
    "def random_sample_dataframe(dataframe, percentage):\n",
    "    # Генерируем случайные числа от 0 до 1 и фильтруем строки\n",
    "    df = dataframe.filter(f.rand() < percentage)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62ddc595-f4f4-4033-b890-cf42b5c3d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# agg_hour.coalesce(1).write.csv(\"/work/data/taxis_agg_hour_growth.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c59bfe-cbcc-4a69-8cb4-c6931b61b50a",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb679a8d-5f92-49ab-80b2-d3a76792f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# agg_hour = spark.read.load('/work/data/taxis_agg_hour_growth.csv', \n",
    "#                        format='csv', header='true'\n",
    "#                         , inferSchema='true'\n",
    "#                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df4f4d57-25f7-4c70-976f-fd4452ca779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_hour.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9d40cea-0b3e-4396-b995-928bfc94e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_hour.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ade35b3b-acac-452e-ac1c-82be92d46759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_null = agg_hour.select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in agg_hour.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb0aa96f-4e05-4dab-83ea-23dabb49fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_null = agg_null.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62603269-3dbf-45c6-a9ec-7ec4934cf033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_null[agg_null.iloc[:, 0] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7569a36-ee74-487e-b86b-438e5356fdb2",
   "metadata": {},
   "source": [
    "Загрузим данные по исключаемым (высокоррелированным) полям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bdae2d7-8b16-4707-966d-6b83e15c9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_f = pd.read_csv('/work/data/excluded_fields.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3db3af7-e825-4568-b711-a9fd2434cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для преобразования DataFrame в словарь\n",
    "def get_excuded_fields_to_dict(df):\n",
    "    excluded_fields_tot = {}\n",
    "    grouped = df.groupby(['ct', 'ca'])\n",
    "    for (ct, ca), group in grouped:\n",
    "        excluded_fields_tot[(ct, ca)] = group['excluded_field'].tolist()\n",
    "    return excluded_fields_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f8afe7e-4736-430c-a597-71eb61c6a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_fields_tot = get_excuded_fields_to_dict(excl_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a974c60-8161-4c21-9f67-438d203bca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    (10000000000, 91): ['time_p', 'miles_p', 'farem_p', 'comp1_p', 'comp3_p', 'comp4_p', 'comp5_p', 'extrasm_p']\n",
      "    (12000000001, 53): ['time_p', 'farem_p', 'trips_d', 'time_d', 'miles_d', 'farem_d', 'totalm_p', 'velocity_p_growth_3_to_2']\n",
      "    (12000000002, 75): ['time_p', 'miles_p', 'farem_p', 'tipsm_p', 'comp5_p', 'trips_d', 'time_d', 'miles_d', 'farem_d', 'tipsm_d', 'comp4_p']\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "da.print_dict(dict(islice(excluded_fields_tot.items(), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab429d-b219-4826-aab2-bbcaee4526ed",
   "metadata": {},
   "source": [
    "Будем считать, что от мультиколлинеарности в линейных моделях с помощью исключения этих полей получится избавиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f44b76c1-cfab-4115-a7ce-d840d7bd9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# agg_hour.coalesce(1).write.csv(\"/work/data/taxis_agg_hour_result.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f705c08-5874-478d-a978-3646ba2e282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 ms, sys: 9.13 ms, total: 22.5 ms\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = spark.read.load('/work/data/taxis_agg_hour_result.csv', \n",
    "                       format='csv', header='true'\n",
    "                        , inferSchema='true'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ced2355-4769-4ff4-98e3-4437a9fa7d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n"
     ]
    }
   ],
   "source": [
    "print(pds.gvf(data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "296b0a00-02cb-4a6c-8fe7-44a58ed69f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ct: bigint, ca: int, hour_start: timestamp, time_p: int, miles_p: double, velocity_p: double, farem_p: double, tipsm_p: double, tollsm_p: double, extrasm_p: double, totalm_p: double, comp1_p: int, comp2_p: int, comp3_p: int, comp4_p: int, comp5_p: int, compless5_p: int, trips_d: int, time_d: int, miles_d: double, velocity_d: double, farem_d: double, tipsm_d: double, tollsm_d: double, extrasm_d: double, totalm_d: double, comp1_d: int, comp2_d: int, comp3_d: int, comp4_d: int, comp5_d: int, compless5_d: int, cumulative_balance: int, trips_p_growth_1_to_0: double, trips_p_growth_2_to_1: double, trips_p_growth_3_to_2: double, trips_p_growth_4_to_3: double, trips_d_growth_1_to_0: double, trips_d_growth_2_to_1: double, trips_d_growth_3_to_2: double, trips_d_growth_4_to_3: double, velocity_p_growth_1_to_0: double, velocity_p_growth_2_to_1: double, velocity_p_growth_3_to_2: double, velocity_p_growth_4_to_3: double, velocity_d_growth_1_to_0: double, velocity_d_growth_2_to_1: double, velocity_d_growth_3_to_2: double, velocity_d_growth_4_to_3: double, trips_sh_168: int, trips_sh_84: int, trips_sh_24: int, trips_sh_28: int, trips_sh_12: int, trips_sh_8: int, trips_ma_168: double, trips_ma_84: double, trips_ma_24: double, trips_ma_28: double, trips_ma_12: double, trips_ma_8: double, trips_sh_4: int, trips_ma_4: double, trips_sh_1: int, trips_ma_1: double, trips_ma_168_growth: double, trips_ma_8_growth: double, trips_ma_4_growth: double, trips_target: int]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663d6b9-90ea-4f36-aea7-1520d63a0d8c",
   "metadata": {},
   "source": [
    "### Фрагмент данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699374f-3d4b-4d28-b2fb-d9ddca03138c",
   "metadata": {},
   "source": [
    "Отберем несколько полей и один район для проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08662517-2f13-4728-bfde-a27c48d289a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-------------------+------------+------+-------+--------------------+\n",
      "|         ct| ca|         hour_start|trips_target|time_p|miles_p|          velocity_p|\n",
      "+-----------+---+-------------------+------------+------+-------+--------------------+\n",
      "|17031090200|  6|2021-01-08 01:00:00|           1|     0|    0.0|                 0.0|\n",
      "|17031090200|  6|2021-01-08 02:00:00|           2|   706|   3.24|0.004589235127478754|\n",
      "|17031090200|  6|2021-01-08 03:00:00|           1|   990|    2.9|0.002929292929292929|\n",
      "+-----------+---+-------------------+------------+------+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_to_sel = [\n",
    "    'ct', 'ca', 'hour_start', 'trips_target', 'time_p', 'miles_p', 'velocity_p',\n",
    "]\n",
    "\n",
    "data_sample = data.select(*f_to_sel).filter(f.col('ct')==17031090200)\n",
    "data_sample.show(3)\n",
    "# data_sample.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6b49f0f-b5ee-4c40-ad16-d5d0a279f357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time_p miles_p farem_p tipsm_p comp1_p comp3_p comp4_p trips_d time_d miles_d farem_d tipsm_d extrasm_d comp2_d comp3_d comp4_d extrasm_p comp5_p'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(excluded_fields_tot[(17031090200,6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b28493-6584-4311-bdec-5b8465c2c279",
   "metadata": {},
   "source": [
    "Данные загрузились. Общее количество и отображение фрагмента соответствуют ожиданиям. Исключаемые поля в доступе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38256c91-3952-4833-8844-c0477848f371",
   "metadata": {},
   "source": [
    "### Инвентаризация объектов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08556d06-b00a-405c-9e18-2d0f1440ac3f",
   "metadata": {},
   "source": [
    "Датафреймы pyspark, если они были зарегестрированы в качестве представлений (таблиц), можно молучить используя catalog. Но если это просто объекты Питон типа pyspark.sql.dataframe.DataFrame, такие дф надо искать среди всех объектов в глобальной области видимости программы. Воспользуемся написанной функцией. При этом возможна путаница, связанная с \"магической\" служебной переменной Юпитера с именем \"__\" (последнее сохраненное значение ячейки). Такое имя может заменить реальное имя объекта. Решается двухкратным вызовом, например, `type(__)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a7c4ca1-18b9-4d14-a5ea-92a0da9f29bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "type(__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1741727f-02a6-4134-8afd-755a47201ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca4b95f1-be16-4b0b-82fc-035a4decb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_namespace=globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba9a1eb4-d00d-469b-8163-eeee1ad4b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(pds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd0f5a-d7cc-488c-babd-ffefe4454f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = pds.inventory_objects(globals())\n",
    "\n",
    "ob_df = ob[ob['Type'].str.contains('DataFrame')]\n",
    "\n",
    "ob_df = pds.get_df_formated(ob_df,\"'\",0,40)\n",
    "\n",
    "print(ob_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "62f6249b-5ea6-4fb6-b382-932e87281931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_df_list(globe_dict):\n",
    "    objects = pds.inventory_objects(globe_dict)\n",
    "    spark_df = objects[objects['Type'].str.contains('Spark DataFrame')].copy()\n",
    "    del objects\n",
    "    spark_df['cached'] = spark_df.apply(lambda x: eval(x['Name']).is_cached \\\n",
    "            if not (x['Name'].startswith(\"DataFrame[\") or x['Name'].startswith(\"_\")) else None, axis=1)\n",
    "    spark_df['cache_level'] = spark_df.apply(lambda x: eval(x['Name']).storageLevel \\\n",
    "            if not (x['Name'].startswith(\"DataFrame[\") or x['Name'].startswith(\"_\")) else None, axis=1)\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "36df8f19-91f8-4db9-852e-3762024ec909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a DataFrame containing the names, types, and sizes of objects...\n",
      "objects count: 425'065\n",
      "total size: 161'529'395\n"
     ]
    }
   ],
   "source": [
    "spark_df = get_spark_df_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1597403e-6ba9-4d6a-aa17-06fcaf51da2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size (bytes)</th>\n",
       "      <th>cached</th>\n",
       "      <th>cache_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319118</th>\n",
       "      <td>test</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "      <td>Disk Memory Deserialized 1x Replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319119</th>\n",
       "      <td>train</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "      <td>Disk Memory Deserialized 1x Replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320996</th>\n",
       "      <td>data</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "      <td>Disk Memory Deserialized 1x Replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321043</th>\n",
       "      <td>data_sample</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "      <td>Serialized 1x Replicated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name             Type  Size (bytes)  cached  \\\n",
       "319118         test  Spark DataFrame            56    True   \n",
       "319119        train  Spark DataFrame            56    True   \n",
       "320996         data  Spark DataFrame            56    True   \n",
       "321043  data_sample  Spark DataFrame            56   False   \n",
       "\n",
       "                                   cache_level  \n",
       "319118  Disk Memory Deserialized 1x Replicated  \n",
       "319119  Disk Memory Deserialized 1x Replicated  \n",
       "320996  Disk Memory Deserialized 1x Replicated  \n",
       "321043                Serialized 1x Replicated  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e27b1-a765-43c4-9406-06b99c337e15",
   "metadata": {},
   "source": [
    "## Порядок действий МО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc2d5694-b4e2-429c-a714-36769278f089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n",
      "CPU times: user 13 ms, sys: 950 µs, total: 14 ms\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pds.gvf(data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7437836-0bec-433c-b814-482e9b205b49",
   "metadata": {},
   "source": [
    "Общее описание порядка действи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ecaa84-bc2b-4840-9df5-d777cf6f62e5",
   "metadata": {},
   "source": [
    "Некоторая информация связанная с векторизацией признаков.  \n",
    "Сохранить датафрейм со столбцом вектором возможно, например, в формате parquet; csv не поддерживает вектор-столбцы. Загруженный паркет-файл вызывает ошибку при выполнении действий.   \n",
    "При кэшировании векторизированного дф (с использованием уровня хранения Disk Memory Deserialized 1x Replicated - уровень, который присваивается по умолчанию при вызове .cache()), такой кэшированный датафрейм с вектор-столбцом не получается обрабатывать из-за нехватки памяти.  \n",
    "\n",
    "Пока остановился на варианте:  \n",
    "- 1. делю входной дф на трэйн и тест, кэширую их .cache() (Disk Memory)\n",
    "- 2. получаю фрагмент geo_df_base_train/test для гео-ключа из трэйн дф, кэширую в ОЗУ geo_df_base_train/test\n",
    "- 3. векторизирую фичи geo_df_base_train/test,\n",
    "- 4. создаю рабочий дф geo_df_train/test дф с нужными полями (гео-ключ, время, метка, фича-вектор)\n",
    "- 5. кэширую в ОЗУ geo_df_train/test;\n",
    "- 6. создаю для geo_df_train скалер\n",
    "- 7. масштабирую-обновляю geo_df_train/test, кэширую в ОЗУ,\n",
    "- 8. обучаю модель лин.регрессии на трэйне (включая тюнинг модели)\n",
    "- 9. сохраняю в пандас дф для данного гео-ключа полученный объект-модель и метрики на трэйне\n",
    "- 10. получаю предикт на трэйне и тесте и добавляю столбец предикта в исходные train/test по гео-ключу, обновляю их и кэширую .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c6200-2ea6-420a-bee6-04a7ee7d861a",
   "metadata": {},
   "source": [
    "### 1. делю входной дф на трэйн и тест, кэширую их .cache() (Disk Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d9d1318-f08d-49b5-879a-1016fe6a9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на тренировочный и тестовый наборы\n",
    "train = data.filter(f.year(f.col(\"hour_start\")) < 2024)\n",
    "test = data.filter(f.year(f.col(\"hour_start\")) == 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1352c5e7-18ec-4556-a451-b2b589fb1170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ct: bigint, ca: int, hour_start: timestamp, time_p: int, miles_p: double, velocity_p: double, farem_p: double, tipsm_p: double, tollsm_p: double, extrasm_p: double, totalm_p: double, comp1_p: int, comp2_p: int, comp3_p: int, comp4_p: int, comp5_p: int, compless5_p: int, trips_d: int, time_d: int, miles_d: double, velocity_d: double, farem_d: double, tipsm_d: double, tollsm_d: double, extrasm_d: double, totalm_d: double, comp1_d: int, comp2_d: int, comp3_d: int, comp4_d: int, comp5_d: int, compless5_d: int, cumulative_balance: int, trips_p_growth_1_to_0: double, trips_p_growth_2_to_1: double, trips_p_growth_3_to_2: double, trips_p_growth_4_to_3: double, trips_d_growth_1_to_0: double, trips_d_growth_2_to_1: double, trips_d_growth_3_to_2: double, trips_d_growth_4_to_3: double, velocity_p_growth_1_to_0: double, velocity_p_growth_2_to_1: double, velocity_p_growth_3_to_2: double, velocity_p_growth_4_to_3: double, velocity_d_growth_1_to_0: double, velocity_d_growth_2_to_1: double, velocity_d_growth_3_to_2: double, velocity_d_growth_4_to_3: double, trips_sh_168: int, trips_sh_84: int, trips_sh_24: int, trips_sh_28: int, trips_sh_12: int, trips_sh_8: int, trips_ma_168: double, trips_ma_84: double, trips_ma_24: double, trips_ma_28: double, trips_ma_12: double, trips_ma_8: double, trips_sh_4: int, trips_ma_4: double, trips_sh_1: int, trips_ma_1: double, trips_ma_168_growth: double, trips_ma_8_growth: double, trips_ma_4_growth: double, trips_target: int]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d71b798-1cf0-42c1-b006-3b201c85c73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2'686'518\n",
      "CPU times: user 0 ns, sys: 16.4 ms, total: 16.4 ms\n",
      "Wall time: 53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pds.gvf(train.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7410b840-3f72-4747-91af-7d7e6620dcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375'525\n",
      "CPU times: user 3.43 ms, sys: 475 µs, total: 3.91 ms\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pds.gvf(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "68cebea1-c64d-48e9-9fb5-d9f2c5dd4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable_name = 'trips_target'\n",
    "# исключаемые из расчетов ключевые поля для всех районов\n",
    "exc_cols = ['ct', 'ca', 'hour_start']\n",
    "# поля которые будут в рабочих ДФ\n",
    "# ['ct', 'ca', 'hour_start', 'trips_target', 'features']\n",
    "# 'features' - немасштабированный вектор-столбец\n",
    "selectedCols = exc_cols + [target_variable_name, 'features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "06396d4b-64c9-4749-aebc-74f89e4ce7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение уникальных комбинаций районов и округов\n",
    "geo_keys = data.select('ct', 'ca').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379d4dd-7f9e-4d56-8f79-b0b42d278060",
   "metadata": {},
   "source": [
    "### 2. получаю фрагмент geo_df_base_train/test для гео-ключа из трэйн дф, кэширую в ОЗУ geo_df_base_train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6014459d-d71e-4735-842a-5065a277b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаю фрагмент geo_df_base_train/test\n",
    "# для гео-ключа из трэйн/тест дф\n",
    "def get_geo_df_base(point, train, test):\n",
    "    ct = point[0]\n",
    "    ca = point[1]\n",
    "    # Фильтрация данных для текущего района и округа\n",
    "    geo_data_base_train = train.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "    geo_data_base_test = test.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "\n",
    "    return geo_data_base_train, geo_data_base_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ce3369e4-4a12-4b86-82fd-2ff2edff3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_df_in_ram(df):\n",
    "    df.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "07328fd2-ea0e-43ed-b1d0-929a3bae8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "point = (17031090200,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5744cda8-b5f4-4537-b30c-90955b813ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_data_base_train, geo_data_base_test = get_geo_df_base(point, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2be05558-7f0d-4b86-992b-241a60e0a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# кэширую в ОЗУ geo_df_base_train/test\n",
    "cache_df_in_ram(geo_data_base_train)\n",
    "cache_df_in_ram(geo_data_base_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f8f80b-89fe-4ca6-bee4-75019d9065f1",
   "metadata": {},
   "source": [
    "### 3. векторизирую фичи geo_df_base_train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "68bf10d8-2625-4c2a-b4ec-8106d7819855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# векторизация указанных полей датафрейма\n",
    "def assemble_vectors(df, features_list, selectedCols):\n",
    "    stages = []\n",
    "    assembler = VectorAssembler(inputCols=features_list, outputCol='features')\n",
    "    stages = [assembler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    # assemble model\n",
    "    assembleModel = pipeline.fit(df)\n",
    "    # apply assembler model on data\n",
    "    df = assembleModel.transform(df).select(selectedCols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "62e89d75-3235-462e-bd51-50bfff95caf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time_p miles_p farem_p tipsm_p comp1_p comp3_p comp4_p trips_d time_d miles_d farem_d tipsm_d extrasm_d comp2_d comp3_d comp4_d extrasm_p comp5_p'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list = excluded_fields_tot[point]\n",
    "\" \".join(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8d202-c31d-4bf0-a663-d95944b0a6c1",
   "metadata": {},
   "source": [
    "### 4. создаю рабочий дф geo_df_train/test дф с нужными полями (гео-ключ, время, метка, фича-вектор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "10fd3181-a768-4952-b3a2-c3a5f574e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_train = assemble_vectors(geo_data_base_train,features_list,selectedCols)\n",
    "geo_df_test = assemble_vectors(geo_data_base_test,features_list,selectedCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f33cda-6b20-4bb1-b51a-70aa32bfd796",
   "metadata": {},
   "source": [
    "### 5. кэширую в ОЗУ geo_df_train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "948ab9fc-11f8-46f2-ab16-0bd206f5a11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26'111\n"
     ]
    }
   ],
   "source": [
    "print(pds.gvf(geo_df_train.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3eebcef1-dd57-4b85-abb9-6ef8cf644308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'648\n"
     ]
    }
   ],
   "source": [
    "print(pds.gvf(geo_df_test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "af6ae2fb-d7fb-4630-af64-747ba65c351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# кэширую в ОЗУ geo_df_base_train/test\n",
    "cache_df_in_ram(geo_df_train)\n",
    "cache_df_in_ram(geo_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c7284-88a7-43b1-b1db-5f8ca6e4ac3e",
   "metadata": {},
   "source": [
    "### 6. создаю для geo_df_train скалер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c1fde410-8b3b-4d0e-91dc-83135ad080f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitted_scaler(df, featureCol):\n",
    "    outputCol = featureCol+\"_scaled\"\n",
    "    stages = []\n",
    "    scaler = StandardScaler(inputCol = featureCol,\n",
    "                            outputCol=outputCol,\n",
    "                            withStd=True, withMean=True)\n",
    "    stages = [scaler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    scaledAssembleModel = pipeline.fit(df)\n",
    "\n",
    "    return scaledAssembleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5d31d082-fcbf-4579-bbaa-2e3d84b8416e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ct', 'ca', 'hour_start', 'trips_target', 'features']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5545804a-7cd6-49f1-b11a-7046686160ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_train_scaler = get_fitted_scaler(geo_df_train, 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec96191-16dc-4985-a004-b7ea78901e7c",
   "metadata": {},
   "source": [
    "### 7. масштабирую-обновляю geo_df_train/test, кэширую в ОЗУ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6b229029-c3c0-4b6a-8f8c-e4a658e2a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_train = geo_df_train_scaler.transform(geo_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "dc210ae8-0637-4c43-96d2-399d9197f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_test = geo_df_train_scaler.transform(geo_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "938200af-fcbe-46ea-a823-818bceb7c153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26'111\n"
     ]
    }
   ],
   "source": [
    "print(pds.gvf(geo_df_train.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f8261612-45c6-418d-9acd-98e85c04f4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'648\n"
     ]
    }
   ],
   "source": [
    "print(pds.gvf(geo_df_test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b2eccacc-dc6d-49fe-8d9b-944d706aced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-------------------+------------+--------------------+--------------------+\n",
      "|         ct| ca|         hour_start|trips_target|            features|     features_scaled|\n",
      "+-----------+---+-------------------+------------+--------------------+--------------------+\n",
      "|17031090200|  6|2021-01-08 01:00:00|           1|(18,[7,8,9,10,11]...|[-1.1108452941626...|\n",
      "|17031090200|  6|2021-01-08 02:00:00|           2|(18,[0,1,2,4,7,8,...|[-1.0740070011258...|\n",
      "|17031090200|  6|2021-01-08 03:00:00|           1|(18,[0,1,2,4,7,8,...|[-1.0591881976946...|\n",
      "+-----------+---+-------------------+------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geo_df_train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304f373-506e-4890-823c-aa2f977ac610",
   "metadata": {},
   "source": [
    "### 8. обучаю модель лин.регрессии на трэйне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3ba663a7-34b1-483f-ae99-5039e0c8f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_regr_fit(point,\n",
    "                 scaled_geo_train,\n",
    "                 scaled_geo_test,\n",
    "                 features_col = 'features',\n",
    "                 scaled_features_col = 'features_scaled',\n",
    "                 target_variable_name = 'trips_target',\n",
    "                 exc_cols = None\n",
    "):\n",
    "    if exc_cols is None:\n",
    "        exc_cols = ['ct', 'ca', 'hour_start']\n",
    "    \n",
    "    reg = LinearRegression(\n",
    "        featuresCol = scaled_features_col,\n",
    "        labelCol = target_variable_name\n",
    "    )\n",
    "    \n",
    "    ct = point[0]\n",
    "    ca = point[1]\n",
    "        \n",
    "    selectedCols = exc_cols + [target_variable_name] + [scaled_features_col]\n",
    "\n",
    "    reg_model = reg.fit(scaled_geo_train)\n",
    "\n",
    "    geo_train_transformed = reg_model.transform(scaled_geo_train)\n",
    "    geo_test_transformed = reg_model.transform(scaled_geo_test)\n",
    "    \n",
    "    # Создание объекта RegressionEvaluator для оценки модели\n",
    "    evaluator = RegressionEvaluator()\n",
    "\n",
    "    # Настройка параметров оценки (RMSE и MAPE)\n",
    "    evaluator.setMetricName(\"rmse\")\n",
    "    evaluator.setPredictionCol(\"prediction\")\n",
    "    evaluator.setLabelCol(target_variable_name)\n",
    "\n",
    "    # Оценка модели\n",
    "    rmse = evaluator.evaluate(geo_train_transformed)\n",
    "    \n",
    "    # Аналогично для MAE\n",
    "    evaluator.setMetricName(\"mae\")\n",
    "    mae = evaluator.evaluate(geo_train_transformed)\n",
    "\n",
    "    # Получение предсказаний и реальных значений\n",
    "    # Вычисление MAPE\n",
    "    geo_train_transformed = geo_train_transformed.withColumn(\"abs_pct_error\", \n",
    "        f.abs((geo_train_transformed[\"prediction\"] - geo_train_transformed[target_variable_name]) \\\n",
    "            / geo_train_transformed[target_variable_name])\n",
    "    )\n",
    "    mape = geo_train_transformed.selectExpr(\"mean(abs_pct_error) * 100 as mape\")\\\n",
    "        .collect()[0][\"mape\"]\n",
    "    \n",
    "    return reg_model, geo_train_transformed, geo_test_transformed, rmse, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2f40a0cb-be55-4055-96ca-084833dee5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model, geo_train_transormed, geo_test_transformed, rmse, mae, mape =\\\n",
    "lin_regr_fit(point,\n",
    "                 geo_df_train,\n",
    "                 geo_df_test,\n",
    "                 features_col = 'features',\n",
    "                 scaled_features_col = 'features_scaled',\n",
    "                 target_variable_name = 'trips_target',\n",
    "                 exc_cols = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a2299274-9204-4f64-af1b-0f3e67fe7cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-------------------+------------+--------------------+--------------------+------------------+-------------------+\n",
      "|         ct| ca|         hour_start|trips_target|            features|     features_scaled|        prediction|      abs_pct_error|\n",
      "+-----------+---+-------------------+------------+--------------------+--------------------+------------------+-------------------+\n",
      "|17031090200|  6|2021-01-08 01:00:00|           1|(18,[7,8,9,10,11]...|[-1.1108452941626...| 4.558061596034879| 3.5580615960348787|\n",
      "|17031090200|  6|2021-01-08 02:00:00|           2|(18,[0,1,2,4,7,8,...|[-1.0740070011258...| 5.716880539490116| 1.8584402697450582|\n",
      "|17031090200|  6|2021-01-08 03:00:00|           1|(18,[0,1,2,4,7,8,...|[-1.0591881976946...| 5.865526914000082|  4.865526914000082|\n",
      "|17031090200|  6|2021-01-08 04:00:00|           3|(18,[0,1,2,4],[42...|[-1.0889301623277...| 5.501880167738534| 0.8339600559128447|\n",
      "|17031090200|  6|2021-01-08 05:00:00|           5|(18,[0,1,2,4,7,8,...|[-1.0655018428185...| 7.294559615249254|0.45891192304985073|\n",
      "|17031090200|  6|2021-01-08 06:00:00|           9|[3825.0,22.32,100...|[-0.9112610578090...|  9.39709346861265|0.04412149651251676|\n",
      "|17031090200|  6|2021-01-08 07:00:00|           9|[3008.0,22.7,9.68...|[-0.9538912071164...| 9.125310588193662|0.01392339868818462|\n",
      "|17031090200|  6|2021-01-08 08:00:00|          14|[6430.0,15.030000...|[-0.7753350615473...|10.963733754565311|0.21687616038819207|\n",
      "|17031090200|  6|2021-01-08 09:00:00|          23|[5862.0,22.909999...|[-0.8049726684097...| 17.77001656766738|0.22739058401446174|\n",
      "|17031090200|  6|2021-01-08 10:00:00|          18|[12893.0,53.05,15...|[-0.4381029257164...| 23.93119491675232| 0.3295108287084623|\n",
      "|17031090200|  6|2021-01-08 11:00:00|          18|[13006.0,51.48000...|[-0.4322067116751...|18.923412993868446|0.05130072188158034|\n",
      "|17031090200|  6|2021-01-08 12:00:00|          19|[16061.0,60.26,22...|[-0.2728002170188...| 21.33826651893915|0.12306665889153423|\n",
      "|17031090200|  6|2021-01-08 13:00:00|          17|[12205.0,53.41999...|[-0.4740019988174...| 22.32418140411383| 0.3131871414184606|\n",
      "|17031090200|  6|2021-01-08 14:00:00|           7|[18431.0,67.13,18...|[-0.1491362588076...|19.823928439972004| 1.8319897771388578|\n",
      "|17031090200|  6|2021-01-08 15:00:00|          21|[7157.0,35.64,16....|[-0.7374010119188...|10.385326329795772| 0.5054606509621061|\n",
      "|17031090200|  6|2021-01-08 16:00:00|          19|[11812.0,39.36,12...|[-0.4945083007486...| 22.10427290001847|0.16338278421149835|\n",
      "|17031090200|  6|2021-01-08 17:00:00|           8|[16430.0,62.32,17...|[-0.2535462083353...|21.181995145863297| 1.6477493932329121|\n",
      "|17031090200|  6|2021-01-08 18:00:00|           7|[16365.0,63.16999...|[-0.2569378358812...| 14.44828523395704| 1.0640407477081486|\n",
      "|17031090200|  6|2021-01-08 19:00:00|           7|[2173.0,7.2399999...|[-0.9974605763596...| 9.036846809999753| 0.2909781157142505|\n",
      "|17031090200|  6|2021-01-08 20:00:00|           2|[9698.0,39.63,14....|[-0.6048144643176...|10.640420370495587|  4.320210185247793|\n",
      "+-----------+---+-------------------+------------+--------------------+--------------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geo_train_transormed.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "db50b934-69d1-4a65-92a7-44f93af13b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.527609279906736, 6.870859422792388, 57.625668708390506)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "28fa335c-fb9f-46d7-86d8-79ca0c2d2ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a DataFrame containing the names, types, and sizes of objects...\n",
      "objects count: 533'692\n",
      "total size: 418'408'507\n"
     ]
    }
   ],
   "source": [
    "spark_df = get_spark_df_list(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "80c825cd-0eb3-4cc0-b1b1-d79a244b3745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size (bytes)</th>\n",
       "      <th>cached</th>\n",
       "      <th>cache_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>380186</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380187</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381643</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381665</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382496</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382644</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384897</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386097</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388317</th>\n",
       "      <td>geo_train_transormed</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "      <td>Serialized 1x Replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388372</th>\n",
       "      <td>geo_test_transformed</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "      <td>Serialized 1x Replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388885</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388886</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390123</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390136</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392722</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392723</th>\n",
       "      <td>DataFrame[ct: bigint, ca: int, hour_start: tim...</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395362</th>\n",
       "      <td>geo_data_base_test</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "      <td>Memory Serialized 1x Replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395363</th>\n",
       "      <td>geo_data_base_train</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "      <td>Memory Serialized 1x Replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395756</th>\n",
       "      <td>geo_df_test</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "      <td>Serialized 1x Replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395758</th>\n",
       "      <td>geo_df_train</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "      <td>Serialized 1x Replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416869</th>\n",
       "      <td>train</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "      <td>Disk Memory Deserialized 1x Replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416871</th>\n",
       "      <td>test</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "      <td>Disk Memory Deserialized 1x Replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418735</th>\n",
       "      <td>data_sample</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "      <td>Serialized 1x Replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418753</th>\n",
       "      <td>data</td>\n",
       "      <td>Spark DataFrame</td>\n",
       "      <td>56</td>\n",
       "      <td>True</td>\n",
       "      <td>Disk Memory Deserialized 1x Replicated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Name             Type  \\\n",
       "380186  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "380187  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "381643  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "381665  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "382496  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "382644  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "384897  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "386097  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "388317                               geo_train_transormed  Spark DataFrame   \n",
       "388372                               geo_test_transformed  Spark DataFrame   \n",
       "388885  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "388886  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "390123  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "390136  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "392722  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "392723  DataFrame[ct: bigint, ca: int, hour_start: tim...  Spark DataFrame   \n",
       "395362                                 geo_data_base_test  Spark DataFrame   \n",
       "395363                                geo_data_base_train  Spark DataFrame   \n",
       "395756                                        geo_df_test  Spark DataFrame   \n",
       "395758                                       geo_df_train  Spark DataFrame   \n",
       "416869                                              train  Spark DataFrame   \n",
       "416871                                               test  Spark DataFrame   \n",
       "418735                                        data_sample  Spark DataFrame   \n",
       "418753                                               data  Spark DataFrame   \n",
       "\n",
       "        Size (bytes) cached                             cache_level  \n",
       "380186            56   None                                    None  \n",
       "380187            56   None                                    None  \n",
       "381643            56   None                                    None  \n",
       "381665            56   None                                    None  \n",
       "382496            56   None                                    None  \n",
       "382644            56   None                                    None  \n",
       "384897            56   None                                    None  \n",
       "386097            56   None                                    None  \n",
       "388317            56  False                Serialized 1x Replicated  \n",
       "388372            56  False                Serialized 1x Replicated  \n",
       "388885            56   None                                    None  \n",
       "388886            56   None                                    None  \n",
       "390123            56   None                                    None  \n",
       "390136            56   None                                    None  \n",
       "392722            56   None                                    None  \n",
       "392723            56   None                                    None  \n",
       "395362            56   True         Memory Serialized 1x Replicated  \n",
       "395363            56   True         Memory Serialized 1x Replicated  \n",
       "395756            56  False                Serialized 1x Replicated  \n",
       "395758            56  False                Serialized 1x Replicated  \n",
       "416869            56   True  Disk Memory Deserialized 1x Replicated  \n",
       "416871            56   True  Disk Memory Deserialized 1x Replicated  \n",
       "418735            56  False                Serialized 1x Replicated  \n",
       "418753            56   True  Disk Memory Deserialized 1x Replicated  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab01cde6-17a9-48f6-9287-f53b885bf601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # векторизация\n",
    "# df_vec = None\n",
    "# for row in tqdm(geo_keys):\n",
    "    \n",
    "#     ct = row['ct']\n",
    "#     ca = row['ca']\n",
    "#     # Фильтрация данных для текущего района и округа\n",
    "#     geo_data = data.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "#     # print(row,geo_data.count())\n",
    "#     excluded_fields_list = excluded_fields_tot[row] + exc_cols\n",
    "#     features_list = [col for col in data.columns if col not in excluded_fields_list]\n",
    "#     # print(features_list)\n",
    "#     assembled_data = assemble_vectors(df=geo_data,\n",
    "#                                 features_list=features_list,\n",
    "                                \n",
    "#                                 target_variable_name='trips_target')\n",
    "#     # print(assembled_data.count())\n",
    "#     # Если это первая итерация, инициализируем df_vec\n",
    "#     if df_vec is None:\n",
    "#         df_vec = assembled_data\n",
    "#     else:\n",
    "#         df_vec = df_vec.union(assembled_data)\n",
    "#         # print(df_vec.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17ac098d-ef32-4628-a70a-145f48e72d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание нового пустого DataFrame с той же схемой\n",
    "# empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), geo_data.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2ab4ab9-7a78-41bc-9e63-dc193edc6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vec.show(3)\n",
    "# df_vec.cache()\n",
    "# print(pds.gvf(df_vec.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b50e09af-ce57-44b0-9fc3-ea3330971b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_to_sel = [\n",
    "#     'ct', 'ca', 'hour_start', 'trips_target', 'features',\n",
    "# ]\n",
    "\n",
    "# df_vec.select(*f_to_sel).filter(f.col('ct')==17031838200).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b1511a2-9748-4b09-a014-c85268486732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if df_vec.is_cached:\n",
    "#  print(\"DataFrame закэширован\")\n",
    "# else:\n",
    "#  print(\"DataFrame не закэширован\")\n",
    "\n",
    "# if df_vec.storageLevel == StorageLevel.MEMORY_ONLY:\n",
    "#  print(\"DataFrame закэширован в памяти\")\n",
    "# elif df_vec.storageLevel == StorageLevel.DISK_ONLY:\n",
    "#  print(\"DataFrame закэширован на диске\")\n",
    "# else:\n",
    " # print(\"DataFrame закэширован в:\",df_vec.storageLevel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfac8fae-4225-44ec-ad09-3d174e3ba4ed",
   "metadata": {},
   "source": [
    "данные DataFrame были закэшированы с использованием уровня хранения Disk Memory Deserialized 1x Replicated. Это значит, что данные сначала сериализуются и сохраняются на диске, а затем десериализуются в память для обработки. Также данные были реплицированы один раз, что обеспечивает дополнительную надежность и доступность данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f24b584a-9310-40e8-ba2e-991a304a8157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n",
      "CPU times: user 38.3 ms, sys: 9.54 ms, total: 47.8 ms\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(pds.gvf(df_vec.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a92740d9-8b6c-47ec-88d6-870e9961eb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n",
      "CPU times: user 10.7 ms, sys: 83 µs, total: 10.7 ms\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(pds.gvf(data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8da93d-bcd7-4038-bc56-c303540c2f3e",
   "metadata": {},
   "source": [
    "На данный момент непонятна выгода от векторизации. При более медленном выполнении действий, нет пока понятного способа корректно сохранить-загрузить веторизированный ДФ. В csv нельзя сохранить тип vector, в parquet - можно, но после загрузки выдаются ошибки работы JVM объектов.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e017dad-a9ec-4543-ad44-e2070723e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vec.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19a1ac-4f58-4a44-9357-4b7350b64e16",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc71bd-1391-4ff9-a8a5-a3c795b7ee9e",
   "metadata": {},
   "source": [
    "Разделю ДФ на обучающую и тестовую выборки: тест - 2024 год, трэйн - 2021-2023 гг. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8cee04c1-f12a-4d4e-a7d8-b735bfc92e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на тренировочный и тестовый наборы\n",
    "train = data.filter(f.year(f.col(\"hour_start\")) < 2024)\n",
    "test = data.filter(f.year(f.col(\"hour_start\")) == 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f68a087f-4677-4983-a7d7-9541252f5e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2'686'518\n",
      "375'525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ct: bigint, ca: int, hour_start: timestamp, time_p: int, miles_p: double, velocity_p: double, farem_p: double, tipsm_p: double, tollsm_p: double, extrasm_p: double, totalm_p: double, comp1_p: int, comp2_p: int, comp3_p: int, comp4_p: int, comp5_p: int, compless5_p: int, trips_d: int, time_d: int, miles_d: double, velocity_d: double, farem_d: double, tipsm_d: double, tollsm_d: double, extrasm_d: double, totalm_d: double, comp1_d: int, comp2_d: int, comp3_d: int, comp4_d: int, comp5_d: int, compless5_d: int, cumulative_balance: int, trips_p_growth_1_to_0: double, trips_p_growth_2_to_1: double, trips_p_growth_3_to_2: double, trips_p_growth_4_to_3: double, trips_d_growth_1_to_0: double, trips_d_growth_2_to_1: double, trips_d_growth_3_to_2: double, trips_d_growth_4_to_3: double, velocity_p_growth_1_to_0: double, velocity_p_growth_2_to_1: double, velocity_p_growth_3_to_2: double, velocity_p_growth_4_to_3: double, velocity_d_growth_1_to_0: double, velocity_d_growth_2_to_1: double, velocity_d_growth_3_to_2: double, velocity_d_growth_4_to_3: double, trips_sh_168: int, trips_sh_84: int, trips_sh_24: int, trips_sh_28: int, trips_sh_12: int, trips_sh_8: int, trips_ma_168: double, trips_ma_84: double, trips_ma_24: double, trips_ma_28: double, trips_ma_12: double, trips_ma_8: double, trips_sh_4: int, trips_ma_4: double, trips_sh_1: int, trips_ma_1: double, trips_ma_168_growth: double, trips_ma_8_growth: double, trips_ma_4_growth: double, trips_target: int]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pds.gvf(train.count()))\n",
    "train.cache()\n",
    "print(pds.gvf(test.count()))\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa243414-d169-41a5-92da-b037ea7e4ba6",
   "metadata": {},
   "source": [
    "Масштабирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "347d0590-a2d7-490f-a81f-d7428aa6f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение уникальных комбинаций районов и округов\n",
    "geo_keys = data.select('ct', 'ca').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc8e6f2c-b4b4-4d1b-9628-e0b36c92809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# исключаемые поля для всех районов\n",
    "exc_cols = ['ct', 'ca', 'hour_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b7c0c08d-6d62-4dbf-ba3f-5a5ac0cee8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = data.columns\n",
    "target_variable_name = 'trips_target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f49d2c7f-4ad2-42a1-a26c-e1854792024d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time_p miles_p farem_p tipsm_p comp1_p comp3_p comp4_p trips_d time_d miles_d farem_d tipsm_d extrasm_d comp2_d comp3_d comp4_d extrasm_p comp5_p'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_point = (17031090200,6)\n",
    "\" \".join(excluded_fields_tot[test_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "09611c50-a2ed-4cae-b243-5a8ebb3a011e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17031090200"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_point[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fdf67b9b-0eb8-4ec2-b85a-06c0b7b8a335",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_and_std' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m mean_and_std_cols\u001b[38;5;241m=\u001b[39m[c \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m features_list \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m      8\u001b[0m     (f\u001b[38;5;241m.\u001b[39mmean(col)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m),f\u001b[38;5;241m.\u001b[39mstddev(col)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_std\u001b[39m\u001b[38;5;124m\"\u001b[39m))]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# mean_and_std_cols\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# mean_and_std = test_df.select(mean_and_std_cols).first()\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m scaled_cols\u001b[38;5;241m=\u001b[39m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean_and_std\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mmean_and_std\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_std\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_s\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_list\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# test_df = test_df.select(test_df.columns + scaled_cols)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m scaled_cols\n",
      "Cell \u001b[0;32mIn[93], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m mean_and_std_cols\u001b[38;5;241m=\u001b[39m[c \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m features_list \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m      8\u001b[0m     (f\u001b[38;5;241m.\u001b[39mmean(col)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m),f\u001b[38;5;241m.\u001b[39mstddev(col)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_std\u001b[39m\u001b[38;5;124m\"\u001b[39m))]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# mean_and_std_cols\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# mean_and_std = test_df.select(mean_and_std_cols).first()\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m scaled_cols\u001b[38;5;241m=\u001b[39m[((f\u001b[38;5;241m.\u001b[39mcol(col) \u001b[38;5;241m-\u001b[39m \u001b[43mmean_and_std\u001b[49m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m/\u001b[39mmean_and_std[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_std\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_s\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m features_list]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# test_df = test_df.select(test_df.columns + scaled_cols)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m scaled_cols\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_and_std' is not defined"
     ]
    }
   ],
   "source": [
    "ct, ca = test_point\n",
    "\n",
    "excluded_fields_list = excluded_fields_tot[(ct,ca)] + exc_cols\n",
    "features_list = [col for col in data.columns if col not in excluded_fields_list]\n",
    "scaled_features_list = [f\"{col}_scaled\" for col in features_list]\n",
    "\n",
    "mean_and_std_cols=[c for col in features_list for c in \n",
    "    (f.mean(col).alias(f\"{col}_mean\"),f.stddev(col).alias(f\"{col}_std\"))]\n",
    "\n",
    "# mean_and_std_cols\n",
    "\n",
    "# mean_and_std = test_df.select(mean_and_std_cols).first()\n",
    "scaled_cols=[((f.col(col) - mean_and_std[f\"{col}_mean\"])\n",
    "    /mean_and_std[f\"{col}_std\"]).alias(f\"{col}_scaled\") for col in features_list]\n",
    "# test_df = test_df.select(test_df.columns + scaled_cols)\n",
    "scaled_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "17ebb4e7-57b3-4d10-928d-4b8c8f52e514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"featuresCol\". Could not convert <class 'list'> to string type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:503\u001b[0m, in \u001b[0;36mParams._set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypeConverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:236\u001b[0m, in \u001b[0;36mTypeConverters.toString\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to string type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(value))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert <class 'list'> to string type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reg_model, scaled_geo_train, rmse, mape \u001b[38;5;241m=\u001b[39m \u001b[43mlin_regr_with_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# scaled_geo_train = lin_regr_with_scale(train, test, test_point)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[98], line 41\u001b[0m, in \u001b[0;36mlin_regr_with_scale\u001b[0;34m(train, test, point)\u001b[0m\n\u001b[1;32m     38\u001b[0m scaled_geo_train \u001b[38;5;241m=\u001b[39m scaled_geo_train\u001b[38;5;241m.\u001b[39mselect(selectedCols)\n\u001b[1;32m     39\u001b[0m scaled_geo_test \u001b[38;5;241m=\u001b[39m scaled_geo_test\u001b[38;5;241m.\u001b[39mselect(selectedCols)\n\u001b[0;32m---> 41\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaled_features_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_variable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m reg_model \u001b[38;5;241m=\u001b[39m reg\u001b[38;5;241m.\u001b[39mfit(scaled_geo_train)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Создание объекта RegressionEvaluator для оценки модели\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/regression.py:332\u001b[0m, in \u001b[0;36mLinearRegression.__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.regression.LinearRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid\n\u001b[1;32m    330\u001b[0m )\n\u001b[1;32m    331\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetParams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/regression.py:363\u001b[0m, in \u001b[0;36mLinearRegression.setParams\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03msetParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m          maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03mSets params for linear regression.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    362\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:505\u001b[0m, in \u001b[0;36mParams._set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             value \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mtypeConverter(value)\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 505\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid param value given for param \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (p\u001b[38;5;241m.\u001b[39mname, e))\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paramMap[p] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid param value given for param \"featuresCol\". Could not convert <class 'list'> to string type"
     ]
    }
   ],
   "source": [
    "reg_model, scaled_geo_train, rmse, mape = lin_regr_with_scale(train, test, test_point)\n",
    "# scaled_geo_train = lin_regr_with_scale(train, test, test_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "97b0848e-7bed-47f4-ae8a-dc6fc9de4749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ct ca hour_start time_p miles_p velocity_p farem_p tipsm_p tollsm_p extrasm_p totalm_p comp1_p comp2_p comp3_p comp4_p comp5_p compless5_p trips_d time_d miles_d velocity_d farem_d tipsm_d tollsm_d extrasm_d totalm_d comp1_d comp2_d comp3_d comp4_d comp5_d compless5_d cumulative_balance trips_p_growth_1_to_0 trips_p_growth_2_to_1 trips_p_growth_3_to_2 trips_p_growth_4_to_3 trips_d_growth_1_to_0 trips_d_growth_2_to_1 trips_d_growth_3_to_2 trips_d_growth_4_to_3 velocity_p_growth_1_to_0 velocity_p_growth_2_to_1 velocity_p_growth_3_to_2 velocity_p_growth_4_to_3 velocity_d_growth_1_to_0 velocity_d_growth_2_to_1 velocity_d_growth_3_to_2 velocity_d_growth_4_to_3 trips_sh_168 trips_sh_84 trips_sh_24 trips_sh_28 trips_sh_12 trips_sh_8 trips_ma_168 trips_ma_84 trips_ma_24 trips_ma_28 trips_ma_12 trips_ma_8 trips_sh_4 trips_ma_4 trips_sh_1 trips_ma_1 trips_ma_168_growth trips_ma_8_growth trips_ma_4_growth trips_target velocity_p_scaled tollsm_p_scaled totalm_p_scaled comp2_p_scaled compless5_p_scaled velocity_d_scaled tollsm_d_scaled totalm_d_scaled comp1_d_scaled comp5_d_scaled compless5_d_scaled cumulative_balance_scaled trips_p_growth_1_to_0_scaled trips_p_growth_2_to_1_scaled trips_p_growth_3_to_2_scaled trips_p_growth_4_to_3_scaled trips_d_growth_1_to_0_scaled trips_d_growth_2_to_1_scaled trips_d_growth_3_to_2_scaled trips_d_growth_4_to_3_scaled velocity_p_growth_1_to_0_scaled velocity_p_growth_2_to_1_scaled velocity_p_growth_3_to_2_scaled velocity_p_growth_4_to_3_scaled velocity_d_growth_1_to_0_scaled velocity_d_growth_2_to_1_scaled velocity_d_growth_3_to_2_scaled velocity_d_growth_4_to_3_scaled trips_sh_168_scaled trips_sh_84_scaled trips_sh_24_scaled trips_sh_28_scaled trips_sh_12_scaled trips_sh_8_scaled trips_ma_168_scaled trips_ma_84_scaled trips_ma_24_scaled trips_ma_28_scaled trips_ma_12_scaled trips_ma_8_scaled trips_sh_4_scaled trips_ma_4_scaled trips_sh_1_scaled trips_ma_1_scaled trips_ma_168_growth_scaled trips_ma_8_growth_scaled trips_ma_4_growth_scaled trips_target_scaled'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(scaled_geo_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc7fb8-f9d9-4797-977f-c5f7fbbd0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_geo_test = geo_scaler.transform(geo_test).select(selectedCols)\n",
    "\n",
    "#     # Если это первая итерация, инициализируем df_scaled_\n",
    "#     if df_scaled_train is None:\n",
    "#         df_scaled_train = scaled_geo_train\n",
    "#     else:\n",
    "#         df_scaled_train = df_scaled_train.union(scaled_geo_train)\n",
    "        \n",
    "#     if df_scaled_test is None:\n",
    "#         df_scaled_test = scaled_geo_test\n",
    "#     else:\n",
    "#         df_scaled_test = df_scaled_test.union(scaled_geo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "215ddd75-0773-413d-ae72-8c36e37bd86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# масштабирование\n",
    "# df_scaled_train = None\n",
    "# df_scaled_test = None\n",
    "# for row in tqdm(geo_keys):\n",
    "    \n",
    "#     ct = row['ct']\n",
    "#     ca = row['ca']\n",
    "#     # Фильтрация данных для текущего района и округа\n",
    "#     geo_train = train.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "#     geo_test = test.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "    \n",
    "#     # print(features_list)\n",
    "#      # select all the columns + target + newly created 'features' column\n",
    "#     geo_scaler = fitted_scaler(geo_train, target_variable_name)\n",
    "#     scaled_geo_train = geo_scaler.transform(geo_train).select(selectedCols)\n",
    "#     scaled_geo_test = geo_scaler.transform(geo_test).select(selectedCols)\n",
    "    \n",
    "#     # print(assembled_data.count())\n",
    "#     # Если это первая итерация, инициализируем df_scaled_\n",
    "#     if df_scaled_train is None:\n",
    "#         df_scaled_train = scaled_geo_train\n",
    "#     else:\n",
    "#         df_scaled_train = df_scaled_train.union(scaled_geo_train)\n",
    "        \n",
    "#     if df_scaled_test is None:\n",
    "#         df_scaled_test = scaled_geo_test\n",
    "#     else:\n",
    "#         df_scaled_test = df_scaled_test.union(scaled_geo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e812d-24f3-45ff-8aea-5ff17fb839a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521e4cee-02df-434b-a30b-1d0010502be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled_train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981750c-a9fe-49a8-a9fa-3e655ae6dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea99e463-9360-473c-9bd5-ebb92186dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38acdf-e6d0-46fc-9e11-6565f85e6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scaler():\n",
    "    # масштабируем в лоб, поскольку работаем не с ветор-столбцом\n",
    "    # mean_and_std_cols=[c for col in features_list for c in \n",
    "    # (f.mean(col).alias(f\"{col}_mean\"),f.stddev(col).alias(f\"{col}_std\"))]\n",
    "    \n",
    "    # mean_and_std = geo_train.select(mean_and_std_cols).first()\n",
    "    # scaled_cols=[((f.col(col) - mean_and_std[f\"{col}_mean\"])\n",
    "    #     /mean_and_std[f\"{col}_std\"]).alias(f\"{col}_scaled\") for col in features_list]\n",
    "    # scaled_geo_train = geo_train.select(geo_train.columns + scaled_cols)\n",
    "    # scaled_geo_test = geo_test.select(geo_test.columns + scaled_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
