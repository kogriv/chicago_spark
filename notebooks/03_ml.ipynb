{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ac4a7a-4644-45d3-8786-944ff88f222a",
   "metadata": {},
   "source": [
    "Ноутбук по проекту chicago_spark.  \n",
    "МО"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de64cacf-4b21-4fc7-85ca-0d7b7de73885",
   "metadata": {},
   "source": [
    "## Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f9b287-4613-4f96-9125-2472424b9cfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c57728-71d4-43e9-a453-c5d23d6a57a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.signal import welch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbb9443-57d6-4aab-ad7d-c67719e98e03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from itertools import islice\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d04534c3-1092-4cb4-9ed0-2a6b99acbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c506d768-519f-49a0-9af6-b44cbcf036bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enviserv.dictan import DictAnalyzer # анализ словарей\n",
    "import pandserv as pds # форматирование небольших пандас ДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c22c0b8-1ea3-417e-8289-34ca57326d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkserv import SparkApp, Cols\n",
    "# в SparkApp упакованы функции создания спарк приложения \n",
    "# с определением IP мастер-ноды и с подключением к кластеру\n",
    "\n",
    "# Col - класс для формирования коротких псевдонимов имен столбцов\n",
    "# при этом исходные имена полей не меняются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63596c0d-9087-4e1a-80e3-a1fd7a90babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import DataFrame as pydf\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db2ec96-2997-458c-b67a-f9e5dd6aa586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# гео библиотеки\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e47d77-cebf-4472-99e9-c2e852867df5",
   "metadata": {},
   "source": [
    "## Создание сессии, загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c292a0-9c16-44d8-8aa6-e5a51b5e08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:spark_master_ip: 172.18.0.2\n",
      "INFO:spark_app:pyspark version: 3.4.1\n",
      "INFO:spark_app:starting building spark app object: pyspark-taxi-forecasting\n",
      "INFO:spark_app:Spark app object built as: <pyspark.sql.session.SparkSession object at 0x7fe494cce4d0>\n",
      "INFO:spark_app:==================================================================\n",
      "INFO:spark_app:Spark object can be accessed as the SparkApp_object.spark property\n",
      "INFO:spark_app:==================================================================\n"
     ]
    }
   ],
   "source": [
    "spark_app = SparkApp(my_logger_create_level = 'INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4de973af-73c4-43f8-90b6-b1d422a3360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:spark_master_ip: 172.18.0.2\n"
     ]
    }
   ],
   "source": [
    "spark_master_ip = spark_app.get_spark_master_ip()\n",
    "# print(spark_master_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55a91437-60e9-4bc2-a9cb-58365f000efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = spark_app.build_spark_app(spark_master_ip=spark_master_ip)\n",
    "# spark = spark_app.spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c7d30-9eed-479c-b899-2fcd6c2d78d4",
   "metadata": {},
   "source": [
    "Для корректного завершения спарк-сессии (например, для переключения между ноутбуками) следует останавливать сессию полностью. Для этого использую метод .stop_spark_app() класса SparkApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "412582e5-44f8-44dd-a8f0-7d9fe46ad1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_app.stop_spark_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b17caf8-7f00-4c20-a83b-744c2d0ab81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fe494cce4d0>\n"
     ]
    }
   ],
   "source": [
    "print(spark_app.spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5221d404-34f2-4fd8-8b0c-531fd6f93bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:pyspark version: 3.4.1\n",
      "INFO:spark_app:starting building spark app object: pyspark-taxi-forecasting\n",
      "INFO:spark_app:Spark app object built as: <pyspark.sql.session.SparkSession object at 0x7fe494cce4d0>\n",
      "INFO:spark_app:==================================================================\n",
      "INFO:spark_app:Spark object can be accessed as the SparkApp_object.spark property\n",
      "INFO:spark_app:==================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d10fb5fe9c53:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://172.18.0.2:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-taxi-forecasting</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe494cce4d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_app.build_spark_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab1b14-51a4-4d05-9ada-4217282f13ce",
   "metadata": {},
   "source": [
    "Получим стандартный объект `spark` из созданного выше объекта `spark_app`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56cdf778-5944-484e-a72f-6e151b2ec3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = spark_app.spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca259b2f-ac8d-4de8-800b-754588a3963c",
   "metadata": {},
   "source": [
    "Проверка работы спарк-объекта на кластере. Если все в порядке, то тест должен выполниться достаточно быстро и отобразить тестовый ДФ.  \n",
    "```txt\n",
    "+------------+-----------+\n",
    "|student_name|student_age|\n",
    "+------------+-----------+\n",
    "|       Alice|         10|\n",
    "|         Bob|         20|\n",
    "+------------+-----------+\n",
    "```\n",
    "\n",
    "Если исходные образы кластера собраны с ошибкой, возможно \"зависание\" работы теста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65e12fff-3cf5-4918-b8f7-d4ce0d306949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n",
      "DataFrame created successfully.\n",
      "Alias DataFrame created successfully.\n",
      "DataFrame data matches expected result.\n",
      "+------------+-----------+\n",
      "|student_name|student_age|\n",
      "+------------+-----------+\n",
      "|       Alice|         10|\n",
      "|         Bob|         20|\n",
      "+------------+-----------+\n",
      "\n",
      "DataFrame show output matches expected output.\n",
      "\n",
      "=======use======================\n",
      "*      ____              __    *\n",
      "*     / __/__  ___ _____/ /__  *\n",
      "*    _\\ \\/ _ \\/ _ `/ __/  '_/  *\n",
      "*   /__ / .__/\\_,_/_/ /_/\\_\\   *\n",
      "*      /_/                     * \n",
      "================================\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "spark_app.test_spark_functionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd529952-7b42-4dff-bf7b-d4a3d304ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = DictAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a49c4e5f-8973-485c-8714-94448bbe1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция простой рандомизированной выборки\n",
    "def random_sample_dataframe(dataframe, percentage):\n",
    "    # Генерируем случайные числа от 0 до 1 и фильтруем строки\n",
    "    df = dataframe.filter(f.rand() < percentage)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62ddc595-f4f4-4033-b890-cf42b5c3d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# agg_hour.coalesce(1).write.csv(\"/work/data/taxis_agg_hour_growth.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c59bfe-cbcc-4a69-8cb4-c6931b61b50a",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb679a8d-5f92-49ab-80b2-d3a76792f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# agg_hour = spark.read.load('/work/data/taxis_agg_hour_growth.csv', \n",
    "#                        format='csv', header='true'\n",
    "#                         , inferSchema='true'\n",
    "#                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df4f4d57-25f7-4c70-976f-fd4452ca779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_hour.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9d40cea-0b3e-4396-b995-928bfc94e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_hour.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ade35b3b-acac-452e-ac1c-82be92d46759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_null = agg_hour.select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in agg_hour.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb0aa96f-4e05-4dab-83ea-23dabb49fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_null = agg_null.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62603269-3dbf-45c6-a9ec-7ec4934cf033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_null[agg_null.iloc[:, 0] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7569a36-ee74-487e-b86b-438e5356fdb2",
   "metadata": {},
   "source": [
    "Загрузим данные по исключаемым (высокоррелированным) полям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bdae2d7-8b16-4707-966d-6b83e15c9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_f = pd.read_csv('/work/data/excluded_fields.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3db3af7-e825-4568-b711-a9fd2434cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для преобразования DataFrame в словарь\n",
    "def get_excuded_fields_to_dict(df):\n",
    "    excluded_fields_tot = {}\n",
    "    grouped = df.groupby(['ct', 'ca'])\n",
    "    for (ct, ca), group in grouped:\n",
    "        excluded_fields_tot[(ct, ca)] = group['excluded_field'].tolist()\n",
    "    return excluded_fields_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f8afe7e-4736-430c-a597-71eb61c6a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_fields_tot = get_excuded_fields_to_dict(excl_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a974c60-8161-4c21-9f67-438d203bca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    (10000000000, 91): ['time_p', 'miles_p', 'farem_p', 'comp1_p', 'comp3_p', 'comp4_p', 'comp5_p', 'extrasm_p']\n",
      "    (12000000001, 53): ['time_p', 'farem_p', 'trips_d', 'time_d', 'miles_d', 'farem_d', 'totalm_p', 'velocity_p_growth_3_to_2']\n",
      "    (12000000002, 75): ['time_p', 'miles_p', 'farem_p', 'tipsm_p', 'comp5_p', 'trips_d', 'time_d', 'miles_d', 'farem_d', 'tipsm_d', 'comp4_p']\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "da.print_dict(dict(islice(excluded_fields_tot.items(), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab429d-b219-4826-aab2-bbcaee4526ed",
   "metadata": {},
   "source": [
    "Будем считать, что от мультиколлинеарности в линейных моделях с помощью исключения этих полей получится избавиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f44b76c1-cfab-4115-a7ce-d840d7bd9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# agg_hour.coalesce(1).write.csv(\"/work/data/taxis_agg_hour_result.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f705c08-5874-478d-a978-3646ba2e282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 ms, sys: 0 ns, total: 17.2 ms\n",
      "Wall time: 41.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = spark.read.load('/work/data/taxis_agg_hour_result.csv', \n",
    "                       format='csv', header='true'\n",
    "                        , inferSchema='true'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ced2355-4769-4ff4-98e3-4437a9fa7d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n"
     ]
    }
   ],
   "source": [
    "print(pds.gvf(data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "296b0a00-02cb-4a6c-8fe7-44a58ed69f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ct: bigint, ca: int, hour_start: timestamp, time_p: int, miles_p: double, velocity_p: double, farem_p: double, tipsm_p: double, tollsm_p: double, extrasm_p: double, totalm_p: double, comp1_p: int, comp2_p: int, comp3_p: int, comp4_p: int, comp5_p: int, compless5_p: int, trips_d: int, time_d: int, miles_d: double, velocity_d: double, farem_d: double, tipsm_d: double, tollsm_d: double, extrasm_d: double, totalm_d: double, comp1_d: int, comp2_d: int, comp3_d: int, comp4_d: int, comp5_d: int, compless5_d: int, cumulative_balance: int, trips_p_growth_1_to_0: double, trips_p_growth_2_to_1: double, trips_p_growth_3_to_2: double, trips_p_growth_4_to_3: double, trips_d_growth_1_to_0: double, trips_d_growth_2_to_1: double, trips_d_growth_3_to_2: double, trips_d_growth_4_to_3: double, velocity_p_growth_1_to_0: double, velocity_p_growth_2_to_1: double, velocity_p_growth_3_to_2: double, velocity_p_growth_4_to_3: double, velocity_d_growth_1_to_0: double, velocity_d_growth_2_to_1: double, velocity_d_growth_3_to_2: double, velocity_d_growth_4_to_3: double, trips_sh_168: int, trips_sh_84: int, trips_sh_24: int, trips_sh_28: int, trips_sh_12: int, trips_sh_8: int, trips_ma_168: double, trips_ma_84: double, trips_ma_24: double, trips_ma_28: double, trips_ma_12: double, trips_ma_8: double, trips_sh_4: int, trips_ma_4: double, trips_sh_1: int, trips_ma_1: double, trips_ma_168_growth: double, trips_ma_8_growth: double, trips_ma_4_growth: double, trips_target: int]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663d6b9-90ea-4f36-aea7-1520d63a0d8c",
   "metadata": {},
   "source": [
    "### Фрагмент данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699374f-3d4b-4d28-b2fb-d9ddca03138c",
   "metadata": {},
   "source": [
    "Отберем несколько полей и один район для проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08662517-2f13-4728-bfde-a27c48d289a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-------------------+------------+------+-------+--------------------+\n",
      "|         ct| ca|         hour_start|trips_target|time_p|miles_p|          velocity_p|\n",
      "+-----------+---+-------------------+------------+------+-------+--------------------+\n",
      "|17031090200|  6|2021-01-08 01:00:00|           1|     0|    0.0|                 0.0|\n",
      "|17031090200|  6|2021-01-08 02:00:00|           2|   706|   3.24|0.004589235127478754|\n",
      "|17031090200|  6|2021-01-08 03:00:00|           1|   990|    2.9|0.002929292929292929|\n",
      "+-----------+---+-------------------+------------+------+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_to_sel = [\n",
    "    'ct', 'ca', 'hour_start', 'trips_target', 'time_p', 'miles_p', 'velocity_p',\n",
    "]\n",
    "\n",
    "data_sample = data.select(*f_to_sel).filter(f.col('ct')==17031090200)\n",
    "data_sample.show(3)\n",
    "# data_sample.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6b49f0f-b5ee-4c40-ad16-d5d0a279f357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time_p miles_p farem_p tipsm_p comp1_p comp3_p comp4_p trips_d time_d miles_d farem_d tipsm_d extrasm_d comp2_d comp3_d comp4_d extrasm_p comp5_p'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(excluded_fields_tot[(17031090200,6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b28493-6584-4311-bdec-5b8465c2c279",
   "metadata": {},
   "source": [
    "Данные загрузились. Общее количество и отображение фрагмента соответствуют ожиданиям. Исключаемые поля в доступе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a7c4ca1-18b9-4d14-a5ea-92a0da9f29bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "type(__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1741727f-02a6-4134-8afd-755a47201ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca4b95f1-be16-4b0b-82fc-035a4decb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_namespace=globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba9a1eb4-d00d-469b-8163-eeee1ad4b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(pds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6804276a-d749-4e79-8f57-2a4cbbca463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a DataFrame containing the names, types, and sizes of objects...\n",
      "objects count: 235'073\n",
      "total size: 41'895'129\n"
     ]
    }
   ],
   "source": [
    "ob = pds.inventory_objects(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b12f480-7bed-46b9-95e2-a87c2cfbb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pds.get_df_formated(ob,\"'\",0,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "580670ac-b4e4-4bbc-b14f-b31510714047",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_df = ob[ob['Type'].str.contains('DataFrame')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aef05a3a-8cd0-49d5-92a6-8eedb9ef73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_df = pds.get_df_formated(ob_df,\"'\",0,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5d5fa88-8eea-4282-8097-726a915888eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Name              Type Size (bytes)\n",
      "1            excl_f  Pandas DataFrame       98'876\n",
      "178291  data_sample   Spark DataFrame           56\n",
      "184383         data   Spark DataFrame           56\n"
     ]
    }
   ],
   "source": [
    "print(ob_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "647660e5-e93a-4da1-bd3c-e62a721cbafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41'895'129\n"
     ]
    }
   ],
   "source": [
    "print(pds.gvf(sum(ob['Size (bytes)'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "feb16679-3331-47ce-8fa6-cb1cd42462af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e27b1-a765-43c4-9406-06b99c337e15",
   "metadata": {},
   "source": [
    "## Подготовка данных к МО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc2d5694-b4e2-429c-a714-36769278f089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n",
      "CPU times: user 11.5 ms, sys: 1.45 ms, total: 13 ms\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pds.gvf(data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7437836-0bec-433c-b814-482e9b205b49",
   "metadata": {},
   "source": [
    "Векторизация признаков"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14537eb1-164d-4666-b843-32f5075cd0af",
   "metadata": {},
   "source": [
    "Некоторая информация связанная с векторизацией признаков.  \n",
    "Сохранить датафрейм со столбцом вектором возможно, например, в формате parquet; csv не поддерживает вектор-столбцы. Загруженный паркет-файл вызывает ошибку при выполнении действий.   \n",
    "При кэшировании векторизированного дф (с использованием уровня хранения Disk Memory Deserialized 1x Replicated - уровень, который присваивается по умолчанию при вызове .cache()), такой кэшированный датафрейм с вектор-столбцом не получается обрабатывать из-за нехватки памяти.  \n",
    "\n",
    "Пока остановился на варианте:  \n",
    "- делю входной дф на трэйн и тест, кэширую их .cache() (Disk Memory)\n",
    "- получаю фрагмент geo_df_base_train/test для гео-ключа из трэйн дф, кэширую в ОЗУ geo_df_base_train/test\n",
    "- векторизирую фичи geo_df_base_train/test,\n",
    "- создаю рабочий дф geo_df_train/test дф с нужными полями (гео-ключ, время, метка, фича-вектор)\n",
    "- кэширую в ОЗУ geo_df_train/test;\n",
    "- создаю для geo_df_train скалер\n",
    "- масштабирую-обновляю geo_df_train/test, кэширую в ОЗУ,\n",
    "- обучаю модель лин.регрессии на трэйне (включая тюнинг модели)\n",
    "- сохраняю в пандас дф для данного гео-ключа полученный объект-модель и метрики на трэйне\n",
    "- получаю предикт на трэйне и тесте и добавляю столбец предикта в исходные train/test по гео-ключу, обновляю их и кэширую .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d1318-f08d-49b5-879a-1016fe6a9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на тренировочный и тестовый наборы\n",
    "train = data.filter(f.year(f.col(\"hour_start\")) < 2024)\n",
    "test = data.filter(f.year(f.col(\"hour_start\")) == 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352c5e7-18ec-4556-a451-b2b589fb1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d71b798-1cf0-42c1-b006-3b201c85c73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2'686'518\n",
      "CPU times: user 6.02 ms, sys: 835 µs, total: 6.86 ms\n",
      "Wall time: 5.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pds.gvf(train.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7410b840-3f72-4747-91af-7d7e6620dcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375'525\n",
      "CPU times: user 3.43 ms, sys: 475 µs, total: 3.91 ms\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pds.gvf(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68cebea1-c64d-48e9-9fb5-d9f2c5dd4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable_name = 'trips_target'\n",
    "# исключаемые из расчетов ключевые поля для всех районов\n",
    "exc_cols = ['ct', 'ca', 'hour_start']\n",
    "# поля которые будут в рабочих ДФ\n",
    "# ['ct', 'ca', 'hour_start', 'trips_target', 'features']\n",
    "# 'features' - немасштабированный вектор-столбец\n",
    "selectedCols = exc_cols + [target_variable_name, 'features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06396d4b-64c9-4749-aebc-74f89e4ce7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение уникальных комбинаций районов и округов\n",
    "geo_keys = data.select('ct', 'ca').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014459d-d71e-4735-842a-5065a277b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаю фрагмент geo_df_base_train/test\n",
    "# для гео-ключа из трэйн/тест дф, кэширую\n",
    "# в ОЗУ geo_df_base_train/test\n",
    "def get_geo_df_base(point, train, test):\n",
    "    ct = point[0]\n",
    "    ca = point[1]\n",
    "    # Фильтрация данных для текущего района и округа\n",
    "    geo_data_base_train = train.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "    geo_data_base_test = test.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "\n",
    "    return geo_data_base_train, geo_data_base_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce3369e4-4a12-4b86-82fd-2ff2edff3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_df_in_ram(df):\n",
    "    df.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68bf10d8-2625-4c2a-b4ec-8106d7819855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# векторизация указанных полей датафрейма\n",
    "def assemble_vectors(df, features_list, selectedCols):\n",
    "    stages = []\n",
    "    assembler = VectorAssembler(inputCols=features_list, outputCol='features')\n",
    "    stages = [assembler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    # assemble model\n",
    "    assembleModel = pipeline.fit(df)\n",
    "    # apply assembler model on data\n",
    "    df = assembleModel.transform(df).select(selectedCols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c1fde410-8b3b-4d0e-91dc-83135ad080f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitted_scaler(df, featureCol, outputCol):\n",
    "    \n",
    "    stages = []\n",
    "    scaler = StandardScaler(inputCol = featureCol,\n",
    "                            outputCol=outputCol,\n",
    "                            withStd=True, withMean=True)\n",
    "    stages = [scaler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    scaledAssembleModel = pipeline.fit(df)\n",
    "\n",
    "    return scaledAssembleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e80836-a43c-48c4-b22a-46f5ec6ba6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = inventory_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f64a8b51-26b5-44d1-a4b8-3a81f60fcb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size (bytes)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(&lt;py4j.clientserver.JavaClient object at 0x7f...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>2926776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Name      Type  Size...</td>\n",
       "      <td>&lt;class 'pandas.core.frame.DataFrame'&gt;</td>\n",
       "      <td>980769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Name      Type  Size...</td>\n",
       "      <td>&lt;class 'pandas.core.frame.DataFrame'&gt;</td>\n",
       "      <td>980769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[&lt;built-in method match of re.Pattern object ...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>632824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0               RewriteSymbolics\n",
       "1            ...</td>\n",
       "      <td>&lt;class 'pandas.core.series.Series'&gt;</td>\n",
       "      <td>492319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0           type\n",
       "1           type\n",
       "2           ...</td>\n",
       "      <td>&lt;class 'pandas.core.series.Series'&gt;</td>\n",
       "      <td>432758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{2: [&lt;ast.Expr object at 0x7ff6379fc2b0&gt;, &lt;ast...</td>\n",
       "      <td>&lt;class 'collections.defaultdict'&gt;</td>\n",
       "      <td>147552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{18: [&lt;ast.Import object at 0x7ff636d27fd0&gt;, &lt;...</td>\n",
       "      <td>&lt;class 'collections.defaultdict'&gt;</td>\n",
       "      <td>147552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'sys': &lt;module 'sys' (built-in)&gt;, 'builtins':...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>103856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ct  ca excluded_field\n",
       "0     120...</td>\n",
       "      <td>&lt;class 'pandas.core.frame.DataFrame'&gt;</td>\n",
       "      <td>98876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{17: [&lt;ast.Expr object at 0x7ff61fc15300&gt;, &lt;as...</td>\n",
       "      <td>&lt;class 'collections.defaultdict'&gt;</td>\n",
       "      <td>73816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{94007612370720: &lt;weakref at 0x7ff684b80d10; t...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>73808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0, 2, 71, 143, 213, 287, 358, 414, 416, 464, ...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>47160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[#, # Licensed to the Apache Software Foundati...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>47160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[#\\n, # Licensed to the Apache Software Founda...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>47160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[#\\n, # core.py\\n, #\\n, import os\\n, import ty...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>47160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{2943: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>36952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{(&lt;function Artist.get_agg_filter at 0x7ff632c...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>36952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{1775: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>36952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[# -*- coding: utf-8 -*-\\n, \"\"\"Main IPython cl...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>33048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[# -*- coding: utf-8 -*-, \"\"\"Main IPython clas...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>33048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{Indian/Mayotte, Indian/Mahe, Asia/Jerusalem, ...</td>\n",
       "      <td>&lt;class 'pytz.lazy.LazySet.__new__.&lt;locals&gt;.Laz...</td>\n",
       "      <td>33000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{HST, Asia/Thimbu, Brazil/Acre, Indian/Mayotte...</td>\n",
       "      <td>&lt;class 'pytz.lazy.LazySet.__new__.&lt;locals&gt;.Laz...</td>\n",
       "      <td>33000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{urllib3, anyio-3.7.1.dist-info, conda_package...</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>32984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{&lt;method 'strip' of 'bytes' objects&gt;, &lt;functio...</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>32984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{_strptime, pygments.console, pygments.modelin...</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>32984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'xkcd:cloudy blue': '#acc2d9', 'xkcd:dark pas...</td>\n",
       "      <td>&lt;class 'matplotlib.colors._ColorMapping'&gt;</td>\n",
       "      <td>26056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[#!/usr/bin/env python3\\n, \"\"\"Generate Python ...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>23128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[#\\n, # Licensed to the Apache Software Founda...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>20536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[0, 59, 61, 133, 200, 273, 339, 400, 469, 531,...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>20536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[# Copyright 2001-2019 by Vinay Sajip. All Rig...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>20536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[# Copyright 2001-2019 by Vinay Sajip. All Rig...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>20536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{(&lt;class 'str'&gt;, '\\([^\\(]*$', 32): re.compile(...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>18512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>{(&lt;function Artist.set_agg_filter at 0x7ff632c...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>18512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>re.compile('var\\\\(--([a-zA-Z0-9_\\\\-\\\\u00B7\\\\u0...</td>\n",
       "      <td>&lt;class 're.Pattern'&gt;</td>\n",
       "      <td>18268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{newbuffer, allclose, log, mod, isrealobj, arr...</td>\n",
       "      <td>&lt;class 'set'&gt;</td>\n",
       "      <td>16600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[\"\"\"Base implementation of event loop.\\n, \\n, ...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>16184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[#\\n, # Licensed to the Apache Software Founda...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>14360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>re.compile('[A-Z_a-zªµºÀ-ÖØ-öø-ˁˆ-ˑˠ-ˤˬˮͰ-ʹͶ-ͷ...</td>\n",
       "      <td>&lt;class 're.Pattern'&gt;</td>\n",
       "      <td>13952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[#\\n, # Licensed to the Apache Software Founda...</td>\n",
       "      <td>&lt;class 'list'&gt;</td>\n",
       "      <td>13528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>{'__name__': 'os', '__doc__': 'OS routines for...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>13056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>{'__name__': 'scipy', '__doc__': '\n",
       "SciPy: A sc...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>13056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>{'__name__': 'numpy.core.numeric', '__doc__': ...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>13056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>{'__name__': 'scipy.stats', '__doc__': '\n",
       ".. _s...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>13056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>{'_generate_next_value_': &lt;function Enum._gene...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>13056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>{'__name__': 'scipy.linalg._flapack', '__doc__...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>13056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>{'__name__': 'scipy.linalg._flapack', '__doc__...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>13056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'__name__': 'pyarrow.compute', '__doc__': Non...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>13056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>{'__name__': 'numpy', '__doc__': '\n",
       "NumPy\n",
       "=====...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>13056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>{'__name__': 'pyarrow.lib', '__doc__': None, '...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>13056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0   [(<py4j.clientserver.JavaClient object at 0x7f...   \n",
       "1                             Name      Type  Size...   \n",
       "2                             Name      Type  Size...   \n",
       "3   [[<built-in method match of re.Pattern object ...   \n",
       "4   0               RewriteSymbolics\n",
       "1            ...   \n",
       "5   0           type\n",
       "1           type\n",
       "2           ...   \n",
       "6   {2: [<ast.Expr object at 0x7ff6379fc2b0>, <ast...   \n",
       "7   {18: [<ast.Import object at 0x7ff636d27fd0>, <...   \n",
       "8   {'sys': <module 'sys' (built-in)>, 'builtins':...   \n",
       "9                  ct  ca excluded_field\n",
       "0     120...   \n",
       "10  {17: [<ast.Expr object at 0x7ff61fc15300>, <as...   \n",
       "11  {94007612370720: <weakref at 0x7ff684b80d10; t...   \n",
       "12  [0, 2, 71, 143, 213, 287, 358, 414, 416, 464, ...   \n",
       "13  [#, # Licensed to the Apache Software Foundati...   \n",
       "14  [#\\n, # Licensed to the Apache Software Founda...   \n",
       "15  [#\\n, # core.py\\n, #\\n, import os\\n, import ty...   \n",
       "16  {2943: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...   \n",
       "17  {(<function Artist.get_agg_filter at 0x7ff632c...   \n",
       "18  {1775: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, ...   \n",
       "19  [# -*- coding: utf-8 -*-\\n, \"\"\"Main IPython cl...   \n",
       "20  [# -*- coding: utf-8 -*-, \"\"\"Main IPython clas...   \n",
       "21  {Indian/Mayotte, Indian/Mahe, Asia/Jerusalem, ...   \n",
       "22  {HST, Asia/Thimbu, Brazil/Acre, Indian/Mayotte...   \n",
       "23  {urllib3, anyio-3.7.1.dist-info, conda_package...   \n",
       "24  {<method 'strip' of 'bytes' objects>, <functio...   \n",
       "25  {_strptime, pygments.console, pygments.modelin...   \n",
       "26  {'xkcd:cloudy blue': '#acc2d9', 'xkcd:dark pas...   \n",
       "27  [#!/usr/bin/env python3\\n, \"\"\"Generate Python ...   \n",
       "28  [#\\n, # Licensed to the Apache Software Founda...   \n",
       "29  [0, 59, 61, 133, 200, 273, 339, 400, 469, 531,...   \n",
       "30  [# Copyright 2001-2019 by Vinay Sajip. All Rig...   \n",
       "31  [# Copyright 2001-2019 by Vinay Sajip. All Rig...   \n",
       "32  {(<class 'str'>, '\\([^\\(]*$', 32): re.compile(...   \n",
       "33  {(<function Artist.set_agg_filter at 0x7ff632c...   \n",
       "34  re.compile('var\\\\(--([a-zA-Z0-9_\\\\-\\\\u00B7\\\\u0...   \n",
       "35  {newbuffer, allclose, log, mod, isrealobj, arr...   \n",
       "36  [\"\"\"Base implementation of event loop.\\n, \\n, ...   \n",
       "37  [#\\n, # Licensed to the Apache Software Founda...   \n",
       "38  re.compile('[A-Z_a-zªµºÀ-ÖØ-öø-ˁˆ-ˑˠ-ˤˬˮͰ-ʹͶ-ͷ...   \n",
       "39  [#\\n, # Licensed to the Apache Software Founda...   \n",
       "40  {'__name__': 'os', '__doc__': 'OS routines for...   \n",
       "41  {'__name__': 'scipy', '__doc__': '\n",
       "SciPy: A sc...   \n",
       "42  {'__name__': 'numpy.core.numeric', '__doc__': ...   \n",
       "43  {'__name__': 'scipy.stats', '__doc__': '\n",
       ".. _s...   \n",
       "44  {'_generate_next_value_': <function Enum._gene...   \n",
       "45  {'__name__': 'scipy.linalg._flapack', '__doc__...   \n",
       "46  {'__name__': 'scipy.linalg._flapack', '__doc__...   \n",
       "47  {'__name__': 'pyarrow.compute', '__doc__': Non...   \n",
       "48  {'__name__': 'numpy', '__doc__': '\n",
       "NumPy\n",
       "=====...   \n",
       "49  {'__name__': 'pyarrow.lib', '__doc__': None, '...   \n",
       "\n",
       "                                                 Type  Size (bytes)  \n",
       "0                                      <class 'list'>       2926776  \n",
       "1               <class 'pandas.core.frame.DataFrame'>        980769  \n",
       "2               <class 'pandas.core.frame.DataFrame'>        980769  \n",
       "3                                      <class 'list'>        632824  \n",
       "4                 <class 'pandas.core.series.Series'>        492319  \n",
       "5                 <class 'pandas.core.series.Series'>        432758  \n",
       "6                   <class 'collections.defaultdict'>        147552  \n",
       "7                   <class 'collections.defaultdict'>        147552  \n",
       "8                                      <class 'dict'>        103856  \n",
       "9               <class 'pandas.core.frame.DataFrame'>         98876  \n",
       "10                  <class 'collections.defaultdict'>         73816  \n",
       "11                                     <class 'dict'>         73808  \n",
       "12                                     <class 'list'>         47160  \n",
       "13                                     <class 'list'>         47160  \n",
       "14                                     <class 'list'>         47160  \n",
       "15                                     <class 'list'>         47160  \n",
       "16                                     <class 'dict'>         36952  \n",
       "17                                     <class 'dict'>         36952  \n",
       "18                                     <class 'dict'>         36952  \n",
       "19                                     <class 'list'>         33048  \n",
       "20                                     <class 'list'>         33048  \n",
       "21  <class 'pytz.lazy.LazySet.__new__.<locals>.Laz...         33000  \n",
       "22  <class 'pytz.lazy.LazySet.__new__.<locals>.Laz...         33000  \n",
       "23                                      <class 'set'>         32984  \n",
       "24                                      <class 'set'>         32984  \n",
       "25                                      <class 'set'>         32984  \n",
       "26          <class 'matplotlib.colors._ColorMapping'>         26056  \n",
       "27                                     <class 'list'>         23128  \n",
       "28                                     <class 'list'>         20536  \n",
       "29                                     <class 'list'>         20536  \n",
       "30                                     <class 'list'>         20536  \n",
       "31                                     <class 'list'>         20536  \n",
       "32                                     <class 'dict'>         18512  \n",
       "33                                     <class 'dict'>         18512  \n",
       "34                               <class 're.Pattern'>         18268  \n",
       "35                                      <class 'set'>         16600  \n",
       "36                                     <class 'list'>         16184  \n",
       "37                                     <class 'list'>         14360  \n",
       "38                               <class 're.Pattern'>         13952  \n",
       "39                                     <class 'list'>         13528  \n",
       "40                                     <class 'dict'>         13056  \n",
       "41                                     <class 'dict'>         13056  \n",
       "42                                     <class 'dict'>         13056  \n",
       "43                                     <class 'dict'>         13056  \n",
       "44                                     <class 'dict'>         13056  \n",
       "45                                     <class 'dict'>         13056  \n",
       "46                                     <class 'dict'>         13056  \n",
       "47                                     <class 'dict'>         13056  \n",
       "48                                     <class 'dict'>         13056  \n",
       "49                                     <class 'dict'>         13056  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b3ecf218-44b5-4e51-abed-68e24650d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inventory = pds.inventory_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "60cfe706-2161-4124-ab4d-bf23e9c74cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: flex; justify-content: space-between;\"><div style=\"flex: 1; padding-right: 10px;\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size (bytes)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>HTML</td>\n",
       "      <td>type</td>\n",
       "      <td>1688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>display</td>\n",
       "      <td>function</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>get_column_types</td>\n",
       "      <td>function</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>get_df_formated</td>\n",
       "      <td>function</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gvf</td>\n",
       "      <td>function</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>inventory_objects</td>\n",
       "      <td>function</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>display_chunked</td>\n",
       "      <td>function</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pd</td>\n",
       "      <td>module</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sys</td>\n",
       "      <td>module</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div><div style=\"flex: 1; padding-left: 10px;\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size (bytes)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying rows 0 to 8 of 9\n"
     ]
    }
   ],
   "source": [
    "pds.display_chunked(df_inventory, chunk_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ba1eb00-8d9d-489f-a20e-9bedb9694c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cached DataFrames\n",
    "cached_dataframes = spark.catalog.listTables()\n",
    "\n",
    "# Print the cached DataFrames and their caching level\n",
    "for dataframe in cached_dataframes:\n",
    "    print(\"DataFrame Name:\", dataframe.name)\n",
    "    print(\"Caching Level:\", spark.catalog.isCached(dataframe.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b6c1153-eef1-4db5-8c1d-65dd3f6116d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Catalog' object has no attribute 'listCacheTables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cached_dataframes \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistCacheTables\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Catalog' object has no attribute 'listCacheTables'"
     ]
    }
   ],
   "source": [
    "cached_dataframes = spark.catalog.listCacheTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8abaa690-8d3e-4304-bd64-0a132e32a846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b64598be-6517-4b6f-99b8-1435d16fda37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "703b3a6e-3b95-49a7-bfb0-09d6e4f7d44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, True, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c641bf3-506b-4cd3-805f-6a64319e2c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3ba663a7-34b1-483f-ae99-5039e0c8f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in tqdm(geo_keys):\n",
    "    \n",
    "#     ct = row['ct']\n",
    "#     ca = row['ca']\n",
    "    \n",
    "def lin_regr_with_scale(train, test, point):\n",
    "\n",
    "    ct = point[0]\n",
    "    ca = point[1]\n",
    "    geo_train = train.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "    geo_train.cache()\n",
    "    geo_test = test.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "    geo_test.cache()\n",
    "\n",
    "    \n",
    "    excluded_fields_list = excluded_fields_tot[(ct,ca)] + exc_cols\n",
    "    features_list = [col for col in data.columns if col not in excluded_fields_list]\n",
    "    scaled_features_list = [f\"{col}_scaled\" for col in features_list]\n",
    "\n",
    "    # масштабируем в лоб, поскольку работаем не с ветор-столбцом\n",
    "    mean_and_std_cols=[c for col in features_list for c in \n",
    "    (f.mean(col).alias(f\"{col}_mean\"),f.stddev(col).alias(f\"{col}_std\"))]\n",
    "    \n",
    "    mean_and_std = geo_train.select(mean_and_std_cols).first()\n",
    "    scaled_cols=[((f.col(col) - mean_and_std[f\"{col}_mean\"])\n",
    "        /mean_and_std[f\"{col}_std\"]).alias(f\"{col}_scaled\") for col in features_list]\n",
    "    scaled_geo_train = geo_train.select(geo_train.columns + scaled_cols)\n",
    "    scaled_geo_test = geo_test.select(geo_test.columns + scaled_cols)\n",
    "    \n",
    "    selectedCols = exc_cols + [target_variable_name] + scaled_features_list\n",
    "\n",
    "    # for feat in features_list:\n",
    "    #     scaled_feat = f\"{feat}_scaled\"\n",
    "    #     geo_scaler = fitted_scaler(geo_train, feat, scaled_feat)\n",
    "    #     scaled_geo_train = geo_scaler.transform(geo_train)\n",
    "    #     scaled_geo_test = geo_scaler.transform(geo_test)\n",
    "\n",
    "    scaled_geo_train = scaled_geo_train.select(selectedCols)\n",
    "    scaled_geo_test = scaled_geo_test.select(selectedCols)\n",
    "    \n",
    "    reg = LinearRegression(featuresCol = scaled_features_list, labelCol = target_variable_name)\n",
    "\n",
    "    reg_model = reg.fit(scaled_geo_train)\n",
    "    \n",
    "    # Создание объекта RegressionEvaluator для оценки модели\n",
    "    evaluator = RegressionEvaluator()\n",
    "\n",
    "    # Настройка параметров оценки (RMSE и MAPE)\n",
    "    evaluator.setMetricName(\"rmse\")\n",
    "    evaluator.setPredictionCol(\"prediction\")\n",
    "    evaluator.setLabelCol(\"label\")\n",
    "\n",
    "    # Оценка модели\n",
    "    rmse = evaluator.evaluate(reg_model.transform(scaled_geo_train))\n",
    "    \n",
    "    # Аналогично для MAPE\n",
    "    evaluator.setMetricName(\"mape\")\n",
    "    mape = evaluator.evaluate(reg_model.transform(scaled_geo_train))\n",
    "\n",
    "    return reg_model, scaled_geo_train, rmse, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab01cde6-17a9-48f6-9287-f53b885bf601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # векторизация\n",
    "# df_vec = None\n",
    "# for row in tqdm(geo_keys):\n",
    "    \n",
    "#     ct = row['ct']\n",
    "#     ca = row['ca']\n",
    "#     # Фильтрация данных для текущего района и округа\n",
    "#     geo_data = data.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "#     # print(row,geo_data.count())\n",
    "#     excluded_fields_list = excluded_fields_tot[row] + exc_cols\n",
    "#     features_list = [col for col in data.columns if col not in excluded_fields_list]\n",
    "#     # print(features_list)\n",
    "#     assembled_data = assemble_vectors(df=geo_data,\n",
    "#                                 features_list=features_list,\n",
    "                                \n",
    "#                                 target_variable_name='trips_target')\n",
    "#     # print(assembled_data.count())\n",
    "#     # Если это первая итерация, инициализируем df_vec\n",
    "#     if df_vec is None:\n",
    "#         df_vec = assembled_data\n",
    "#     else:\n",
    "#         df_vec = df_vec.union(assembled_data)\n",
    "#         # print(df_vec.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17ac098d-ef32-4628-a70a-145f48e72d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание нового пустого DataFrame с той же схемой\n",
    "# empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), geo_data.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2ab4ab9-7a78-41bc-9e63-dc193edc6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vec.show(3)\n",
    "# df_vec.cache()\n",
    "# print(pds.gvf(df_vec.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b50e09af-ce57-44b0-9fc3-ea3330971b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_to_sel = [\n",
    "#     'ct', 'ca', 'hour_start', 'trips_target', 'features',\n",
    "# ]\n",
    "\n",
    "# df_vec.select(*f_to_sel).filter(f.col('ct')==17031838200).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b1511a2-9748-4b09-a014-c85268486732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if df_vec.is_cached:\n",
    "#  print(\"DataFrame закэширован\")\n",
    "# else:\n",
    "#  print(\"DataFrame не закэширован\")\n",
    "\n",
    "# if df_vec.storageLevel == StorageLevel.MEMORY_ONLY:\n",
    "#  print(\"DataFrame закэширован в памяти\")\n",
    "# elif df_vec.storageLevel == StorageLevel.DISK_ONLY:\n",
    "#  print(\"DataFrame закэширован на диске\")\n",
    "# else:\n",
    " # print(\"DataFrame закэширован в:\",df_vec.storageLevel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfac8fae-4225-44ec-ad09-3d174e3ba4ed",
   "metadata": {},
   "source": [
    "данные DataFrame были закэшированы с использованием уровня хранения Disk Memory Deserialized 1x Replicated. Это значит, что данные сначала сериализуются и сохраняются на диске, а затем десериализуются в память для обработки. Также данные были реплицированы один раз, что обеспечивает дополнительную надежность и доступность данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f24b584a-9310-40e8-ba2e-991a304a8157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n",
      "CPU times: user 38.3 ms, sys: 9.54 ms, total: 47.8 ms\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(pds.gvf(df_vec.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a92740d9-8b6c-47ec-88d6-870e9961eb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n",
      "CPU times: user 10.7 ms, sys: 83 µs, total: 10.7 ms\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(pds.gvf(data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8da93d-bcd7-4038-bc56-c303540c2f3e",
   "metadata": {},
   "source": [
    "На данный момент непонятна выгода от векторизации. При более медленном выполнении действий, нет пока понятного способа корректно сохранить-загрузить веторизированный ДФ. В csv нельзя сохранить тип vector, в parquet - можно, но после загрузки выдаются ошибки работы JVM объектов.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e017dad-a9ec-4543-ad44-e2070723e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vec.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19a1ac-4f58-4a44-9357-4b7350b64e16",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc71bd-1391-4ff9-a8a5-a3c795b7ee9e",
   "metadata": {},
   "source": [
    "Разделю ДФ на обучающую и тестовую выборки: тест - 2024 год, трэйн - 2021-2023 гг. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8cee04c1-f12a-4d4e-a7d8-b735bfc92e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на тренировочный и тестовый наборы\n",
    "train = data.filter(f.year(f.col(\"hour_start\")) < 2024)\n",
    "test = data.filter(f.year(f.col(\"hour_start\")) == 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f68a087f-4677-4983-a7d7-9541252f5e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2'686'518\n",
      "375'525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ct: bigint, ca: int, hour_start: timestamp, time_p: int, miles_p: double, velocity_p: double, farem_p: double, tipsm_p: double, tollsm_p: double, extrasm_p: double, totalm_p: double, comp1_p: int, comp2_p: int, comp3_p: int, comp4_p: int, comp5_p: int, compless5_p: int, trips_d: int, time_d: int, miles_d: double, velocity_d: double, farem_d: double, tipsm_d: double, tollsm_d: double, extrasm_d: double, totalm_d: double, comp1_d: int, comp2_d: int, comp3_d: int, comp4_d: int, comp5_d: int, compless5_d: int, cumulative_balance: int, trips_p_growth_1_to_0: double, trips_p_growth_2_to_1: double, trips_p_growth_3_to_2: double, trips_p_growth_4_to_3: double, trips_d_growth_1_to_0: double, trips_d_growth_2_to_1: double, trips_d_growth_3_to_2: double, trips_d_growth_4_to_3: double, velocity_p_growth_1_to_0: double, velocity_p_growth_2_to_1: double, velocity_p_growth_3_to_2: double, velocity_p_growth_4_to_3: double, velocity_d_growth_1_to_0: double, velocity_d_growth_2_to_1: double, velocity_d_growth_3_to_2: double, velocity_d_growth_4_to_3: double, trips_sh_168: int, trips_sh_84: int, trips_sh_24: int, trips_sh_28: int, trips_sh_12: int, trips_sh_8: int, trips_ma_168: double, trips_ma_84: double, trips_ma_24: double, trips_ma_28: double, trips_ma_12: double, trips_ma_8: double, trips_sh_4: int, trips_ma_4: double, trips_sh_1: int, trips_ma_1: double, trips_ma_168_growth: double, trips_ma_8_growth: double, trips_ma_4_growth: double, trips_target: int]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pds.gvf(train.count()))\n",
    "train.cache()\n",
    "print(pds.gvf(test.count()))\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa243414-d169-41a5-92da-b037ea7e4ba6",
   "metadata": {},
   "source": [
    "Масштабирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "347d0590-a2d7-490f-a81f-d7428aa6f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение уникальных комбинаций районов и округов\n",
    "geo_keys = data.select('ct', 'ca').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc8e6f2c-b4b4-4d1b-9628-e0b36c92809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# исключаемые поля для всех районов\n",
    "exc_cols = ['ct', 'ca', 'hour_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b7c0c08d-6d62-4dbf-ba3f-5a5ac0cee8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = data.columns\n",
    "target_variable_name = 'trips_target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f49d2c7f-4ad2-42a1-a26c-e1854792024d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time_p miles_p farem_p tipsm_p comp1_p comp3_p comp4_p trips_d time_d miles_d farem_d tipsm_d extrasm_d comp2_d comp3_d comp4_d extrasm_p comp5_p'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_point = (17031090200,6)\n",
    "\" \".join(excluded_fields_tot[test_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "09611c50-a2ed-4cae-b243-5a8ebb3a011e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17031090200"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_point[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fdf67b9b-0eb8-4ec2-b85a-06c0b7b8a335",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_and_std' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m mean_and_std_cols\u001b[38;5;241m=\u001b[39m[c \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m features_list \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m      8\u001b[0m     (f\u001b[38;5;241m.\u001b[39mmean(col)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m),f\u001b[38;5;241m.\u001b[39mstddev(col)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_std\u001b[39m\u001b[38;5;124m\"\u001b[39m))]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# mean_and_std_cols\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# mean_and_std = test_df.select(mean_and_std_cols).first()\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m scaled_cols\u001b[38;5;241m=\u001b[39m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean_and_std\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mmean_and_std\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_std\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_s\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_list\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# test_df = test_df.select(test_df.columns + scaled_cols)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m scaled_cols\n",
      "Cell \u001b[0;32mIn[93], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m mean_and_std_cols\u001b[38;5;241m=\u001b[39m[c \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m features_list \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m      8\u001b[0m     (f\u001b[38;5;241m.\u001b[39mmean(col)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m),f\u001b[38;5;241m.\u001b[39mstddev(col)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_std\u001b[39m\u001b[38;5;124m\"\u001b[39m))]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# mean_and_std_cols\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# mean_and_std = test_df.select(mean_and_std_cols).first()\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m scaled_cols\u001b[38;5;241m=\u001b[39m[((f\u001b[38;5;241m.\u001b[39mcol(col) \u001b[38;5;241m-\u001b[39m \u001b[43mmean_and_std\u001b[49m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m/\u001b[39mmean_and_std[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_std\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_s\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m features_list]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# test_df = test_df.select(test_df.columns + scaled_cols)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m scaled_cols\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_and_std' is not defined"
     ]
    }
   ],
   "source": [
    "ct, ca = test_point\n",
    "\n",
    "excluded_fields_list = excluded_fields_tot[(ct,ca)] + exc_cols\n",
    "features_list = [col for col in data.columns if col not in excluded_fields_list]\n",
    "scaled_features_list = [f\"{col}_scaled\" for col in features_list]\n",
    "\n",
    "mean_and_std_cols=[c for col in features_list for c in \n",
    "    (f.mean(col).alias(f\"{col}_mean\"),f.stddev(col).alias(f\"{col}_std\"))]\n",
    "\n",
    "# mean_and_std_cols\n",
    "\n",
    "# mean_and_std = test_df.select(mean_and_std_cols).first()\n",
    "scaled_cols=[((f.col(col) - mean_and_std[f\"{col}_mean\"])\n",
    "    /mean_and_std[f\"{col}_std\"]).alias(f\"{col}_scaled\") for col in features_list]\n",
    "# test_df = test_df.select(test_df.columns + scaled_cols)\n",
    "scaled_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "17ebb4e7-57b3-4d10-928d-4b8c8f52e514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"featuresCol\". Could not convert <class 'list'> to string type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:503\u001b[0m, in \u001b[0;36mParams._set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypeConverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:236\u001b[0m, in \u001b[0;36mTypeConverters.toString\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to string type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(value))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert <class 'list'> to string type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reg_model, scaled_geo_train, rmse, mape \u001b[38;5;241m=\u001b[39m \u001b[43mlin_regr_with_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# scaled_geo_train = lin_regr_with_scale(train, test, test_point)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[98], line 41\u001b[0m, in \u001b[0;36mlin_regr_with_scale\u001b[0;34m(train, test, point)\u001b[0m\n\u001b[1;32m     38\u001b[0m scaled_geo_train \u001b[38;5;241m=\u001b[39m scaled_geo_train\u001b[38;5;241m.\u001b[39mselect(selectedCols)\n\u001b[1;32m     39\u001b[0m scaled_geo_test \u001b[38;5;241m=\u001b[39m scaled_geo_test\u001b[38;5;241m.\u001b[39mselect(selectedCols)\n\u001b[0;32m---> 41\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaled_features_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_variable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m reg_model \u001b[38;5;241m=\u001b[39m reg\u001b[38;5;241m.\u001b[39mfit(scaled_geo_train)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Создание объекта RegressionEvaluator для оценки модели\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/regression.py:332\u001b[0m, in \u001b[0;36mLinearRegression.__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.regression.LinearRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid\n\u001b[1;32m    330\u001b[0m )\n\u001b[1;32m    331\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetParams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/regression.py:363\u001b[0m, in \u001b[0;36mLinearRegression.setParams\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03msetParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m          maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03mSets params for linear regression.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    362\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:505\u001b[0m, in \u001b[0;36mParams._set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             value \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mtypeConverter(value)\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 505\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid param value given for param \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (p\u001b[38;5;241m.\u001b[39mname, e))\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paramMap[p] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid param value given for param \"featuresCol\". Could not convert <class 'list'> to string type"
     ]
    }
   ],
   "source": [
    "reg_model, scaled_geo_train, rmse, mape = lin_regr_with_scale(train, test, test_point)\n",
    "# scaled_geo_train = lin_regr_with_scale(train, test, test_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "97b0848e-7bed-47f4-ae8a-dc6fc9de4749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ct ca hour_start time_p miles_p velocity_p farem_p tipsm_p tollsm_p extrasm_p totalm_p comp1_p comp2_p comp3_p comp4_p comp5_p compless5_p trips_d time_d miles_d velocity_d farem_d tipsm_d tollsm_d extrasm_d totalm_d comp1_d comp2_d comp3_d comp4_d comp5_d compless5_d cumulative_balance trips_p_growth_1_to_0 trips_p_growth_2_to_1 trips_p_growth_3_to_2 trips_p_growth_4_to_3 trips_d_growth_1_to_0 trips_d_growth_2_to_1 trips_d_growth_3_to_2 trips_d_growth_4_to_3 velocity_p_growth_1_to_0 velocity_p_growth_2_to_1 velocity_p_growth_3_to_2 velocity_p_growth_4_to_3 velocity_d_growth_1_to_0 velocity_d_growth_2_to_1 velocity_d_growth_3_to_2 velocity_d_growth_4_to_3 trips_sh_168 trips_sh_84 trips_sh_24 trips_sh_28 trips_sh_12 trips_sh_8 trips_ma_168 trips_ma_84 trips_ma_24 trips_ma_28 trips_ma_12 trips_ma_8 trips_sh_4 trips_ma_4 trips_sh_1 trips_ma_1 trips_ma_168_growth trips_ma_8_growth trips_ma_4_growth trips_target velocity_p_scaled tollsm_p_scaled totalm_p_scaled comp2_p_scaled compless5_p_scaled velocity_d_scaled tollsm_d_scaled totalm_d_scaled comp1_d_scaled comp5_d_scaled compless5_d_scaled cumulative_balance_scaled trips_p_growth_1_to_0_scaled trips_p_growth_2_to_1_scaled trips_p_growth_3_to_2_scaled trips_p_growth_4_to_3_scaled trips_d_growth_1_to_0_scaled trips_d_growth_2_to_1_scaled trips_d_growth_3_to_2_scaled trips_d_growth_4_to_3_scaled velocity_p_growth_1_to_0_scaled velocity_p_growth_2_to_1_scaled velocity_p_growth_3_to_2_scaled velocity_p_growth_4_to_3_scaled velocity_d_growth_1_to_0_scaled velocity_d_growth_2_to_1_scaled velocity_d_growth_3_to_2_scaled velocity_d_growth_4_to_3_scaled trips_sh_168_scaled trips_sh_84_scaled trips_sh_24_scaled trips_sh_28_scaled trips_sh_12_scaled trips_sh_8_scaled trips_ma_168_scaled trips_ma_84_scaled trips_ma_24_scaled trips_ma_28_scaled trips_ma_12_scaled trips_ma_8_scaled trips_sh_4_scaled trips_ma_4_scaled trips_sh_1_scaled trips_ma_1_scaled trips_ma_168_growth_scaled trips_ma_8_growth_scaled trips_ma_4_growth_scaled trips_target_scaled'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(scaled_geo_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc7fb8-f9d9-4797-977f-c5f7fbbd0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_geo_test = geo_scaler.transform(geo_test).select(selectedCols)\n",
    "\n",
    "#     # Если это первая итерация, инициализируем df_scaled_\n",
    "#     if df_scaled_train is None:\n",
    "#         df_scaled_train = scaled_geo_train\n",
    "#     else:\n",
    "#         df_scaled_train = df_scaled_train.union(scaled_geo_train)\n",
    "        \n",
    "#     if df_scaled_test is None:\n",
    "#         df_scaled_test = scaled_geo_test\n",
    "#     else:\n",
    "#         df_scaled_test = df_scaled_test.union(scaled_geo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "215ddd75-0773-413d-ae72-8c36e37bd86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# масштабирование\n",
    "# df_scaled_train = None\n",
    "# df_scaled_test = None\n",
    "# for row in tqdm(geo_keys):\n",
    "    \n",
    "#     ct = row['ct']\n",
    "#     ca = row['ca']\n",
    "#     # Фильтрация данных для текущего района и округа\n",
    "#     geo_train = train.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "#     geo_test = test.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "    \n",
    "#     # print(features_list)\n",
    "#      # select all the columns + target + newly created 'features' column\n",
    "#     geo_scaler = fitted_scaler(geo_train, target_variable_name)\n",
    "#     scaled_geo_train = geo_scaler.transform(geo_train).select(selectedCols)\n",
    "#     scaled_geo_test = geo_scaler.transform(geo_test).select(selectedCols)\n",
    "    \n",
    "#     # print(assembled_data.count())\n",
    "#     # Если это первая итерация, инициализируем df_scaled_\n",
    "#     if df_scaled_train is None:\n",
    "#         df_scaled_train = scaled_geo_train\n",
    "#     else:\n",
    "#         df_scaled_train = df_scaled_train.union(scaled_geo_train)\n",
    "        \n",
    "#     if df_scaled_test is None:\n",
    "#         df_scaled_test = scaled_geo_test\n",
    "#     else:\n",
    "#         df_scaled_test = df_scaled_test.union(scaled_geo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e812d-24f3-45ff-8aea-5ff17fb839a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521e4cee-02df-434b-a30b-1d0010502be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled_train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981750c-a9fe-49a8-a9fa-3e655ae6dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea99e463-9360-473c-9bd5-ebb92186dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38acdf-e6d0-46fc-9e11-6565f85e6b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
