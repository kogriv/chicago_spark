{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ac4a7a-4644-45d3-8786-944ff88f222a",
   "metadata": {},
   "source": [
    "Ноутбук по проекту chicago_spark.  \n",
    "МО"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de64cacf-4b21-4fc7-85ca-0d7b7de73885",
   "metadata": {},
   "source": [
    "## Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f9b287-4613-4f96-9125-2472424b9cfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c57728-71d4-43e9-a453-c5d23d6a57a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.signal import welch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efbb9443-57d6-4aab-ad7d-c67719e98e03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from itertools import islice\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04534c3-1092-4cb4-9ed0-2a6b99acbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c506d768-519f-49a0-9af6-b44cbcf036bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enviserv.dictan import DictAnalyzer # анализ словарей\n",
    "import pandserv as pds # форматирование небольших пандас ДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c22c0b8-1ea3-417e-8289-34ca57326d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkserv import SparkApp, Cols\n",
    "# в SparkApp упакованы функции создания спарк приложения \n",
    "# с определением IP мастер-ноды и с подключением к кластеру\n",
    "\n",
    "# Col - класс для формирования коротких псевдонимов имен столбцов\n",
    "# при этом исходные имена полей не меняются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63596c0d-9087-4e1a-80e3-a1fd7a90babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import DataFrame as pydf\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db2ec96-2997-458c-b67a-f9e5dd6aa586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# гео библиотеки\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e47d77-cebf-4472-99e9-c2e852867df5",
   "metadata": {},
   "source": [
    "## Создание сессии, загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6c292a0-9c16-44d8-8aa6-e5a51b5e08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:spark_master_ip: 172.18.0.2\n",
      "INFO:spark_app:pyspark version: 3.4.1\n",
      "INFO:spark_app:starting building spark app object: pyspark-taxi-forecasting\n",
      "INFO:spark_app:Spark app object built as: <pyspark.sql.session.SparkSession object at 0x7fd4b7010f50>\n",
      "INFO:spark_app:==================================================================\n",
      "INFO:spark_app:Spark object can be accessed as the SparkApp_object.spark property\n",
      "INFO:spark_app:==================================================================\n"
     ]
    }
   ],
   "source": [
    "spark_app = SparkApp(my_logger_create_level = 'INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de973af-73c4-43f8-90b6-b1d422a3360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:spark_master_ip: 172.18.0.2\n"
     ]
    }
   ],
   "source": [
    "spark_master_ip = spark_app.get_spark_master_ip()\n",
    "# print(spark_master_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55a91437-60e9-4bc2-a9cb-58365f000efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = spark_app.build_spark_app(spark_master_ip=spark_master_ip)\n",
    "# spark = spark_app.spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c7d30-9eed-479c-b899-2fcd6c2d78d4",
   "metadata": {},
   "source": [
    "Для корректного завершения спарк-сессии (например, для переключения между ноутбуками) следует останавливать сессию полностью. Для этого использую метод .stop_spark_app() класса SparkApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "412582e5-44f8-44dd-a8f0-7d9fe46ad1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_app.stop_spark_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b17caf8-7f00-4c20-a83b-744c2d0ab81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fd4b7010f50>\n"
     ]
    }
   ],
   "source": [
    "print(spark_app.spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5221d404-34f2-4fd8-8b0c-531fd6f93bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:pyspark version: 3.4.1\n",
      "INFO:spark_app:starting building spark app object: pyspark-taxi-forecasting\n",
      "INFO:spark_app:Spark app object built as: <pyspark.sql.session.SparkSession object at 0x7fd4b7010f50>\n",
      "INFO:spark_app:==================================================================\n",
      "INFO:spark_app:Spark object can be accessed as the SparkApp_object.spark property\n",
      "INFO:spark_app:==================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d10fb5fe9c53:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://172.18.0.2:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-taxi-forecasting</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd4b7010f50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_app.build_spark_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab1b14-51a4-4d05-9ada-4217282f13ce",
   "metadata": {},
   "source": [
    "Получим стандартный объект `spark` из созданного выше объекта `spark_app`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56cdf778-5944-484e-a72f-6e151b2ec3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = spark_app.spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca259b2f-ac8d-4de8-800b-754588a3963c",
   "metadata": {},
   "source": [
    "Проверка работы спарк-объекта на кластере. Если все в порядке, то тест должен выполниться достаточно быстро и отобразить тестовый ДФ.  \n",
    "```txt\n",
    "+------------+-----------+\n",
    "|student_name|student_age|\n",
    "+------------+-----------+\n",
    "|       Alice|         10|\n",
    "|         Bob|         20|\n",
    "+------------+-----------+\n",
    "```\n",
    "\n",
    "Если исходные образы кластера собраны с ошибкой, возможно \"зависание\" работы теста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65e12fff-3cf5-4918-b8f7-d4ce0d306949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n",
      "DataFrame created successfully.\n",
      "Alias DataFrame created successfully.\n",
      "DataFrame data matches expected result.\n",
      "+------------+-----------+\n",
      "|student_name|student_age|\n",
      "+------------+-----------+\n",
      "|       Alice|         10|\n",
      "|         Bob|         20|\n",
      "+------------+-----------+\n",
      "\n",
      "DataFrame show output matches expected output.\n",
      "\n",
      "=======use======================\n",
      "*      ____              __    *\n",
      "*     / __/__  ___ _____/ /__  *\n",
      "*    _\\ \\/ _ \\/ _ `/ __/  '_/  *\n",
      "*   /__ / .__/\\_,_/_/ /_/\\_\\   *\n",
      "*      /_/                     * \n",
      "================================\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "spark_app.test_spark_functionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd529952-7b42-4dff-bf7b-d4a3d304ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = DictAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a49c4e5f-8973-485c-8714-94448bbe1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция простой рандомизированной выборки\n",
    "def random_sample_dataframe(dataframe, percentage):\n",
    "    # Генерируем случайные числа от 0 до 1 и фильтруем строки\n",
    "    df = dataframe.filter(f.rand() < percentage)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62ddc595-f4f4-4033-b890-cf42b5c3d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# agg_hour.coalesce(1).write.csv(\"/work/data/taxis_agg_hour_growth.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c59bfe-cbcc-4a69-8cb4-c6931b61b50a",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb679a8d-5f92-49ab-80b2-d3a76792f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# agg_hour = spark.read.load('/work/data/taxis_agg_hour_growth.csv', \n",
    "#                        format='csv', header='true'\n",
    "#                         , inferSchema='true'\n",
    "#                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df4f4d57-25f7-4c70-976f-fd4452ca779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_hour.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9d40cea-0b3e-4396-b995-928bfc94e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_hour.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ade35b3b-acac-452e-ac1c-82be92d46759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_null = agg_hour.select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in agg_hour.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb0aa96f-4e05-4dab-83ea-23dabb49fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_null = agg_null.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62603269-3dbf-45c6-a9ec-7ec4934cf033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_null[agg_null.iloc[:, 0] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7569a36-ee74-487e-b86b-438e5356fdb2",
   "metadata": {},
   "source": [
    "Загрузим данные по исключаемым (высокоррелированным) полям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bdae2d7-8b16-4707-966d-6b83e15c9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_f = pd.read_csv('/work/data/excluded_fields.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3db3af7-e825-4568-b711-a9fd2434cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для преобразования DataFrame в словарь\n",
    "def get_excuded_fields_to_dict(df):\n",
    "    excluded_fields_tot = {}\n",
    "    grouped = df.groupby(['ct', 'ca'])\n",
    "    for (ct, ca), group in grouped:\n",
    "        excluded_fields_tot[(ct, ca)] = group['excluded_field'].tolist()\n",
    "    return excluded_fields_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f8afe7e-4736-430c-a597-71eb61c6a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_fields_tot = get_excuded_fields_to_dict(excl_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a974c60-8161-4c21-9f67-438d203bca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    (10000000000, 91): ['time_p', 'miles_p', 'farem_p', 'comp1_p', 'comp3_p', 'comp4_p', 'comp5_p', 'extrasm_p']\n",
      "    (12000000001, 53): ['time_p', 'farem_p', 'trips_d', 'time_d', 'miles_d', 'farem_d', 'totalm_p', 'velocity_p_growth_3_to_2']\n",
      "    (12000000002, 75): ['time_p', 'miles_p', 'farem_p', 'tipsm_p', 'comp5_p', 'trips_d', 'time_d', 'miles_d', 'farem_d', 'tipsm_d', 'comp4_p']\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "da.print_dict(dict(islice(excluded_fields_tot.items(), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab429d-b219-4826-aab2-bbcaee4526ed",
   "metadata": {},
   "source": [
    "Будем считать, что от мультиколлинеарности в линейных моделях с помощью исключения этих полей получится избавиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f44b76c1-cfab-4115-a7ce-d840d7bd9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# agg_hour.coalesce(1).write.csv(\"/work/data/taxis_agg_hour_result.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f705c08-5874-478d-a978-3646ba2e282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 20.6 ms, total: 20.6 ms\n",
      "Wall time: 49.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = spark.read.load('/work/data/taxis_agg_hour_result.csv', \n",
    "                       format='csv', header='true'\n",
    "                        , inferSchema='true'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ced2355-4769-4ff4-98e3-4437a9fa7d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n"
     ]
    }
   ],
   "source": [
    "print(pds.gvf(data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "296b0a00-02cb-4a6c-8fe7-44a58ed69f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ct: bigint, ca: int, hour_start: timestamp, time_p: int, miles_p: double, velocity_p: double, farem_p: double, tipsm_p: double, tollsm_p: double, extrasm_p: double, totalm_p: double, comp1_p: int, comp2_p: int, comp3_p: int, comp4_p: int, comp5_p: int, compless5_p: int, trips_d: int, time_d: int, miles_d: double, velocity_d: double, farem_d: double, tipsm_d: double, tollsm_d: double, extrasm_d: double, totalm_d: double, comp1_d: int, comp2_d: int, comp3_d: int, comp4_d: int, comp5_d: int, compless5_d: int, cumulative_balance: int, trips_p_growth_1_to_0: double, trips_p_growth_2_to_1: double, trips_p_growth_3_to_2: double, trips_p_growth_4_to_3: double, trips_d_growth_1_to_0: double, trips_d_growth_2_to_1: double, trips_d_growth_3_to_2: double, trips_d_growth_4_to_3: double, velocity_p_growth_1_to_0: double, velocity_p_growth_2_to_1: double, velocity_p_growth_3_to_2: double, velocity_p_growth_4_to_3: double, velocity_d_growth_1_to_0: double, velocity_d_growth_2_to_1: double, velocity_d_growth_3_to_2: double, velocity_d_growth_4_to_3: double, trips_sh_168: int, trips_sh_84: int, trips_sh_24: int, trips_sh_28: int, trips_sh_12: int, trips_sh_8: int, trips_ma_168: double, trips_ma_84: double, trips_ma_24: double, trips_ma_28: double, trips_ma_12: double, trips_ma_8: double, trips_sh_4: int, trips_ma_4: double, trips_sh_1: int, trips_ma_1: double, trips_ma_168_growth: double, trips_ma_8_growth: double, trips_ma_4_growth: double, trips_target: int]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663d6b9-90ea-4f36-aea7-1520d63a0d8c",
   "metadata": {},
   "source": [
    "### Фрагмент данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699374f-3d4b-4d28-b2fb-d9ddca03138c",
   "metadata": {},
   "source": [
    "Отберем несколько полей и один район для проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08662517-2f13-4728-bfde-a27c48d289a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-------------------+------------+------+-------+--------------------+\n",
      "|         ct| ca|         hour_start|trips_target|time_p|miles_p|          velocity_p|\n",
      "+-----------+---+-------------------+------------+------+-------+--------------------+\n",
      "|17031090200|  6|2021-01-08 01:00:00|           1|     0|    0.0|                 0.0|\n",
      "|17031090200|  6|2021-01-08 02:00:00|           2|   706|   3.24|0.004589235127478754|\n",
      "|17031090200|  6|2021-01-08 03:00:00|           1|   990|    2.9|0.002929292929292929|\n",
      "+-----------+---+-------------------+------------+------+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_to_sel = [\n",
    "    'ct', 'ca', 'hour_start', 'trips_target', 'time_p', 'miles_p', 'velocity_p',\n",
    "]\n",
    "\n",
    "data_sample = data.select(*f_to_sel).filter(f.col('ct')==17031090200)\n",
    "data_sample.show(3)\n",
    "# data_sample.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6b49f0f-b5ee-4c40-ad16-d5d0a279f357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time_p miles_p farem_p tipsm_p comp1_p comp3_p comp4_p trips_d time_d miles_d farem_d tipsm_d extrasm_d comp2_d comp3_d comp4_d extrasm_p comp5_p'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(excluded_fields_tot[(17031090200,6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b28493-6584-4311-bdec-5b8465c2c279",
   "metadata": {},
   "source": [
    "Данные загрузились. Общее количество и отображение фрагмента соответствуют ожиданиям. Исключаемые поля в доступе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38256c91-3952-4833-8844-c0477848f371",
   "metadata": {},
   "source": [
    "### Инвентаризация объектов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08556d06-b00a-405c-9e18-2d0f1440ac3f",
   "metadata": {},
   "source": [
    "Датафреймы pyspark, если они были зарегестрированы в качестве представлений (таблиц), можно молучить используя catalog. Но если это просто объекты Питон типа pyspark.sql.dataframe.DataFrame, такие дф надо искать среди всех объектов в глобальной области видимости программы. Воспользуемся написанной функцией. При этом возможна путаница, связанная с \"магической\" служебной переменной Юпитера с именем \"__\" (последнее сохраненное значение ячейки). Такое имя может заменить реальное имя объекта. Решается двухкратным вызовом, например, `type(__)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a7c4ca1-18b9-4d14-a5ea-92a0da9f29bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "type(__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1741727f-02a6-4134-8afd-755a47201ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca4b95f1-be16-4b0b-82fc-035a4decb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_namespace=globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba9a1eb4-d00d-469b-8163-eeee1ad4b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(pds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bdd0f5a-d7cc-488c-babd-ffefe4454f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ob = pds.inventory_objects(globals())\n",
    "# ob_df = ob[ob['Type'].str.contains('DataFrame')]\n",
    "# ob_df = pds.get_df_formated(ob_df,\"'\",0,40)\n",
    "# print(ob_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62f6249b-5ea6-4fb6-b382-932e87281931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_df_list(globe_dict):\n",
    "    objects = pds.inventory_objects(globe_dict)\n",
    "    spark_df = objects[objects['Type'].str.contains('Spark DataFrame') & \n",
    "                       ~objects['Name'].str.startswith('DataFrame[') & \n",
    "                       ~objects['Name'].str.startswith('_')].copy()\n",
    "    del objects\n",
    "    spark_df['cached'] = spark_df.apply(lambda x: eval(x['Name']).is_cached \\\n",
    "            if not (x['Name'].startswith(\"DataFrame[\") or x['Name'].startswith(\"_\")) else None, axis=1)\n",
    "    spark_df['cache_level'] = spark_df.apply(lambda x: eval(x['Name']).storageLevel \\\n",
    "            if not (x['Name'].startswith(\"DataFrame[\") or x['Name'].startswith(\"_\")) else None, axis=1)\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36df8f19-91f8-4db9-852e-3762024ec909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df = get_spark_df_list(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1597403e-6ba9-4d6a-aa17-06fcaf51da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e27b1-a765-43c4-9406-06b99c337e15",
   "metadata": {},
   "source": [
    "## Порядок действий МО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc2d5694-b4e2-429c-a714-36769278f089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n",
      "CPU times: user 14.7 ms, sys: 1.88 ms, total: 16.6 ms\n",
      "Wall time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pds.gvf(data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7437836-0bec-433c-b814-482e9b205b49",
   "metadata": {},
   "source": [
    "Общее описание порядка действий"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ecaa84-bc2b-4840-9df5-d777cf6f62e5",
   "metadata": {},
   "source": [
    "Некоторая информация связанная с векторизацией признаков.  \n",
    "Сохранить датафрейм со столбцом вектором возможно, например, в формате parquet; csv не поддерживает вектор-столбцы. Загруженный паркет-файл вызывает ошибку при выполнении действий.   \n",
    "При кэшировании векторизированного дф (с использованием уровня хранения Disk Memory Deserialized 1x Replicated - уровень, который присваивается по умолчанию при вызове .cache()), такой кэшированный датафрейм с вектор-столбцом не получается обрабатывать из-за нехватки памяти.  \n",
    "\n",
    "Пока остановился на варианте:  \n",
    "- 1. делю входной дф на трэйн и тест, кэширую их .cache() (Disk Memory)\n",
    "- 2. получаю фрагмент geo_df_base_train/test для гео-ключа из трэйн дф, кэширую в ОЗУ geo_df_base_train/test\n",
    "- 3. векторизирую фичи geo_df_base_train/test,\n",
    "- 4. создаю рабочий дф geo_df_train/test дф с нужными полями (гео-ключ, время, метка, фича-вектор)\n",
    "- 5. кэширую в ОЗУ geo_df_train/test;\n",
    "- 6. создаю для geo_df_train скалер\n",
    "- 7. масштабирую-обновляю geo_df_train/test, кэширую в ОЗУ,\n",
    "- 8. обучаю модель лин.регрессии на трэйне (включая тюнинг модели)\n",
    "- 9. сохраняю в пандас дф для данного гео-ключа полученный объект-модель и метрики на трэйне\n",
    "- 10. получаю предикт на трэйне и тесте и добавляю столбец предикта в исходные train/test по гео-ключу, обновляю их и кэширую .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c6200-2ea6-420a-bee6-04a7ee7d861a",
   "metadata": {},
   "source": [
    "### 1. делю входной дф на трэйн и тест, кэширую их .cache() (Disk Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d9d1318-f08d-49b5-879a-1016fe6a9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на тренировочный и тестовый наборы\n",
    "train = data.filter(f.year(f.col(\"hour_start\")) < 2024)\n",
    "test = data.filter(f.year(f.col(\"hour_start\")) == 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db8b6537-545f-4d50-adc0-84c154222390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ct: bigint, ca: int, hour_start: timestamp, time_p: int, miles_p: double, velocity_p: double, farem_p: double, tipsm_p: double, tollsm_p: double, extrasm_p: double, totalm_p: double, comp1_p: int, comp2_p: int, comp3_p: int, comp4_p: int, comp5_p: int, compless5_p: int, trips_d: int, time_d: int, miles_d: double, velocity_d: double, farem_d: double, tipsm_d: double, tollsm_d: double, extrasm_d: double, totalm_d: double, comp1_d: int, comp2_d: int, comp3_d: int, comp4_d: int, comp5_d: int, compless5_d: int, cumulative_balance: int, trips_p_growth_1_to_0: double, trips_p_growth_2_to_1: double, trips_p_growth_3_to_2: double, trips_p_growth_4_to_3: double, trips_d_growth_1_to_0: double, trips_d_growth_2_to_1: double, trips_d_growth_3_to_2: double, trips_d_growth_4_to_3: double, velocity_p_growth_1_to_0: double, velocity_p_growth_2_to_1: double, velocity_p_growth_3_to_2: double, velocity_p_growth_4_to_3: double, velocity_d_growth_1_to_0: double, velocity_d_growth_2_to_1: double, velocity_d_growth_3_to_2: double, velocity_d_growth_4_to_3: double, trips_sh_168: int, trips_sh_84: int, trips_sh_24: int, trips_sh_28: int, trips_sh_12: int, trips_sh_8: int, trips_ma_168: double, trips_ma_84: double, trips_ma_24: double, trips_ma_28: double, trips_ma_12: double, trips_ma_8: double, trips_sh_4: int, trips_ma_4: double, trips_sh_1: int, trips_ma_1: double, trips_ma_168_growth: double, trips_ma_8_growth: double, trips_ma_4_growth: double, trips_target: int]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75be39f6-dea9-4864-856d-5a40544c8a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ct: bigint, ca: int, hour_start: timestamp, time_p: int, miles_p: double, velocity_p: double, farem_p: double, tipsm_p: double, tollsm_p: double, extrasm_p: double, totalm_p: double, comp1_p: int, comp2_p: int, comp3_p: int, comp4_p: int, comp5_p: int, compless5_p: int, trips_d: int, time_d: int, miles_d: double, velocity_d: double, farem_d: double, tipsm_d: double, tollsm_d: double, extrasm_d: double, totalm_d: double, comp1_d: int, comp2_d: int, comp3_d: int, comp4_d: int, comp5_d: int, compless5_d: int, cumulative_balance: int, trips_p_growth_1_to_0: double, trips_p_growth_2_to_1: double, trips_p_growth_3_to_2: double, trips_p_growth_4_to_3: double, trips_d_growth_1_to_0: double, trips_d_growth_2_to_1: double, trips_d_growth_3_to_2: double, trips_d_growth_4_to_3: double, velocity_p_growth_1_to_0: double, velocity_p_growth_2_to_1: double, velocity_p_growth_3_to_2: double, velocity_p_growth_4_to_3: double, velocity_d_growth_1_to_0: double, velocity_d_growth_2_to_1: double, velocity_d_growth_3_to_2: double, velocity_d_growth_4_to_3: double, trips_sh_168: int, trips_sh_84: int, trips_sh_24: int, trips_sh_28: int, trips_sh_12: int, trips_sh_8: int, trips_ma_168: double, trips_ma_84: double, trips_ma_24: double, trips_ma_28: double, trips_ma_12: double, trips_ma_8: double, trips_sh_4: int, trips_ma_4: double, trips_sh_1: int, trips_ma_1: double, trips_ma_168_growth: double, trips_ma_8_growth: double, trips_ma_4_growth: double, trips_target: int]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d71b798-1cf0-42c1-b006-3b201c85c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(pds.gvf(train.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7410b840-3f72-4747-91af-7d7e6620dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(pds.gvf(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68cebea1-c64d-48e9-9fb5-d9f2c5dd4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable_name = 'trips_target'\n",
    "# исключаемые из расчетов ключевые поля для всех районов\n",
    "key_cols = ['ct', 'ca', 'hour_start']\n",
    "# поля которые будут в рабочих ДФ\n",
    "# ['ct', 'ca', 'hour_start', 'trips_target', 'features']\n",
    "# 'features' - немасштабированный вектор-столбец\n",
    "selectedCols = key_cols + [target_variable_name, 'features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06396d4b-64c9-4749-aebc-74f89e4ce7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение уникальных комбинаций районов и округов\n",
    "geo_keys = data.select('ct', 'ca').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379d4dd-7f9e-4d56-8f79-b0b42d278060",
   "metadata": {},
   "source": [
    "### 2. получаю фрагмент geo_df_base_train/test для гео-ключа из трэйн дф, кэширую в ОЗУ geo_df_base_train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6014459d-d71e-4735-842a-5065a277b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаю фрагмент geo_df_base_train/test\n",
    "# для гео-ключа из трэйн/тест дф\n",
    "def get_geo_df_base(point, train, test):\n",
    "    ct = point[0]\n",
    "    ca = point[1]\n",
    "    # Фильтрация данных для текущего района и округа\n",
    "    geo_data_base_train = train.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "    geo_data_base_test = test.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "\n",
    "    return geo_data_base_train, geo_data_base_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce3369e4-4a12-4b86-82fd-2ff2edff3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_df_in_ram(df):\n",
    "    df.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07328fd2-ea0e-43ed-b1d0-929a3bae8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# point = (17031090200,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5744cda8-b5f4-4537-b30c-90955b813ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_data_base_train, geo_data_base_test = get_geo_df_base(point, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2be05558-7f0d-4b86-992b-241a60e0a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # кэширую в ОЗУ geo_df_base_train/test\n",
    "# cache_df_in_ram(geo_data_base_train)\n",
    "# cache_df_in_ram(geo_data_base_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f8f80b-89fe-4ca6-bee4-75019d9065f1",
   "metadata": {},
   "source": [
    "### 3. векторизирую фичи geo_df_base_train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "68bf10d8-2625-4c2a-b4ec-8106d7819855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# векторизация указанных полей датафрейма\n",
    "def assemble_vectors(df, features_list, selectedCols):\n",
    "    stages = []\n",
    "    assembler = VectorAssembler(inputCols=features_list, outputCol='features')\n",
    "    stages = [assembler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    # assemble model\n",
    "    assembleModel = pipeline.fit(df)\n",
    "    # apply assembler model on data\n",
    "    df = assembleModel.transform(df).select(selectedCols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d2c42d6-e481-4809-ace3-c0558c71f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_list(point,df,target_variable_name):\n",
    "    base_columns = df.columns\n",
    "    all_excluded_columns = key_cols + [target_variable_name] + excluded_fields_tot[point]\n",
    "    features_list = [col for col in base_columns if col not in all_excluded_columns]\n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f247bc69-7625-43c5-83d9-e4869c83922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_list = get_features_list(point,data,target_variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6339eb75-c57e-4ca1-aafb-683cfdf7d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" \".join(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8d202-c31d-4bf0-a663-d95944b0a6c1",
   "metadata": {},
   "source": [
    "### 4. создаю рабочий дф geo_df_train/test дф с нужными полями (гео-ключ, время, метка, фича-вектор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "10fd3181-a768-4952-b3a2-c3a5f574e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_train = assemble_vectors(geo_data_base_train,features_list,selectedCols)\n",
    "# geo_df_test = assemble_vectors(geo_data_base_test,features_list,selectedCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f33cda-6b20-4bb1-b51a-70aa32bfd796",
   "metadata": {},
   "source": [
    "### 5. кэширую в ОЗУ geo_df_train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "948ab9fc-11f8-46f2-ab16-0bd206f5a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pds.gvf(geo_df_train.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3eebcef1-dd57-4b85-abb9-6ef8cf644308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pds.gvf(geo_df_test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "af6ae2fb-d7fb-4630-af64-747ba65c351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # кэширую в ОЗУ geo_df_base_train/test\n",
    "# cache_df_in_ram(geo_df_train)\n",
    "# cache_df_in_ram(geo_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c7284-88a7-43b1-b1db-5f8ca6e4ac3e",
   "metadata": {},
   "source": [
    "### 6. создаю для geo_df_train скалер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1fde410-8b3b-4d0e-91dc-83135ad080f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitted_scaler(df, featureCol):\n",
    "    outputCol = featureCol+\"_scaled\"\n",
    "    stages = []\n",
    "    scaler = StandardScaler(inputCol = featureCol,\n",
    "                            outputCol=outputCol,\n",
    "                            withStd=True, withMean=True)\n",
    "    stages = [scaler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    scaledAssembleModel = pipeline.fit(df)\n",
    "\n",
    "    return scaledAssembleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5d31d082-fcbf-4579-bbaa-2e3d84b8416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5545804a-7cd6-49f1-b11a-7046686160ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_train_scaler = get_fitted_scaler(geo_df_train, 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec96191-16dc-4985-a004-b7ea78901e7c",
   "metadata": {},
   "source": [
    "### 7. масштабирую-обновляю geo_df_train/test, кэширую в ОЗУ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b229029-c3c0-4b6a-8f8c-e4a658e2a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_train = geo_df_train_scaler.transform(geo_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc210ae8-0637-4c43-96d2-399d9197f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_test = geo_df_train_scaler.transform(geo_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "938200af-fcbe-46ea-a823-818bceb7c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pds.gvf(geo_df_train.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f8261612-45c6-418d-9acd-98e85c04f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pds.gvf(geo_df_test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b2eccacc-dc6d-49fe-8d9b-944d706aced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df_train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304f373-506e-4890-823c-aa2f977ac610",
   "metadata": {},
   "source": [
    "### 8. обучаю модель лин.регрессии на трэйне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3ba663a7-34b1-483f-ae99-5039e0c8f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_regr_fit(point,\n",
    "                 scaled_geo_train,\n",
    "                 scaled_geo_test,\n",
    "                 features_col = 'features',\n",
    "                 scaled_features_col = 'features_scaled',\n",
    "                 target_variable_name = 'trips_target',\n",
    "                 exc_cols = None\n",
    "):\n",
    "    if exc_cols is None:\n",
    "        exc_cols = ['ct', 'ca', 'hour_start']\n",
    "    \n",
    "    reg = LinearRegression(\n",
    "        featuresCol = scaled_features_col,\n",
    "        labelCol = target_variable_name\n",
    "    )\n",
    "    \n",
    "    ct = point[0]\n",
    "    ca = point[1]\n",
    "        \n",
    "    selectedCols = exc_cols + [target_variable_name] + [scaled_features_col]\n",
    "\n",
    "    reg_model = reg.fit(scaled_geo_train)\n",
    "\n",
    "    geo_train_transformed = reg_model.transform(scaled_geo_train)\n",
    "    geo_test_transformed = reg_model.transform(scaled_geo_test)\n",
    "    \n",
    "    # Создание объекта RegressionEvaluator для оценки модели\n",
    "    evaluator = RegressionEvaluator()\n",
    "\n",
    "    # Настройка параметров оценки (RMSE и MAPE)\n",
    "    evaluator.setMetricName(\"rmse\")\n",
    "    evaluator.setPredictionCol(\"prediction\")\n",
    "    evaluator.setLabelCol(target_variable_name)\n",
    "\n",
    "    # Оценка модели\n",
    "    rmse_train = evaluator.evaluate(geo_train_transformed)\n",
    "    rmse_test = evaluator.evaluate(geo_test_transformed)\n",
    "    \n",
    "    # Аналогично для MAE\n",
    "    evaluator.setMetricName(\"mae\")\n",
    "    mae_train = evaluator.evaluate(geo_train_transformed)\n",
    "    mae_test = evaluator.evaluate(geo_test_transformed)\n",
    "    \n",
    "    # mae_train, mae_test, mape_train, mape_test = 0,0,0,0\n",
    "    # print(\"mae_train:\",mae_train,\"mae_test\",mae_test,\"mape_train:\",mape_train,\"mape_test\",mape_test)\n",
    "    \n",
    "    # # Вычисление MAE вручную для train\n",
    "    # geo_train_transformed = geo_train_transformed.withColumn(\"abs_error\", \n",
    "    #     f.abs(geo_train_transformed[\"prediction\"] - geo_train_transformed[target_variable_name])\n",
    "    # )\n",
    "    # mae_train = geo_train_transformed.selectExpr(\"mean(abs_error) as mae\").collect()[0][\"mae\"]\n",
    "    \n",
    "    # # Вычисление MAE вручную для test\n",
    "    # geo_test_transformed = geo_test_transformed.withColumn(\"abs_error\", \n",
    "    #     f.abs(geo_test_transformed[\"prediction\"] - geo_test_transformed[target_variable_name])\n",
    "    # )\n",
    "    # mae_test = geo_test_transformed.selectExpr(\"mean(abs_error) as mae\").collect()[0][\"mae\"]\n",
    "\n",
    "    # print(\"mae_train:\",mae_train,\"mae_test\",mae_test)\n",
    "\n",
    "    # Получение предсказаний и реальных значений\n",
    "    # Вычисление MAPE\n",
    "    geo_train_transformed = geo_train_transformed.withColumn(\"abs_pct_error\", \n",
    "        f.abs((geo_train_transformed[\"prediction\"] - geo_train_transformed[target_variable_name]) \\\n",
    "            / geo_train_transformed[target_variable_name])\n",
    "    )\n",
    "    mape_train = geo_train_transformed.selectExpr(\"mean(abs_pct_error) * 100 as mape\")\\\n",
    "        .collect()[0][\"mape\"]\n",
    "    geo_test_transformed = geo_test_transformed.withColumn(\"abs_pct_error\", \n",
    "        f.abs((geo_test_transformed[\"prediction\"] - geo_test_transformed[target_variable_name]) \\\n",
    "            / geo_test_transformed[target_variable_name])\n",
    "    )\n",
    "    mape_test = geo_test_transformed.selectExpr(\"mean(abs_pct_error) * 100 as mape\")\\\n",
    "        .collect()[0][\"mape\"]\n",
    "    # print(\"mape_train:\",mape_train,\"mape_test\",mape_test)\n",
    "    \n",
    "    return reg_model, geo_train_transformed, geo_test_transformed, \\\n",
    "            rmse_train, mae_train, mape_train, \\\n",
    "            rmse_test, mae_test, mape_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cbdc8f80-5864-4d8d-881e-9d359ed4c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ванильный тюнинг не используется - не меняет результатов\n",
    "# def lin_regr_cv_fit(point,\n",
    "#                  scaled_geo_train,\n",
    "#                  scaled_geo_test,\n",
    "#                  features_col='features',\n",
    "#                  scaled_features_col='features_scaled',\n",
    "#                  target_variable_name='trips_target',\n",
    "#                  exc_cols=None,\n",
    "#                  max_iter=100, \n",
    "#                  reg_param=0.0, \n",
    "#                  elastic_net_param=0.0, \n",
    "#                  param_grid=None,\n",
    "#                  num_folds=3):\n",
    "#     if exc_cols is None:\n",
    "#         exc_cols = ['ct', 'ca', 'hour_start']\n",
    "    \n",
    "#     # Создание модели линейной регрессии с параметрами\n",
    "#     reg = LinearRegression(\n",
    "#         featuresCol=scaled_features_col,\n",
    "#         labelCol=target_variable_name,\n",
    "#         maxIter=max_iter,\n",
    "#         regParam=reg_param,\n",
    "#         elasticNetParam=elastic_net_param\n",
    "#     )\n",
    "    \n",
    "#     ct = point[0]\n",
    "#     ca = point[1]\n",
    "    \n",
    "#     selectedCols = exc_cols + [target_variable_name] + [scaled_features_col]\n",
    "\n",
    "#     # Настройка параметров для кросс-валидации\n",
    "#     if param_grid is None:\n",
    "#         param_grid = (ParamGridBuilder()\n",
    "#                       .addGrid(reg.regParam, [0.0, 0.1, 0.01])\n",
    "#                       .addGrid(reg.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "#                       .build())\n",
    "    \n",
    "#     # Создание кросс-валидатора\n",
    "#     evaluator = RegressionEvaluator(metricName=\"rmse\", predictionCol=\"prediction\", labelCol=target_variable_name)\n",
    "#     crossval = CrossValidator(estimator=reg,\n",
    "#                               estimatorParamMaps=param_grid,\n",
    "#                               evaluator=evaluator,\n",
    "#                               numFolds=num_folds)\n",
    "    \n",
    "#     # Обучение модели с кросс-валидацией\n",
    "#     cv_model = crossval.fit(scaled_geo_train)\n",
    "#     reg_model = cv_model.bestModel\n",
    "    \n",
    "#     geo_train_transformed = reg_model.transform(scaled_geo_train)\n",
    "#     geo_test_transformed = reg_model.transform(scaled_geo_test)\n",
    "    \n",
    "#     # Оценка модели\n",
    "#     rmse_train = evaluator.evaluate(geo_train_transformed)\n",
    "#     rmse_test = evaluator.evaluate(geo_test_transformed)\n",
    "    \n",
    "#     evaluator.setMetricName(\"mae\")\n",
    "#     mae_train = evaluator.evaluate(geo_train_transformed)\n",
    "#     mae_test = evaluator.evaluate(geo_test_transformed)\n",
    "\n",
    "#     \n",
    "    \n",
    "#     # Вычисление MAPE\n",
    "#     geo_train_transformed = geo_train_transformed.withColumn(\"abs_pct_error\", \n",
    "#         f.abs((geo_train_transformed[\"prediction\"] - geo_train_transformed[target_variable_name]) \\\n",
    "#             / geo_train_transformed[target_variable_name])\n",
    "#     )\n",
    "#     mape_train = geo_train_transformed.selectExpr(\"mean(abs_pct_error) * 100 as mape\")\\\n",
    "#         .collect()[0][\"mape\"]\n",
    "#     geo_test_transformed = geo_test_transformed.withColumn(\"abs_pct_error\", \n",
    "#         f.abs((geo_test_transformed[\"prediction\"] - geo_test_transformed[target_variable_name]) \\\n",
    "#             / geo_test_transformed[target_variable_name])\n",
    "#     )\n",
    "#     mape_test = geo_test_transformed.selectExpr(\"mean(abs_pct_error) * 100 as mape\")\\\n",
    "#         .collect()[0][\"mape\"]\n",
    "#     \n",
    "    \n",
    "#     return reg_model, geo_train_transformed, geo_test_transformed, \\\n",
    "#             rmse_train, mae_train, mape_train, \\\n",
    "#             rmse_test, mae_test, mape_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26fea987-808c-4260-88c3-55680ae7b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse, mae, mape = 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f40a0cb-be55-4055-96ca-084833dee5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_model, geo_train_transformed, geo_test_transformed, rmse, mae, mape =\\\n",
    "# lin_regr_fit(point,\n",
    "#                  geo_df_train,\n",
    "#                  geo_df_test,\n",
    "#                  features_col = 'features',\n",
    "#                  scaled_features_col = 'features_scaled',\n",
    "#                  target_variable_name = 'trips_target',\n",
    "#                  exc_cols = None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a2299274-9204-4f64-af1b-0f3e67fe7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_train_transformed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db50b934-69d1-4a65-92a7-44f93af13b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse, mae, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0575d49a-af5e-44fe-9919-a5ca67efc635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e500501-2d4e-4cef-ad5b-b44a4bf7d8f4",
   "metadata": {},
   "source": [
    "### 9. сохраняю в пандас дф для данного гео-ключа полученный объект-модель и метрики на трэйне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f97a6d14-e61f-42ee-b45f-a41fbcf04dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = pd.DataFrame(columns=[\"ct\",\"ca\", \"reg_model\", \"rmse\", \"mae\", \"mape\",\"features\",\"coefficients\",\"intercept\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2cc6d5de-f986-403a-a444-48f78b2063c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model_metrics(df=None, point=None, reg_model=None,\n",
    "                      rmse_train=None, mae_train=None, mape_train=None,\n",
    "                      rmse_test=None, mae_test=None, mape_test=None,\n",
    "                     features_list=None,coefficients=None,intercept=None):\n",
    "    # Создание нового DataFrame, если он не был передан\n",
    "    if df is None:\n",
    "        df = pd.DataFrame(columns=[\"ct\",\"ca\", \"reg_model\",\n",
    "                                   \"rmse_train\", \"mae_train\", \"mape_train\",\n",
    "                                   \"rmse_test\", \"mae_test\", \"mape_test\",\n",
    "                                   \"features\",\"coefficients\",\"intercept\"])\n",
    "    \n",
    "    # Создание новой строки с метриками\n",
    "    new_row = {\n",
    "        \"ct\": point[0],\n",
    "        \"ca\": point[1],\n",
    "        \"reg_model\": reg_model,\n",
    "        \"rmse_train\": rmse_train,\n",
    "        \"mae_train\": mae_train,\n",
    "        \"mape_train\": mape_train,\n",
    "        \"rmse_test\": rmse_test,\n",
    "        \"mae_test\": mae_test,\n",
    "        \"mape_test\": mape_test,\n",
    "        \"features\": features_list,\n",
    "        \"coefficients\": coefficients,\n",
    "        \"intercept\": intercept\n",
    "    }\n",
    "    \n",
    "    # Добавление новой строки в DataFrame\n",
    "    new_df = pd.DataFrame([new_row])\n",
    "    updated_df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aeb96096-ec07-4922-8571-76bf10ea2894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = add_model_metrics(df=models,\n",
    "#                            point=point,\n",
    "#                            reg_model=reg_model,\n",
    "#                            rmse=rmse, mae=mae, mape=mae,\n",
    "#                            features_list=features_list,\n",
    "#                            coefficients=reg_model.coefficients,\n",
    "#                            intercept=reg_model.intercept\n",
    "#                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1a4acb90-0d5e-404f-a262-0e1806dd6cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a83169-558a-4589-9c52-5986c04ca950",
   "metadata": {},
   "source": [
    "### 10. получаю предикт на трэйне и тесте и добавляю столбец предикта в исходные train/test по гео-ключу, обновляю их и кэширую .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a024ca28-5777-49eb-85d6-a70ab9eae079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predictions_to_base_data(geo_data_base_train,\n",
    "                                 geo_data_base_test,\n",
    "                                 geo_train_transformed, \n",
    "                                 eo_test_transformed,\n",
    "                                 join_columns):\n",
    "    # Объединение данных с предсказаниями и исходных данных по указанным столбцам\n",
    "    geo_data_base_train_with_predictions = geo_data_base_train.join(\n",
    "        geo_train_transformed.select(*join_columns, \"prediction\"),\n",
    "        on=join_columns,\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    geo_data_base_test_with_predictions = geo_data_base_test.join(\n",
    "        geo_test_transformed.select(*join_columns, \"prediction\"),\n",
    "        on=join_columns,\n",
    "        how=\"left\"\n",
    "    )\n",
    "    return geo_data_base_train_with_predictions,\\\n",
    "           geo_data_base_test_with_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b1e5bec4-1929-4cf8-a213-b6e707cf6abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_data_base_train, geo_data_base_test = \\\n",
    "# add_predictions_to_base_data(geo_data_base_train,\n",
    "#                              geo_data_base_test,\n",
    "#                              geo_train_transformed, \n",
    "#                              geo_test_transformed,\n",
    "#                              key_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "80e89ed9-d45d-4f35-bd21-877bd4ed99d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_to_sel = key_cols +[target_variable_name]+['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "42950eb5-2934-480e-a556-c517034d06d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_data_base_train.select(*f_to_sel).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe28b0-81ca-48fc-bdaf-506a3c90f445",
   "metadata": {},
   "source": [
    "### Все шаги\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5c730e98-fb1c-46f3-a043-463819b9ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_list = geo_keys[:]\n",
    "# points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a6618ee-1caa-459d-9b31-a35c9ba68953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [22:34<00:00, 13.15s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = pd.DataFrame(columns=[\"ct\",\"ca\", \"reg_model\",\n",
    "                               \"rmse_train\", \"mae_train\", \"mape_train\",\n",
    "                               \"rmse_test\", \"mae_test\", \"mape_test\",\n",
    "                               \"features\",\"coefficients\",\"intercept\"])\n",
    "\n",
    "train_res = None\n",
    "test_res = None\n",
    "\n",
    "for point in tqdm(points_list):\n",
    "    # 2. получаю фрагмент geo_df_base_train/test\n",
    "    # для гео-ключа из трэйн дф,\n",
    "    geo_data_base_train, geo_data_base_test = get_geo_df_base(point, train, test)\n",
    "    # кэширую в ОЗУ geo_df_base_train/test\n",
    "    cache_df_in_ram(geo_data_base_train)\n",
    "    cache_df_in_ram(geo_data_base_test)\n",
    "    # 3. векторизирую фичи geo_df_base_train/test\n",
    "    features_list = get_features_list(point,train,target_variable_name)\n",
    "    # 4. создаю рабочий дф geo_df_train/test дф\n",
    "    # с нужными полями (гео-ключ, время, метка, фича-вектор)\n",
    "    geo_df_train = assemble_vectors(geo_data_base_train,features_list,selectedCols)\n",
    "    geo_df_test = assemble_vectors(geo_data_base_test,features_list,selectedCols)\n",
    "    # 5. кэширую в ОЗУ geo_df_base_train/test\n",
    "    cache_df_in_ram(geo_df_train)\n",
    "    cache_df_in_ram(geo_df_test)\n",
    "    # 6. создаю для geo_df_train скалер\n",
    "    geo_df_train_scaler = get_fitted_scaler(geo_df_train, 'features')\n",
    "    # 7. масштабирую-обновляю geo_df_train/test, кэширую в ОЗУ\n",
    "    geo_df_train = geo_df_train_scaler.transform(geo_df_train)\n",
    "    geo_df_test = geo_df_train_scaler.transform(geo_df_test)\n",
    "    cache_df_in_ram(geo_df_train)\n",
    "    cache_df_in_ram(geo_df_test)\n",
    "    # 8. обучаю модель лин.регрессии на трэйне\n",
    "    # получаю модель прогнозы и метрики\n",
    "    reg_model, geo_train_transformed, geo_test_transformed, \\\n",
    "            rmse_train, mae_train, mape_train, \\\n",
    "            rmse_test, mae_test, mape_test =\\\n",
    "    lin_regr_fit(point,\n",
    "                 geo_df_train,\n",
    "                 geo_df_test,\n",
    "                 features_col = 'features',\n",
    "                 scaled_features_col = 'features_scaled',\n",
    "                 target_variable_name = 'trips_target',\n",
    "                 exc_cols = None\n",
    "    )\n",
    "    # 9. сохраняю в пандас дф для данного\n",
    "    # гео-ключа полученный объект-модель и метрики на трэйне\n",
    "    models = add_model_metrics(df=models,\n",
    "                           point=point,\n",
    "                           reg_model=reg_model,\n",
    "                           rmse_train=rmse_train, mae_train=mae_train, mape_train=mape_train,\n",
    "                           rmse_test=rmse_test, mae_test=mae_test, mape_test=mape_test,\n",
    "                           features_list=features_list,\n",
    "                           coefficients=reg_model.coefficients,\n",
    "                           intercept=reg_model.intercept\n",
    "                          )\n",
    "    # 10. получаю предикт на трэйне и тесте и добавляю столбец\n",
    "    # предикта в исходные train/test по гео-ключу,\n",
    "    # обновляю их и кэширую .cache()\n",
    "    geo_data_base_train, geo_data_base_test = \\\n",
    "    add_predictions_to_base_data(geo_data_base_train,\n",
    "                             geo_data_base_test,\n",
    "                             geo_train_transformed, \n",
    "                             geo_test_transformed,\n",
    "                             key_cols)\n",
    "    # 11. Добавляю полученный датафрейм в результат\n",
    "    if train_res is None:\n",
    "        train_res = geo_data_base_train\n",
    "    else:\n",
    "        train_res = train_res.union(geo_data_base_train)\n",
    "    # train_res.cache()\n",
    "    if test_res is None:\n",
    "        test_res = geo_data_base_test\n",
    "    else:\n",
    "        test_res = test_res.union(geo_data_base_test)\n",
    "    # test_res.cache()\n",
    "    # return train_res, test_res, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2068e60f-3e87-46db-a31c-9c3263991e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_res, test_res, models = all_steps_taxi(points_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "12e7dd9a-48ce-4771-8349-13d435d7f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "493ec428-d50c-4797-9982-8afaa332ea35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>mae_train</th>\n",
       "      <th>mape_train</th>\n",
       "      <th>rmse_test</th>\n",
       "      <th>mae_test</th>\n",
       "      <th>mape_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.361335</td>\n",
       "      <td>2.549306</td>\n",
       "      <td>49.572514</td>\n",
       "      <td>3.414791</td>\n",
       "      <td>2.568554</td>\n",
       "      <td>47.209217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.174596</td>\n",
       "      <td>6.047635</td>\n",
       "      <td>34.219294</td>\n",
       "      <td>9.080190</td>\n",
       "      <td>6.607081</td>\n",
       "      <td>33.351287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.070174</td>\n",
       "      <td>2.353955</td>\n",
       "      <td>42.468547</td>\n",
       "      <td>3.186198</td>\n",
       "      <td>2.422917</td>\n",
       "      <td>44.098728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rmse_train  mae_train  mape_train  rmse_test  mae_test  mape_test\n",
       "0    3.361335   2.549306   49.572514   3.414791  2.568554  47.209217\n",
       "1    8.174596   6.047635   34.219294   9.080190  6.607081  33.351287\n",
       "2    3.070174   2.353955   42.468547   3.186198  2.422917  44.098728"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[[\"rmse_train\", \"mae_train\", \"mape_train\",\n",
    "        \"rmse_test\", \"mae_test\", \"mape_test\",]].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dfcaae6b-8b12-4213-8b38-69d14c4fb262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>mae_train</th>\n",
       "      <th>mape_train</th>\n",
       "      <th>rmse_test</th>\n",
       "      <th>mae_test</th>\n",
       "      <th>mape_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>103.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>103.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.810005</td>\n",
       "      <td>1.945298</td>\n",
       "      <td>55.284813</td>\n",
       "      <td>3.140904</td>\n",
       "      <td>2.153357</td>\n",
       "      <td>55.831406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.234132</td>\n",
       "      <td>2.232060</td>\n",
       "      <td>12.121400</td>\n",
       "      <td>4.106706</td>\n",
       "      <td>2.800895</td>\n",
       "      <td>12.498734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.322773</td>\n",
       "      <td>0.170916</td>\n",
       "      <td>23.589202</td>\n",
       "      <td>0.348340</td>\n",
       "      <td>0.193614</td>\n",
       "      <td>17.662376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.907780</td>\n",
       "      <td>0.636100</td>\n",
       "      <td>46.940058</td>\n",
       "      <td>0.970418</td>\n",
       "      <td>0.624346</td>\n",
       "      <td>47.566825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.457101</td>\n",
       "      <td>1.050917</td>\n",
       "      <td>52.464250</td>\n",
       "      <td>1.600986</td>\n",
       "      <td>1.142249</td>\n",
       "      <td>52.625650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.560571</td>\n",
       "      <td>2.401178</td>\n",
       "      <td>61.051702</td>\n",
       "      <td>3.410993</td>\n",
       "      <td>2.246624</td>\n",
       "      <td>63.417640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.673250</td>\n",
       "      <td>11.626479</td>\n",
       "      <td>88.378262</td>\n",
       "      <td>22.965733</td>\n",
       "      <td>16.667246</td>\n",
       "      <td>89.031368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rmse_train   mae_train  mape_train   rmse_test    mae_test   mape_test\n",
       "count  103.000000  103.000000  103.000000  103.000000  103.000000  103.000000\n",
       "mean     2.810005    1.945298   55.284813    3.140904    2.153357   55.831406\n",
       "std      3.234132    2.232060   12.121400    4.106706    2.800895   12.498734\n",
       "min      0.322773    0.170916   23.589202    0.348340    0.193614   17.662376\n",
       "25%      0.907780    0.636100   46.940058    0.970418    0.624346   47.566825\n",
       "50%      1.457101    1.050917   52.464250    1.600986    1.142249   52.625650\n",
       "75%      3.560571    2.401178   61.051702    3.410993    2.246624   63.417640\n",
       "max     17.673250   11.626479   88.378262   22.965733   16.667246   89.031368"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[[\"rmse_train\", \"mae_train\", \"mape_train\",\n",
    "        \"rmse_test\", \"mae_test\", \"mape_test\",]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51295a-a4ef-49ec-8de5-546be7185cdf",
   "metadata": {},
   "source": [
    "## Результат"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e4c7c-d073-40c5-9ce6-766a00452e6b",
   "metadata": {},
   "source": [
    "**Задача проекта:** Разработка модели машинного обучения для прогнозирования спроса на такси на основе исторических данных о поездках.\n",
    "\n",
    "**Основные этапы проекта:**\n",
    "\n",
    "1. **Загрузка и сохранение данных:** Данные за 2021-2024/2 гг - около 20М строк. Три csv файла объединены в один датафрейм. Поскольку сложные планы выполнения действий с увеличением операций значительно увеличивают время преобразований, каждый логический этап завершался сохранением файла.\n",
    "01_eda_geo_keys_all.ipynb\n",
    "\n",
    "2. **Предварительный анализ данных:** Анализ в основном заключался в поиске возможных способов восполнить пропуски в гео-ключах (код района, округа, координаты). Использовался внешний датафрейм также с сайта ситиофЧикаго, использовался метод ближайших соседей. Очень много джойнов и проверок внутри датафрейма. Также удбраны выбросы (поездки дороже 315 баксов, более 7900 секунд и более 53 миль). Посмотрел 3Д гистограмму частот. О'Хара и центр делают кассу.\n",
    "01_eda_geo_keys_all.ipynb\n",
    "\n",
    "3.  **Подготовка данных:** Были выполнены следующие шаги по подготовке данных:\n",
    "   - Группировка по 15 минутным интервалам, выравниевание сетки интервалов\n",
    "   - Доп группировка по гео-ключам. Оставил отдельно крупные районы в некоторых округах. Но основная часть поездок привязана в большинстве округов к одному району в округе. В результате получил не 77(78) гео-ключей, а 103.\n",
    "   - Расчет средней скорости за 15мин интервал, остальные показатели суммируются\n",
    "   - Группировка по часовым интервалам. 15минутные скорости и Трипсы внутри часа выстроены в 4 столбца и посчитан внутричасовые приросты для 15минутных Трипсов и скоростей\n",
    "   - Для каждого района вычислены высококоррелированные поля (без Трипса - таргета), списки одной стороны из таких пар полей сохранены в отдельный ДФ. Так при моделировании будем исключать эти поля для избежания мультиколлинеарности в линейной регрессии.\n",
    "   - Вычислены наиболее значимые частоты. Посчитанв лаги, скользящие средние и приросты скользящих для лагов. Это все суть новые фичи.\n",
    "   - Разделение данных на тренировочную (2021-2023гг), тестовую (2024) выборки.\n",
    "   - Категориальных признаков не было.\n",
    "   - сдвинул назад Трипс-таргет, чтобы он стал таргетом.\n",
    "02_aggfe.ipynb\n",
    "\n",
    "4. **Машинное обучение:** Использовалась простая линейная регрессия без тюнинга и без регуляризации. Считались модели для каждого гео-ключа. Вырезалась и кэшировалась в ОЗУ (для скорости) часть датафрейма, на которой обучалась модель. Данные моделей собирались в итоговый пандас датафрейм. Из вырезанного ДФ удалялись мультколлинеарные поля. Фичи векторизировались и масштабировались (скалер обучен на трэйне и применен на тесте).\n",
    "\n",
    "5. **Оценка модели:** Валидационная выборка не выделялась и кросс-валидация не проводилась. Посчитаны метрики RMSE, MAE, MAPE. Средние значения соответственно:\n",
    "```yaml\n",
    "| Statistic | RMSE_Train | MAE_Train | MAPE_Train | RMSE_Test | MAE_Test | MAPE_Test |\n",
    "|-----------|------------|-----------|------------|-----------|----------|-----------|\n",
    "| Mean      | 2.810005   | 1.945298  | 55.284813   | 3.140904  | 2.153357 | 55.831406 |\n",
    "| Std       | 3.234132   | 2.232060  | 12.121400   | 4.106706  | 2.800895 | 12.498734 |\n",
    "| Min       | 0.322773   | 0.170916  | 23.589202   | 0.348340  | 0.193614 | 17.662376 |\n",
    "| 25%       | 0.907780   | 0.636100  | 46.940058   | 0.970418  | 0.624346 | 47.566825 |\n",
    "| 50%       | 1.457101   | 1.050917  | 52.464250   | 1.600986  | 1.142249 | 52.625650 |\n",
    "| 75%       | 3.560571   | 2.401178  | 61.051702   | 3.410993  | 2.246624 | 63.417640 |\n",
    "| Max       | 17.673250  | 11.626479 | 88.378262   | 22.965733 | 16.667246 | 89.031368 |\n",
    "\n",
    "```\n",
    "Модель имеет хорошую обобщающую способность.  \n",
    "Важность признаков не анализировалась.\n",
    "\n",
    "**Результаты проекта:**\n",
    "\n",
    "- LinearRegression справляется.\n",
    "- Модель демонстрирует хорошие средние показатели качества с MAE меньше 2.\n",
    "\n",
    "**Выводы:**\n",
    "\n",
    "В процессе работы над проектом выучил ООП, немного бэкенда, Докер, немного Баша, логирование, ну и конечно же PySpark. Создан вычислительный кластер. Проведена большая работа по предобработке данных и конструированию признаков. Как всегда предподготовка обеспечивает успех даже для простой модели.\n",
    "\n",
    "**Дальнейшая работа над проектом \"Прогнозирование спроса на такси\" может включать в себя следующие улучшения:**\n",
    "\n",
    "1. **Инженерия признаков:**\n",
    "\n",
    "- Разработать дополнительные временные признаки, такие как выходные дни, праздники, сезонные факторы.\n",
    "- Исследовать и использовать информацию о погодных условиях, событиях в городе, которые могут влиять на спрос на такси.\n",
    "\n",
    "2. **Улучшение модели:**\n",
    "\n",
    "- Попробовать другие модели машинного обучения, такие как нейронные сети, временные ряды ARIMA или SARIMA, для сравнения и выбора наилучшей.\n",
    "- Дополнительная настройка гиперпараметров модели для улучшения ее производительности.\n",
    "- Рассмотреть ансамблевые методы, чтобы комбинировать преимущества разных моделей.\n",
    "- прогнозировать именно всплески, т.е. превышение прогнозов (используя дискретизацию отклонений от прогноза и классификацию таких отклонений - как вариант)\n",
    "- Графический анализ - пики, впадины..\n",
    "\n",
    "3. **Связь между районами:**\n",
    "\n",
    "- Оедельно улучшить (проанализировать) результаты по районам с плохими метриками\n",
    "- Оценить потоки между районами\n",
    "- Классифицировать ряды. Анализировать ряды не изолированно по районам."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1746f0-53c9-43af-ad7d-c59548b413c0",
   "metadata": {},
   "source": [
    "## Project FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38acdf-e6d0-46fc-9e11-6565f85e6b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
