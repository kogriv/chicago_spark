{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ac4a7a-4644-45d3-8786-944ff88f222a",
   "metadata": {},
   "source": [
    "Ноутбук по проекту chicago_spark.  \n",
    "Агрегация данных по гео-ключам (районы, округа) и временным промежуткам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de64cacf-4b21-4fc7-85ca-0d7b7de73885",
   "metadata": {},
   "source": [
    "## Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f9b287-4613-4f96-9125-2472424b9cfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c57728-71d4-43e9-a453-c5d23d6a57a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.signal import welch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbb9443-57d6-4aab-ad7d-c67719e98e03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from itertools import islice\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d04534c3-1092-4cb4-9ed0-2a6b99acbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c506d768-519f-49a0-9af6-b44cbcf036bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enviserv.dictan import DictAnalyzer # анализ словарей\n",
    "import pandserv as pds # форматирование небольших пандас ДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c22c0b8-1ea3-417e-8289-34ca57326d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkserv import SparkApp, Cols\n",
    "# в SparkApp упакованы функции создания спарк приложения \n",
    "# с определением IP мастер-ноды и с подключением к кластеру\n",
    "\n",
    "# Col - класс для формирования коротких псевдонимов имен столбцов\n",
    "# при этом исходные имена полей не меняются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "63596c0d-9087-4e1a-80e3-a1fd7a90babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db2ec96-2997-458c-b67a-f9e5dd6aa586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# гео библиотеки\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e47d77-cebf-4472-99e9-c2e852867df5",
   "metadata": {},
   "source": [
    "## Создание сессии, загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c292a0-9c16-44d8-8aa6-e5a51b5e08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:spark_master_ip: 172.18.0.2\n",
      "INFO:spark_app:pyspark version: 3.4.1\n",
      "INFO:spark_app:starting building spark app object: pyspark-taxi-forecasting\n",
      "INFO:spark_app:Spark app object built as: <pyspark.sql.session.SparkSession object at 0x7f7e92efd710>\n",
      "INFO:spark_app:==================================================================\n",
      "INFO:spark_app:Spark object can be accessed as the SparkApp_object.spark property\n",
      "INFO:spark_app:==================================================================\n"
     ]
    }
   ],
   "source": [
    "spark_app = SparkApp(my_logger_create_level = 'INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4de973af-73c4-43f8-90b6-b1d422a3360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:spark_master_ip: 172.18.0.2\n"
     ]
    }
   ],
   "source": [
    "spark_master_ip = spark_app.get_spark_master_ip()\n",
    "# print(spark_master_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55a91437-60e9-4bc2-a9cb-58365f000efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = spark_app.build_spark_app(spark_master_ip=spark_master_ip)\n",
    "# spark = spark_app.spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c7d30-9eed-479c-b899-2fcd6c2d78d4",
   "metadata": {},
   "source": [
    "Для корректного завершения спарк-сессии (например, для переключения между ноутбуками) следует останавливать сессию полностью. Для этого использую метод .stop_spark_app() класса SparkApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "412582e5-44f8-44dd-a8f0-7d9fe46ad1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_app.stop_spark_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b17caf8-7f00-4c20-a83b-744c2d0ab81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f7e92efd710>\n"
     ]
    }
   ],
   "source": [
    "print(spark_app.spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5221d404-34f2-4fd8-8b0c-531fd6f93bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:pyspark version: 3.4.1\n",
      "INFO:spark_app:starting building spark app object: pyspark-taxi-forecasting\n",
      "INFO:spark_app:Spark app object built as: <pyspark.sql.session.SparkSession object at 0x7f7e92efd710>\n",
      "INFO:spark_app:==================================================================\n",
      "INFO:spark_app:Spark object can be accessed as the SparkApp_object.spark property\n",
      "INFO:spark_app:==================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d10fb5fe9c53:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://172.18.0.2:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-taxi-forecasting</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7e92efd710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_app.build_spark_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab1b14-51a4-4d05-9ada-4217282f13ce",
   "metadata": {},
   "source": [
    "Получим стандартный объект `spark` из созданного выше объекта `spark_app`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56cdf778-5944-484e-a72f-6e151b2ec3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = spark_app.spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca259b2f-ac8d-4de8-800b-754588a3963c",
   "metadata": {},
   "source": [
    "Проверка работы спарк-объекта на кластере. Если все в порядке, то тест должен выполниться достаточно быстро и отобразить тестовый ДФ.  \n",
    "```txt\n",
    "+------------+-----------+\n",
    "|student_name|student_age|\n",
    "+------------+-----------+\n",
    "|       Alice|         10|\n",
    "|         Bob|         20|\n",
    "+------------+-----------+\n",
    "```\n",
    "\n",
    "Если исходные образы кластера собраны с ошибкой, возможно \"зависание\" работы теста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65e12fff-3cf5-4918-b8f7-d4ce0d306949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n",
      "DataFrame created successfully.\n",
      "Alias DataFrame created successfully.\n",
      "DataFrame data matches expected result.\n",
      "+------------+-----------+\n",
      "|student_name|student_age|\n",
      "+------------+-----------+\n",
      "|       Alice|         10|\n",
      "|         Bob|         20|\n",
      "+------------+-----------+\n",
      "\n",
      "DataFrame show output matches expected output.\n",
      "\n",
      "=======use======================\n",
      "*      ____              __    *\n",
      "*     / __/__  ___ _____/ /__  *\n",
      "*    _\\ \\/ _ \\/ _ `/ __/  '_/  *\n",
      "*   /__ / .__/\\_,_/_/ /_/\\_\\   *\n",
      "*      /_/                     * \n",
      "================================\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "spark_app.test_spark_functionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd529952-7b42-4dff-bf7b-d4a3d304ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = DictAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a49c4e5f-8973-485c-8714-94448bbe1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция простой рандомизированной выборки\n",
    "def random_sample_dataframe(dataframe, percentage):\n",
    "    # Генерируем случайные числа от 0 до 1 и фильтруем строки\n",
    "    df = dataframe.filter(f.rand() < percentage)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62ddc595-f4f4-4033-b890-cf42b5c3d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# agg_hour.coalesce(1).write.csv(\"/work/data/taxis_agg_hour_growth.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c59bfe-cbcc-4a69-8cb4-c6931b61b50a",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb679a8d-5f92-49ab-80b2-d3a76792f376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 ms, sys: 10.6 ms, total: 22.2 ms\n",
      "Wall time: 43.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "agg_hour = spark.read.load('/work/data/taxis_agg_hour_growth.csv', \n",
    "                       format='csv', header='true'\n",
    "                        , inferSchema='true'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df4f4d57-25f7-4c70-976f-fd4452ca779f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3079450"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_hour.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9d40cea-0b3e-4396-b995-928bfc94e0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ct: bigint, ca: int, hour_start: timestamp, trips_p: int, time_p: int, miles_p: double, velocity_p: double, farem_p: double, tipsm_p: double, tollsm_p: double, extrasm_p: double, totalm_p: double, comp1_p: int, comp2_p: int, comp3_p: int, comp4_p: int, comp5_p: int, compless5_p: int, trips_d: int, time_d: int, miles_d: double, velocity_d: double, farem_d: double, tipsm_d: double, tollsm_d: double, extrasm_d: double, totalm_d: double, comp1_d: int, comp2_d: int, comp3_d: int, comp4_d: int, comp5_d: int, compless5_d: int, cumulative_balance: int, trips_p_growth_1_to_0: double, trips_p_growth_2_to_1: double, trips_p_growth_3_to_2: double, trips_p_growth_4_to_3: double, trips_d_growth_1_to_0: double, trips_d_growth_2_to_1: double, trips_d_growth_3_to_2: double, trips_d_growth_4_to_3: double, velocity_p_growth_1_to_0: double, velocity_p_growth_2_to_1: double, velocity_p_growth_3_to_2: double, velocity_p_growth_4_to_3: double, velocity_d_growth_1_to_0: double, velocity_d_growth_2_to_1: double, velocity_d_growth_3_to_2: double, velocity_d_growth_4_to_3: double]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_hour.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ade35b3b-acac-452e-ac1c-82be92d46759",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_null = agg_hour.select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in agg_hour.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb0aa96f-4e05-4dab-83ea-23dabb49fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_null = agg_null.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62603269-3dbf-45c6-a9ec-7ec4934cf033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [0]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_null[agg_null.iloc[:, 0] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7569a36-ee74-487e-b86b-438e5356fdb2",
   "metadata": {},
   "source": [
    "Загрузим данные по исключаемым (высокоррелированным) полям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bdae2d7-8b16-4707-966d-6b83e15c9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_f = pd.read_csv('/work/data/excluded_fields.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3db3af7-e825-4568-b711-a9fd2434cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для преобразования DataFrame в словарь\n",
    "def get_excuded_fields_to_dict(df):\n",
    "    excluded_fields_tot = {}\n",
    "    grouped = df.groupby(['ct', 'ca'])\n",
    "    for (ct, ca), group in grouped:\n",
    "        excluded_fields_tot[(ct, ca)] = group['excluded_field'].tolist()\n",
    "    return excluded_fields_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f8afe7e-4736-430c-a597-71eb61c6a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_fields_tot = get_excuded_fields_to_dict(excl_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a974c60-8161-4c21-9f67-438d203bca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    (10000000000, 91): ['time_p', 'miles_p', 'farem_p', 'comp1_p', 'comp3_p', 'comp4_p', 'comp5_p', 'extrasm_p']\n",
      "    (12000000001, 53): ['time_p', 'farem_p', 'trips_d', 'time_d', 'miles_d', 'farem_d', 'totalm_p', 'velocity_p_growth_3_to_2']\n",
      "    (12000000002, 75): ['time_p', 'miles_p', 'farem_p', 'tipsm_p', 'comp5_p', 'trips_d', 'time_d', 'miles_d', 'farem_d', 'tipsm_d', 'comp4_p']\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "da.print_dict(dict(islice(excluded_fields_tot.items(), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab429d-b219-4826-aab2-bbcaee4526ed",
   "metadata": {},
   "source": [
    "Будем считать, что от мультиколлинеарности в линейных моделях с помощью исключения этих полей получится избавиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f44b76c1-cfab-4115-a7ce-d840d7bd9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# agg_hour.coalesce(1).write.csv(\"/work/data/taxis_agg_hour_result.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f705c08-5874-478d-a978-3646ba2e282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.1 ms, sys: 11 ms, total: 30.1 ms\n",
      "Wall time: 45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = spark.read.load('/work/data/taxis_agg_hour_result.csv', \n",
    "                       format='csv', header='true'\n",
    "                        , inferSchema='true'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05653d0b-af65-48d4-98dc-76b4ccaa856e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ct: bigint, ca: int, hour_start: timestamp, time_p: int, miles_p: double, velocity_p: double, farem_p: double, tipsm_p: double, tollsm_p: double, extrasm_p: double, totalm_p: double, comp1_p: int, comp2_p: int, comp3_p: int, comp4_p: int, comp5_p: int, compless5_p: int, trips_d: int, time_d: int, miles_d: double, velocity_d: double, farem_d: double, tipsm_d: double, tollsm_d: double, extrasm_d: double, totalm_d: double, comp1_d: int, comp2_d: int, comp3_d: int, comp4_d: int, comp5_d: int, compless5_d: int, cumulative_balance: int, trips_p_growth_1_to_0: double, trips_p_growth_2_to_1: double, trips_p_growth_3_to_2: double, trips_p_growth_4_to_3: double, trips_d_growth_1_to_0: double, trips_d_growth_2_to_1: double, trips_d_growth_3_to_2: double, trips_d_growth_4_to_3: double, velocity_p_growth_1_to_0: double, velocity_p_growth_2_to_1: double, velocity_p_growth_3_to_2: double, velocity_p_growth_4_to_3: double, velocity_d_growth_1_to_0: double, velocity_d_growth_2_to_1: double, velocity_d_growth_3_to_2: double, velocity_d_growth_4_to_3: double, trips_sh_168: int, trips_sh_84: int, trips_sh_24: int, trips_sh_28: int, trips_sh_12: int, trips_sh_8: int, trips_ma_168: double, trips_ma_84: double, trips_ma_24: double, trips_ma_28: double, trips_ma_12: double, trips_ma_8: double, trips_sh_4: int, trips_ma_4: double, trips_sh_1: int, trips_ma_1: double, trips_ma_168_growth: double, trips_ma_8_growth: double, trips_ma_4_growth: double, trips_target: int]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pds.gvf(data.count()))\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663d6b9-90ea-4f36-aea7-1520d63a0d8c",
   "metadata": {},
   "source": [
    "### Фрагмент данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699374f-3d4b-4d28-b2fb-d9ddca03138c",
   "metadata": {},
   "source": [
    "Отберем несколько полей и один район для проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08662517-2f13-4728-bfde-a27c48d289a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-------------------+------------+------+-------+--------------------+\n",
      "|         ct| ca|         hour_start|trips_target|time_p|miles_p|          velocity_p|\n",
      "+-----------+---+-------------------+------------+------+-------+--------------------+\n",
      "|17031090200|  6|2021-01-08 01:00:00|           1|     0|    0.0|                 0.0|\n",
      "|17031090200|  6|2021-01-08 02:00:00|           2|   706|   3.24|0.004589235127478754|\n",
      "|17031090200|  6|2021-01-08 03:00:00|           1|   990|    2.9|0.002929292929292929|\n",
      "+-----------+---+-------------------+------------+------+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_to_sel = [\n",
    "    'ct', 'ca', 'hour_start', 'trips_target', 'time_p', 'miles_p', 'velocity_p',\n",
    "]\n",
    "\n",
    "data_sample = data.select(*f_to_sel).filter(f.col('ct')==17031090200)\n",
    "data_sample.show(3)\n",
    "# data_sample.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6b49f0f-b5ee-4c40-ad16-d5d0a279f357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time_p miles_p farem_p tipsm_p comp1_p comp3_p comp4_p trips_d time_d miles_d farem_d tipsm_d extrasm_d comp2_d comp3_d comp4_d extrasm_p comp5_p'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(excluded_fields_tot[(17031090200,6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106c7ab-1c92-4eea-94c7-e39f28e04ad9",
   "metadata": {},
   "source": [
    "Данные загрузились. Общее количество и отображение фрагмента соответствуют ожиданиям. Исключаемые поля в доступе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e27b1-a765-43c4-9406-06b99c337e15",
   "metadata": {},
   "source": [
    "## Подготовка данных к МО"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7437836-0bec-433c-b814-482e9b205b49",
   "metadata": {},
   "source": [
    "Векторизация признаков"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14537eb1-164d-4666-b843-32f5075cd0af",
   "metadata": {},
   "source": [
    "Векторизация признаков в данном блокноте закомментирована, т.к. не получилось сохранить-загрузить датафрейм со столбцом вектором. А даже кэшированный датафрейм с вектор-столбцом не получается обрабатывать из-за нехватки памяти и выполнение очень долгое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc2d5694-b4e2-429c-a714-36769278f089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ct: bigint, ca: int, hour_start: timestamp, time_p: int, miles_p: double, velocity_p: double, farem_p: double, tipsm_p: double, tollsm_p: double, extrasm_p: double, totalm_p: double, comp1_p: int, comp2_p: int, comp3_p: int, comp4_p: int, comp5_p: int, compless5_p: int, trips_d: int, time_d: int, miles_d: double, velocity_d: double, farem_d: double, tipsm_d: double, tollsm_d: double, extrasm_d: double, totalm_d: double, comp1_d: int, comp2_d: int, comp3_d: int, comp4_d: int, comp5_d: int, compless5_d: int, cumulative_balance: int, trips_p_growth_1_to_0: double, trips_p_growth_2_to_1: double, trips_p_growth_3_to_2: double, trips_p_growth_4_to_3: double, trips_d_growth_1_to_0: double, trips_d_growth_2_to_1: double, trips_d_growth_3_to_2: double, trips_d_growth_4_to_3: double, velocity_p_growth_1_to_0: double, velocity_p_growth_2_to_1: double, velocity_p_growth_3_to_2: double, velocity_p_growth_4_to_3: double, velocity_d_growth_1_to_0: double, velocity_d_growth_2_to_1: double, velocity_d_growth_3_to_2: double, velocity_d_growth_4_to_3: double, trips_sh_168: int, trips_sh_84: int, trips_sh_24: int, trips_sh_28: int, trips_sh_12: int, trips_sh_8: int, trips_ma_168: double, trips_ma_84: double, trips_ma_24: double, trips_ma_28: double, trips_ma_12: double, trips_ma_8: double, trips_sh_4: int, trips_ma_4: double, trips_sh_1: int, trips_ma_1: double, trips_ma_168_growth: double, trips_ma_8_growth: double, trips_ma_4_growth: double, trips_target: int]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3953ead8-20da-4859-bf00-ee2b6098c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def all_output_columns(df,target_variable_name):\n",
    "#     base_cols = df.columns\n",
    "#     base_cols.remove(target_variable_name)\n",
    "#     selectedCols = [target_variable_name, 'features'] + base_cols\n",
    "#     return selectedCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68bf10d8-2625-4c2a-b4ec-8106d7819855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def assemble_vectors(df, features_list, target_variable_name):\n",
    "#     stages = []\n",
    "#     # assemble vectors\n",
    "#     assembler = VectorAssembler(inputCols=features_list, outputCol='features')\n",
    "#     stages = [assembler]\n",
    "#     # select all the columns + target + newly created 'features' column\n",
    "#     selectedCols = all_output_columns(df,target_variable_name)\n",
    "#     # use pipeline to process sequentially\n",
    "#     pipeline = Pipeline(stages=stages)\n",
    "#     # assemble model\n",
    "#     assembleModel = pipeline.fit(df)\n",
    "#     # apply assembler model on data\n",
    "#     df = assembleModel.transform(df).select(selectedCols)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06396d4b-64c9-4749-aebc-74f89e4ce7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение уникальных комбинаций районов и округов\n",
    "# geo_keys = data.select('ct', 'ca').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68cebea1-c64d-48e9-9fb5-d9f2c5dd4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# исключаемые поля для всех районов\n",
    "# exc_cols = ['ct', 'ca', 'hour_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab01cde6-17a9-48f6-9287-f53b885bf601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # векторизация\n",
    "# df_vec = None\n",
    "# for row in tqdm(geo_keys):\n",
    "    \n",
    "#     ct = row['ct']\n",
    "#     ca = row['ca']\n",
    "#     # Фильтрация данных для текущего района и округа\n",
    "#     geo_data = data.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "#     # print(row,geo_data.count())\n",
    "#     excluded_fields_list = excluded_fields_tot[row] + exc_cols\n",
    "#     features_list = [col for col in data.columns if col not in excluded_fields_list]\n",
    "#     # print(features_list)\n",
    "#     assembled_data = assemble_vectors(df=geo_data,\n",
    "#                                 features_list=features_list,\n",
    "                                \n",
    "#                                 target_variable_name='trips_target')\n",
    "#     # print(assembled_data.count())\n",
    "#     # Если это первая итерация, инициализируем df_vec\n",
    "#     if df_vec is None:\n",
    "#         df_vec = assembled_data\n",
    "#     else:\n",
    "#         df_vec = df_vec.union(assembled_data)\n",
    "#         # print(df_vec.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17ac098d-ef32-4628-a70a-145f48e72d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание нового пустого DataFrame с той же схемой\n",
    "# empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), geo_data.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2ab4ab9-7a78-41bc-9e63-dc193edc6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vec.show(3)\n",
    "# df_vec.cache()\n",
    "# print(pds.gvf(df_vec.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b50e09af-ce57-44b0-9fc3-ea3330971b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_to_sel = [\n",
    "#     'ct', 'ca', 'hour_start', 'trips_target', 'features',\n",
    "# ]\n",
    "\n",
    "# df_vec.select(*f_to_sel).filter(f.col('ct')==17031838200).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b1511a2-9748-4b09-a014-c85268486732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if df_vec.is_cached:\n",
    "#  print(\"DataFrame закэширован\")\n",
    "# else:\n",
    "#  print(\"DataFrame не закэширован\")\n",
    "\n",
    "# if df_vec.storageLevel == StorageLevel.MEMORY_ONLY:\n",
    "#  print(\"DataFrame закэширован в памяти\")\n",
    "# elif df_vec.storageLevel == StorageLevel.DISK_ONLY:\n",
    "#  print(\"DataFrame закэширован на диске\")\n",
    "# else:\n",
    " # print(\"DataFrame закэширован в:\",df_vec.storageLevel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfac8fae-4225-44ec-ad09-3d174e3ba4ed",
   "metadata": {},
   "source": [
    "данные DataFrame были закэшированы с использованием уровня хранения Disk Memory Deserialized 1x Replicated. Это значит, что данные сначала сериализуются и сохраняются на диске, а затем десериализуются в память для обработки. Также данные были реплицированы один раз, что обеспечивает дополнительную надежность и доступность данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f24b584a-9310-40e8-ba2e-991a304a8157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n",
      "CPU times: user 38.3 ms, sys: 9.54 ms, total: 47.8 ms\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(pds.gvf(df_vec.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a92740d9-8b6c-47ec-88d6-870e9961eb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3'062'043\n",
      "CPU times: user 10.7 ms, sys: 83 µs, total: 10.7 ms\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# print(pds.gvf(data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8da93d-bcd7-4038-bc56-c303540c2f3e",
   "metadata": {},
   "source": [
    "На данный момент непонятна выгода от векторизации. При более медленном выполнении действий, нет пока понятного способа корректно сохранить-загрузить веторизированный ДФ. В csv нельзя сохранить тип vector, в parquet - можно, но после загрузки выдаются ошибки работы JVM объектов.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e017dad-a9ec-4543-ad44-e2070723e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vec.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19a1ac-4f58-4a44-9357-4b7350b64e16",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc71bd-1391-4ff9-a8a5-a3c795b7ee9e",
   "metadata": {},
   "source": [
    "Разделю ДФ на обучающую и тестовую выборки: тест - 2024 год, трэйн - 2021-2023 гг. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8cee04c1-f12a-4d4e-a7d8-b735bfc92e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на тренировочный и тестовый наборы\n",
    "train = data.filter(f.year(f.col(\"hour_start\")) < 2024)\n",
    "test = data.filter(f.year(f.col(\"hour_start\")) == 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f68a087f-4677-4983-a7d7-9541252f5e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2'686'518\n",
      "375'525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[ct: bigint, ca: int, hour_start: timestamp, time_p: int, miles_p: double, velocity_p: double, farem_p: double, tipsm_p: double, tollsm_p: double, extrasm_p: double, totalm_p: double, comp1_p: int, comp2_p: int, comp3_p: int, comp4_p: int, comp5_p: int, compless5_p: int, trips_d: int, time_d: int, miles_d: double, velocity_d: double, farem_d: double, tipsm_d: double, tollsm_d: double, extrasm_d: double, totalm_d: double, comp1_d: int, comp2_d: int, comp3_d: int, comp4_d: int, comp5_d: int, compless5_d: int, cumulative_balance: int, trips_p_growth_1_to_0: double, trips_p_growth_2_to_1: double, trips_p_growth_3_to_2: double, trips_p_growth_4_to_3: double, trips_d_growth_1_to_0: double, trips_d_growth_2_to_1: double, trips_d_growth_3_to_2: double, trips_d_growth_4_to_3: double, velocity_p_growth_1_to_0: double, velocity_p_growth_2_to_1: double, velocity_p_growth_3_to_2: double, velocity_p_growth_4_to_3: double, velocity_d_growth_1_to_0: double, velocity_d_growth_2_to_1: double, velocity_d_growth_3_to_2: double, velocity_d_growth_4_to_3: double, trips_sh_168: int, trips_sh_84: int, trips_sh_24: int, trips_sh_28: int, trips_sh_12: int, trips_sh_8: int, trips_ma_168: double, trips_ma_84: double, trips_ma_24: double, trips_ma_28: double, trips_ma_12: double, trips_ma_8: double, trips_sh_4: int, trips_ma_4: double, trips_sh_1: int, trips_ma_1: double, trips_ma_168_growth: double, trips_ma_8_growth: double, trips_ma_4_growth: double, trips_target: int]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pds.gvf(train.count()))\n",
    "train.cache()\n",
    "print(pds.gvf(test.count()))\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa243414-d169-41a5-92da-b037ea7e4ba6",
   "metadata": {},
   "source": [
    "Масштабирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "347d0590-a2d7-490f-a81f-d7428aa6f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение уникальных комбинаций районов и округов\n",
    "geo_keys = data.select('ct', 'ca').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc8e6f2c-b4b4-4d1b-9628-e0b36c92809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# исключаемые поля для всех районов\n",
    "exc_cols = ['ct', 'ca', 'hour_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c1fde410-8b3b-4d0e-91dc-83135ad080f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitted_scaler(df, featureCol, outputCol):\n",
    "    \n",
    "    stages = []\n",
    "\n",
    "    scaler = StandardScaler(inputCol = featureCol,\n",
    "                            outputCol=outputCol,\n",
    "                            withStd=True, withMean=True\n",
    "                           )\n",
    "    \n",
    "    stages = [scaler]\n",
    "    \n",
    "    # use pipeline to process sequentially\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    # assemble model\n",
    "    scaledAssembleModel = pipeline.fit(df)\n",
    "\n",
    "    return scaledAssembleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b7c0c08d-6d62-4dbf-ba3f-5a5ac0cee8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = data.columns\n",
    "target_variable_name = 'trips_target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f49d2c7f-4ad2-42a1-a26c-e1854792024d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time_p miles_p farem_p tipsm_p comp1_p comp3_p comp4_p trips_d time_d miles_d farem_d tipsm_d extrasm_d comp2_d comp3_d comp4_d extrasm_p comp5_p'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_point = (17031090200,6)\n",
    "\" \".join(excluded_fields_tot[test_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "09611c50-a2ed-4cae-b243-5a8ebb3a011e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17031090200"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_point[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fdf67b9b-0eb8-4ec2-b85a-06c0b7b8a335",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_and_std' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m mean_and_std_cols\u001b[38;5;241m=\u001b[39m[c \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m features_list \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m      8\u001b[0m     (f\u001b[38;5;241m.\u001b[39mmean(col)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m),f\u001b[38;5;241m.\u001b[39mstddev(col)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_std\u001b[39m\u001b[38;5;124m\"\u001b[39m))]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# mean_and_std_cols\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# mean_and_std = test_df.select(mean_and_std_cols).first()\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m scaled_cols\u001b[38;5;241m=\u001b[39m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean_and_std\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mmean_and_std\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_std\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_s\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_list\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# test_df = test_df.select(test_df.columns + scaled_cols)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m scaled_cols\n",
      "Cell \u001b[0;32mIn[93], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m mean_and_std_cols\u001b[38;5;241m=\u001b[39m[c \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m features_list \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m      8\u001b[0m     (f\u001b[38;5;241m.\u001b[39mmean(col)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m),f\u001b[38;5;241m.\u001b[39mstddev(col)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_std\u001b[39m\u001b[38;5;124m\"\u001b[39m))]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# mean_and_std_cols\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# mean_and_std = test_df.select(mean_and_std_cols).first()\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m scaled_cols\u001b[38;5;241m=\u001b[39m[((f\u001b[38;5;241m.\u001b[39mcol(col) \u001b[38;5;241m-\u001b[39m \u001b[43mmean_and_std\u001b[49m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m/\u001b[39mmean_and_std[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_std\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_s\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m features_list]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# test_df = test_df.select(test_df.columns + scaled_cols)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m scaled_cols\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_and_std' is not defined"
     ]
    }
   ],
   "source": [
    "ct, ca = test_point\n",
    "\n",
    "excluded_fields_list = excluded_fields_tot[(ct,ca)] + exc_cols\n",
    "features_list = [col for col in data.columns if col not in excluded_fields_list]\n",
    "scaled_features_list = [f\"{col}_scaled\" for col in features_list]\n",
    "\n",
    "mean_and_std_cols=[c for col in features_list for c in \n",
    "    (f.mean(col).alias(f\"{col}_mean\"),f.stddev(col).alias(f\"{col}_std\"))]\n",
    "\n",
    "# mean_and_std_cols\n",
    "\n",
    "# mean_and_std = test_df.select(mean_and_std_cols).first()\n",
    "scaled_cols=[((f.col(col) - mean_and_std[f\"{col}_mean\"])\n",
    "    /mean_and_std[f\"{col}_std\"]).alias(f\"{col}_scaled\") for col in features_list]\n",
    "# test_df = test_df.select(test_df.columns + scaled_cols)\n",
    "scaled_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3ba663a7-34b1-483f-ae99-5039e0c8f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in tqdm(geo_keys):\n",
    "    \n",
    "#     ct = row['ct']\n",
    "#     ca = row['ca']\n",
    "    \n",
    "def lin_regr_with_scale(train, test, point):\n",
    "\n",
    "    ct = point[0]\n",
    "    ca = point[1]\n",
    "    geo_train = train.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "    geo_train.cache()\n",
    "    geo_test = test.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "    geo_test.cache()\n",
    "\n",
    "    \n",
    "    excluded_fields_list = excluded_fields_tot[(ct,ca)] + exc_cols\n",
    "    features_list = [col for col in data.columns if col not in excluded_fields_list]\n",
    "    scaled_features_list = [f\"{col}_scaled\" for col in features_list]\n",
    "\n",
    "    # масштабируем в лоб, поскольку работаем не с ветор-столбцом\n",
    "    mean_and_std_cols=[c for col in features_list for c in \n",
    "    (f.mean(col).alias(f\"{col}_mean\"),f.stddev(col).alias(f\"{col}_std\"))]\n",
    "    \n",
    "    mean_and_std = geo_train.select(mean_and_std_cols).first()\n",
    "    scaled_cols=[((f.col(col) - mean_and_std[f\"{col}_mean\"])\n",
    "        /mean_and_std[f\"{col}_std\"]).alias(f\"{col}_scaled\") for col in features_list]\n",
    "    scaled_geo_train = geo_train.select(geo_train.columns + scaled_cols)\n",
    "    scaled_geo_test = geo_test.select(geo_test.columns + scaled_cols)\n",
    "    \n",
    "    selectedCols = exc_cols + [target_variable_name] + scaled_features_list\n",
    "\n",
    "    # for feat in features_list:\n",
    "    #     scaled_feat = f\"{feat}_scaled\"\n",
    "    #     geo_scaler = fitted_scaler(geo_train, feat, scaled_feat)\n",
    "    #     scaled_geo_train = geo_scaler.transform(geo_train)\n",
    "    #     scaled_geo_test = geo_scaler.transform(geo_test)\n",
    "\n",
    "    scaled_geo_train = scaled_geo_train.select(selectedCols)\n",
    "    scaled_geo_test = scaled_geo_test.select(selectedCols)\n",
    "    \n",
    "    reg = LinearRegression(featuresCol = scaled_features_list, labelCol = target_variable_name)\n",
    "\n",
    "    reg_model = reg.fit(scaled_geo_train)\n",
    "    \n",
    "    # Создание объекта RegressionEvaluator для оценки модели\n",
    "    evaluator = RegressionEvaluator()\n",
    "\n",
    "    # Настройка параметров оценки (RMSE и MAPE)\n",
    "    evaluator.setMetricName(\"rmse\")\n",
    "    evaluator.setPredictionCol(\"prediction\")\n",
    "    evaluator.setLabelCol(\"label\")\n",
    "\n",
    "    # Оценка модели\n",
    "    rmse = evaluator.evaluate(reg_model.transform(scaled_geo_train))\n",
    "    \n",
    "    # Аналогично для MAPE\n",
    "    evaluator.setMetricName(\"mape\")\n",
    "    mape = evaluator.evaluate(reg_model.transform(scaled_geo_train))\n",
    "\n",
    "    return reg_model, scaled_geo_train, rmse, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "17ebb4e7-57b3-4d10-928d-4b8c8f52e514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"featuresCol\". Could not convert <class 'list'> to string type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:503\u001b[0m, in \u001b[0;36mParams._set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypeConverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:236\u001b[0m, in \u001b[0;36mTypeConverters.toString\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to string type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(value))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert <class 'list'> to string type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reg_model, scaled_geo_train, rmse, mape \u001b[38;5;241m=\u001b[39m \u001b[43mlin_regr_with_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# scaled_geo_train = lin_regr_with_scale(train, test, test_point)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[98], line 41\u001b[0m, in \u001b[0;36mlin_regr_with_scale\u001b[0;34m(train, test, point)\u001b[0m\n\u001b[1;32m     38\u001b[0m scaled_geo_train \u001b[38;5;241m=\u001b[39m scaled_geo_train\u001b[38;5;241m.\u001b[39mselect(selectedCols)\n\u001b[1;32m     39\u001b[0m scaled_geo_test \u001b[38;5;241m=\u001b[39m scaled_geo_test\u001b[38;5;241m.\u001b[39mselect(selectedCols)\n\u001b[0;32m---> 41\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaled_features_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_variable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m reg_model \u001b[38;5;241m=\u001b[39m reg\u001b[38;5;241m.\u001b[39mfit(scaled_geo_train)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Создание объекта RegressionEvaluator для оценки модели\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/regression.py:332\u001b[0m, in \u001b[0;36mLinearRegression.__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.regression.LinearRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid\n\u001b[1;32m    330\u001b[0m )\n\u001b[1;32m    331\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetParams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/regression.py:363\u001b[0m, in \u001b[0;36mLinearRegression.setParams\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03msetParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m          maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03mSets params for linear regression.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    362\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py:505\u001b[0m, in \u001b[0;36mParams._set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             value \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mtypeConverter(value)\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 505\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid param value given for param \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (p\u001b[38;5;241m.\u001b[39mname, e))\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paramMap[p] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid param value given for param \"featuresCol\". Could not convert <class 'list'> to string type"
     ]
    }
   ],
   "source": [
    "reg_model, scaled_geo_train, rmse, mape = lin_regr_with_scale(train, test, test_point)\n",
    "# scaled_geo_train = lin_regr_with_scale(train, test, test_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "97b0848e-7bed-47f4-ae8a-dc6fc9de4749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ct ca hour_start time_p miles_p velocity_p farem_p tipsm_p tollsm_p extrasm_p totalm_p comp1_p comp2_p comp3_p comp4_p comp5_p compless5_p trips_d time_d miles_d velocity_d farem_d tipsm_d tollsm_d extrasm_d totalm_d comp1_d comp2_d comp3_d comp4_d comp5_d compless5_d cumulative_balance trips_p_growth_1_to_0 trips_p_growth_2_to_1 trips_p_growth_3_to_2 trips_p_growth_4_to_3 trips_d_growth_1_to_0 trips_d_growth_2_to_1 trips_d_growth_3_to_2 trips_d_growth_4_to_3 velocity_p_growth_1_to_0 velocity_p_growth_2_to_1 velocity_p_growth_3_to_2 velocity_p_growth_4_to_3 velocity_d_growth_1_to_0 velocity_d_growth_2_to_1 velocity_d_growth_3_to_2 velocity_d_growth_4_to_3 trips_sh_168 trips_sh_84 trips_sh_24 trips_sh_28 trips_sh_12 trips_sh_8 trips_ma_168 trips_ma_84 trips_ma_24 trips_ma_28 trips_ma_12 trips_ma_8 trips_sh_4 trips_ma_4 trips_sh_1 trips_ma_1 trips_ma_168_growth trips_ma_8_growth trips_ma_4_growth trips_target velocity_p_scaled tollsm_p_scaled totalm_p_scaled comp2_p_scaled compless5_p_scaled velocity_d_scaled tollsm_d_scaled totalm_d_scaled comp1_d_scaled comp5_d_scaled compless5_d_scaled cumulative_balance_scaled trips_p_growth_1_to_0_scaled trips_p_growth_2_to_1_scaled trips_p_growth_3_to_2_scaled trips_p_growth_4_to_3_scaled trips_d_growth_1_to_0_scaled trips_d_growth_2_to_1_scaled trips_d_growth_3_to_2_scaled trips_d_growth_4_to_3_scaled velocity_p_growth_1_to_0_scaled velocity_p_growth_2_to_1_scaled velocity_p_growth_3_to_2_scaled velocity_p_growth_4_to_3_scaled velocity_d_growth_1_to_0_scaled velocity_d_growth_2_to_1_scaled velocity_d_growth_3_to_2_scaled velocity_d_growth_4_to_3_scaled trips_sh_168_scaled trips_sh_84_scaled trips_sh_24_scaled trips_sh_28_scaled trips_sh_12_scaled trips_sh_8_scaled trips_ma_168_scaled trips_ma_84_scaled trips_ma_24_scaled trips_ma_28_scaled trips_ma_12_scaled trips_ma_8_scaled trips_sh_4_scaled trips_ma_4_scaled trips_sh_1_scaled trips_ma_1_scaled trips_ma_168_growth_scaled trips_ma_8_growth_scaled trips_ma_4_growth_scaled trips_target_scaled'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(scaled_geo_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc7fb8-f9d9-4797-977f-c5f7fbbd0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_geo_test = geo_scaler.transform(geo_test).select(selectedCols)\n",
    "\n",
    "#     # Если это первая итерация, инициализируем df_scaled_\n",
    "#     if df_scaled_train is None:\n",
    "#         df_scaled_train = scaled_geo_train\n",
    "#     else:\n",
    "#         df_scaled_train = df_scaled_train.union(scaled_geo_train)\n",
    "        \n",
    "#     if df_scaled_test is None:\n",
    "#         df_scaled_test = scaled_geo_test\n",
    "#     else:\n",
    "#         df_scaled_test = df_scaled_test.union(scaled_geo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "215ddd75-0773-413d-ae72-8c36e37bd86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# масштабирование\n",
    "# df_scaled_train = None\n",
    "# df_scaled_test = None\n",
    "# for row in tqdm(geo_keys):\n",
    "    \n",
    "#     ct = row['ct']\n",
    "#     ca = row['ca']\n",
    "#     # Фильтрация данных для текущего района и округа\n",
    "#     geo_train = train.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "#     geo_test = test.filter((f.col('ct') == ct) & (f.col('ca') == ca))\n",
    "    \n",
    "#     # print(features_list)\n",
    "#      # select all the columns + target + newly created 'features' column\n",
    "#     geo_scaler = fitted_scaler(geo_train, target_variable_name)\n",
    "#     scaled_geo_train = geo_scaler.transform(geo_train).select(selectedCols)\n",
    "#     scaled_geo_test = geo_scaler.transform(geo_test).select(selectedCols)\n",
    "    \n",
    "#     # print(assembled_data.count())\n",
    "#     # Если это первая итерация, инициализируем df_scaled_\n",
    "#     if df_scaled_train is None:\n",
    "#         df_scaled_train = scaled_geo_train\n",
    "#     else:\n",
    "#         df_scaled_train = df_scaled_train.union(scaled_geo_train)\n",
    "        \n",
    "#     if df_scaled_test is None:\n",
    "#         df_scaled_test = scaled_geo_test\n",
    "#     else:\n",
    "#         df_scaled_test = df_scaled_test.union(scaled_geo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e812d-24f3-45ff-8aea-5ff17fb839a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521e4cee-02df-434b-a30b-1d0010502be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled_train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981750c-a9fe-49a8-a9fa-3e655ae6dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea99e463-9360-473c-9bd5-ebb92186dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scaled_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38acdf-e6d0-46fc-9e11-6565f85e6b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
