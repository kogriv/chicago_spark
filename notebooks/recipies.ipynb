{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d45a451e-f916-4e31-b325-66f6371dc28e",
   "metadata": {},
   "source": [
    "### Ноутбук с рецептами работы в Спарк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f9b287-4613-4f96-9125-2472424b9cfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ec09c2-44e6-46ba-99ec-f18eeb95ecea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from importlib import reload\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8758a9dc-809e-4ed9-b18f-56bd13566448",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 | packaged by conda-forge | (main, Jun 10 2023, 18:08:17) [GCC 12.2.0]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381846bf-27a8-4715-b8c7-11fb92995bdf",
   "metadata": {},
   "source": [
    "### Загрузка пакетов и данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c506d768-519f-49a0-9af6-b44cbcf036bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enviserv.dictan\n",
    "from enviserv.dictan import DictAnalyzer\n",
    "import pandserv as pds # сервисные функции для пандас и не только"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a511f-2197-4e04-9f23-f9b7cf5dd4a0",
   "metadata": {},
   "source": [
    "Создание сессии Спарк с подключением к кластеру упаковано в класс SparkApp.  \n",
    "Обращение с полями датафрейма Спарк- создание псевдонимов - в классе Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c22c0b8-1ea3-417e-8289-34ca57326d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkserv import SparkApp, Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63596c0d-9087-4e1a-80e3-a1fd7a90babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f169e93e-6a6b-46bb-aa00-1b6a3c72591b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c292a0-9c16-44d8-8aa6-e5a51b5e08ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:spark_master_ip: 172.18.0.2\n",
      "INFO:spark_app:pyspark version: 3.4.1\n",
      "INFO:spark_app:starting building spark app object: pyspark-taxi-forecasting\n",
      "INFO:spark_app:Spark app object built as: <pyspark.sql.session.SparkSession object at 0x7f23da56db50>\n",
      "INFO:spark_app:==================================================================\n",
      "INFO:spark_app:Spark object can be accessed as the SparkApp_object.spark property\n",
      "INFO:spark_app:==================================================================\n"
     ]
    }
   ],
   "source": [
    "spark_app = SparkApp(my_logger_create_level = 'INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4de973af-73c4-43f8-90b6-b1d422a3360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_app:spark_master_ip: 172.18.0.2\n"
     ]
    }
   ],
   "source": [
    "spark_master_ip = spark_app.get_spark_master_ip()\n",
    "# print(spark_master_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55a91437-60e9-4bc2-a9cb-58365f000efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = spark_app.build_spark_app(spark_master_ip=spark_master_ip)\n",
    "spark = spark_app.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53f1ffcc-7333-4ca1-acfd-12723c7f253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.7 ms, sys: 23 ms, total: 78.7 ms\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "taxi = spark.read.load('/work/data/Taxi_Trips_-_2022.csv', \n",
    "                       format='csv', header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0eb7218-c83d-45bf-9d0f-7aed46b0d177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Trip ID='bcfa19f2539021c054809d4c3993d226996ae095', Taxi ID='368ce5511598af2cc07efdb68067d381174fdac1d47a2837d021e647f4c8febb3d9bf330e155845b8217b9b451c29b1b9ecf5d55d4bfe6cfd5d79ffb49b1f334', Trip Start Timestamp='01/01/2022 12:00:00 AM', Trip End Timestamp='01/01/2022 12:00:00 AM', Trip Seconds=152, Trip Miles=0.1, Pickup Census Tract=None, Dropoff Census Tract=None, Pickup Community Area=None, Dropoff Community Area=None, Fare=3.75, Tips=0.0, Tolls=0.0, Extras=0.0, Trip Total=3.75, Payment Type='Cash', Company='Medallion Leasin', Pickup Centroid Latitude=None, Pickup Centroid Longitude=None, Pickup Centroid Location=None, Dropoff Centroid Latitude=None, Dropoff Centroid Longitude=None, Dropoff Centroid  Location=None),\n",
       " Row(Trip ID='2aba69ff015f9ea8e7bff43cab7eddb228f34a12', Taxi ID='449fa4909552757130d09d98ebc7770e2dd94579036b0e1958f92577ffc9fd6deca97bc1f229d2700865ce12b54d454ba96f973077386210ccbe44bfb7aac317', Trip Start Timestamp='01/01/2022 12:00:00 AM', Trip End Timestamp='01/01/2022 12:30:00 AM', Trip Seconds=2360, Trip Miles=17.44, Pickup Census Tract=None, Dropoff Census Tract=None, Pickup Community Area=None, Dropoff Community Area=8, Fare=47.75, Tips=0.0, Tolls=0.0, Extras=5.0, Trip Total=52.75, Payment Type='Cash', Company='Flash Cab', Pickup Centroid Latitude=None, Pickup Centroid Longitude=None, Pickup Centroid Location=None, Dropoff Centroid Latitude=41.899602111, Dropoff Centroid Longitude=-87.633308037, Dropoff Centroid  Location='POINT (-87.6333080367 41.899602111)')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c71fa7-173d-440a-9bf9-9b3b67b063e2",
   "metadata": {},
   "source": [
    "Интерфейс ДатаФрейма работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84592a4a-5677-4480-85cd-d3996a24db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_rdd = spark.sparkContext.textFile('/work/data/Taxi_Trips_-_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b946012-0e0c-4ba1-a07d-223f6f7cbc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trip ID,Taxi ID,Trip Start Timestamp,Trip End Timestamp,Trip Seconds,Trip Miles,Pickup Census Tract,Dropoff Census Tract,Pickup Community Area,Dropoff Community Area,Fare,Tips,Tolls,Extras,Trip Total,Payment Type,Company,Pickup Centroid Latitude,Pickup Centroid Longitude,Pickup Centroid Location,Dropoff Centroid Latitude,Dropoff Centroid Longitude,Dropoff Centroid  Location',\n",
       " 'bcfa19f2539021c054809d4c3993d226996ae095,368ce5511598af2cc07efdb68067d381174fdac1d47a2837d021e647f4c8febb3d9bf330e155845b8217b9b451c29b1b9ecf5d55d4bfe6cfd5d79ffb49b1f334,01/01/2022 12:00:00 AM,01/01/2022 12:00:00 AM,152,0.1,,,,,3.75,0.00,0.00,0.00,3.75,Cash,Medallion Leasin,,,,,,',\n",
       " '2aba69ff015f9ea8e7bff43cab7eddb228f34a12,449fa4909552757130d09d98ebc7770e2dd94579036b0e1958f92577ffc9fd6deca97bc1f229d2700865ce12b54d454ba96f973077386210ccbe44bfb7aac317,01/01/2022 12:00:00 AM,01/01/2022 12:30:00 AM,2360,17.44,,,,8,47.75,0.00,0.00,5.00,52.75,Cash,Flash Cab,,,,41.899602111,-87.633308037,POINT (-87.6333080367 41.899602111)',\n",
       " '54d812a0b88f8f9707825261014b3563a0a60ace,f98ae5e71fdda8806710af321dce58002146886c013f411be2cd336e0c224e78ed85626eeee462f0d1f5e6b29a030514b95f7b1b8ca59888a10b52dfe55ddf99,01/01/2022 12:00:00 AM,01/01/2022 12:00:00 AM,536,4.83,,,28,22,14.75,0.00,0.00,0.00,14.75,Cash,Globe Taxi,41.874005383,-87.66351755,POINT (-87.6635175498 41.874005383),41.92276062,-87.699155343,POINT (-87.6991553432 41.9227606205)',\n",
       " '7125b9e03a0f16c2dfb5eaf73ed057dc51eb68ef,8eca35a570101ad24c638f1f43eecce9d0cb7843e13a75f0af0c911c3e31ddec549c4808e216bcf31634542025c1e7de2442b92d5d7d73463c4e05fd959e47b4,01/01/2022 12:00:00 AM,01/01/2022 12:15:00 AM,897,2.07,,,8,32,9.75,0.00,0.00,1.50,11.25,Cash,Sun Taxi,41.899602111,-87.633308037,POINT (-87.6333080367 41.899602111),41.878865584,-87.625192142,POINT (-87.6251921424 41.8788655841)']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8355c6e-a917-4b83-8ea6-80abc84437d3",
   "metadata": {},
   "source": [
    "Интерфейс РДД также работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93f8e617-56f4-4a62-8e08-ddb0519ecca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip ID: string (nullable = true)\n",
      " |-- Taxi ID: string (nullable = true)\n",
      " |-- Trip Start Timestamp: string (nullable = true)\n",
      " |-- Trip End Timestamp: string (nullable = true)\n",
      " |-- Trip Seconds: integer (nullable = true)\n",
      " |-- Trip Miles: double (nullable = true)\n",
      " |-- Pickup Census Tract: long (nullable = true)\n",
      " |-- Dropoff Census Tract: long (nullable = true)\n",
      " |-- Pickup Community Area: integer (nullable = true)\n",
      " |-- Dropoff Community Area: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tips: double (nullable = true)\n",
      " |-- Tolls: double (nullable = true)\n",
      " |-- Extras: double (nullable = true)\n",
      " |-- Trip Total: double (nullable = true)\n",
      " |-- Payment Type: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Pickup Centroid Latitude: double (nullable = true)\n",
      " |-- Pickup Centroid Longitude: double (nullable = true)\n",
      " |-- Pickup Centroid Location: string (nullable = true)\n",
      " |-- Dropoff Centroid Latitude: double (nullable = true)\n",
      " |-- Dropoff Centroid Longitude: double (nullable = true)\n",
      " |-- Dropoff Centroid  Location: string (nullable = true)\n",
      "\n",
      "CPU times: user 1.57 ms, sys: 298 µs, total: 1.87 ms\n",
      "Wall time: 6.02 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Вывести схему данных\n",
    "taxi.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96fb51-d3cd-4c75-b528-04f8fa2eb318",
   "metadata": {},
   "source": [
    "### Класс Cols- псевдонимы имен полей ДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12869dc1-b838-448b-b34b-d205d1381ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trip_i': 'Trip ID',\n",
       " 'taxi_i': 'Taxi ID',\n",
       " 'trip_st': 'Trip Start Timestamp',\n",
       " 'trip_et': 'Trip End Timestamp',\n",
       " 'trip_s': 'Trip Seconds',\n",
       " 'trip_m': 'Trip Miles',\n",
       " 'pickup_ct': 'Pickup Census Tract',\n",
       " 'dropoff_ct': 'Dropoff Census Tract',\n",
       " 'pickup_ca': 'Pickup Community Area',\n",
       " 'dropoff_ca': 'Dropoff Community Area',\n",
       " 'fare': 'Fare',\n",
       " 'tips': 'Tips',\n",
       " 'tolls': 'Tolls',\n",
       " 'extras': 'Extras',\n",
       " 'trip_t': 'Trip Total',\n",
       " 'payment_t': 'Payment Type',\n",
       " 'company': 'Company',\n",
       " 'pickup_cl': 'Pickup Centroid Latitude',\n",
       " 'pickup_clon': 'Pickup Centroid Longitude',\n",
       " 'pickup_cloc': 'Pickup Centroid Location',\n",
       " 'dropoff_cl': 'Dropoff Centroid Latitude',\n",
       " 'dropoff_clon': 'Dropoff Centroid Longitude',\n",
       " 'dropoff_cloc': 'Dropoff Centroid  Location'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = Cols(taxi)\n",
    "\n",
    "ct.get_aliases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c316fa79-f5a3-405c-95c3-44f087f0bef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trip ID'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Вывод значения - полного имени поля для псевдонима\n",
    "ct.trip_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1dd6d6-42e7-41bb-8bfa-4f583edddf38",
   "metadata": {},
   "source": [
    "### Пандас- показать полное содержимое ячеек столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7a0b69d-d02a-4d95-ba0e-3846d63809f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Показать полное содержимое ячеек столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6890df7-216f-435d-9432-c450cbd661f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Type</th>\n",
       "      <th>Описание</th>\n",
       "      <th>Тип</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Column Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Trip ID</th>\n",
       "      <td>A unique identifier for the trip.</td>\n",
       "      <td>Plain Text</td>\n",
       "      <td>Уникальный идентификатор поездки</td>\n",
       "      <td>Обычный текст</td>\n",
       "      <td>Ошибка:510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taxi ID</th>\n",
       "      <td>A unique identifier for the taxi.</td>\n",
       "      <td>Plain Text</td>\n",
       "      <td>Уникальный идентификатор такси</td>\n",
       "      <td>Обычный текст</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Start Timestamp</th>\n",
       "      <td>When the trip started rounded to the nearest 15 minutes.</td>\n",
       "      <td>Date &amp; Time</td>\n",
       "      <td>Время начала поездки округленное до ближайших 15 минут</td>\n",
       "      <td>Дата и время</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip End Timestamp</th>\n",
       "      <td>When the trip ended rounded to the nearest 15 minutes.</td>\n",
       "      <td>Date &amp; Time</td>\n",
       "      <td>Время окончания поездки округленное до ближайших 15 минут</td>\n",
       "      <td>Дата и время</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Seconds</th>\n",
       "      <td>Time of the trip in seconds.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Продолжительность поездки в секундах</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Miles</th>\n",
       "      <td>Distance of the trip in miles.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Расстояние поездки в милях</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Census Tract</th>\n",
       "      <td>The Census Tract where the trip began. For privacy  this Census Tract is not shown for some trips. This column often will be blank for locations outside Chicago.</td>\n",
       "      <td>Plain Text</td>\n",
       "      <td>Код района по переписи населения  где началась поездка. В целях конфиденциальности этот код не отображается для некоторых поездок. Это поле часто остается пустым для мест за пределами Чикаго.</td>\n",
       "      <td>Обычный текст</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Census Tract</th>\n",
       "      <td>The Census Tract where the trip ended. For privacy  this Census Tract is not shown for some trips. This column often will be blank for locations outside Chicago.</td>\n",
       "      <td>Plain Text</td>\n",
       "      <td>Код района по переписи населения  где завершилась поездка. В целях конфиденциальности этот код не отображается для некоторых поездок. Это поле часто остается пустым для мест за пределами Чикаго.</td>\n",
       "      <td>Обычный текст</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Community Area</th>\n",
       "      <td>The Community Area where the trip began. This column will be blank for locations outside Chicago.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Номер коммунальной области  где началась поездка. Это поле остается пустым для мест за пределами Чикаго.</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Community Area</th>\n",
       "      <td>The Community Area where the trip ended. This column will be blank for locations outside Chicago.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Номер коммунальной области  где завершилась поездка. Это поле остается пустым для мест за пределами Чикаго.</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>The fare for the trip.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Стоимость поездки.</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tips</th>\n",
       "      <td>The tip for the trip. Cash tips generally will not be recorded.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Сумма чаевых за поездку. Наличные чаевые обычно не записываются.</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tolls</th>\n",
       "      <td>The tolls for the trip.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Сумма оплаты за проезд (платные дороги) за поездку.</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extras</th>\n",
       "      <td>Extra charges for the trip.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Дополнительные сборы за поездку.</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Total</th>\n",
       "      <td>Total cost of the trip the total of the previous columns.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Общая стоимость поездки сумма предыдущих столбцов.</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Payment Type</th>\n",
       "      <td>Type of payment for the trip.</td>\n",
       "      <td>Plain Text</td>\n",
       "      <td>Способ оплаты поездки.</td>\n",
       "      <td>Обычный текст</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <td>The taxi company.</td>\n",
       "      <td>Plain Text</td>\n",
       "      <td>Название такси-компании.</td>\n",
       "      <td>Обычный текст</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Centroid Latitude</th>\n",
       "      <td>The latitude of the center of the pickup census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Широта центра района по переписи или коммунальной области для местности, где началась поездка, если код района скрыт в целях конфиденциальности. Это поле часто остается пустым для мест за пределами Чикаго.</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Centroid Longitude</th>\n",
       "      <td>The longitude of the center of the pickup census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Долгота центра района по переписи или коммунальной области для местности, где началась поездка, если код района скрыт в целях конфиденциальности. Это поле часто остается пустым для мест за пределами Чикаго.</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Centroid Location</th>\n",
       "      <td>The location of the center of the pickup census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.</td>\n",
       "      <td>Point</td>\n",
       "      <td>Местоположение центра района по переписи или коммунальной области для местности, где началась поездка, если код района скрыт в целях конфиденциальности. Это поле часто остается пустым для мест за пределами Чикаго.</td>\n",
       "      <td>Точка (координаты)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Centroid Latitude</th>\n",
       "      <td>The latitude of the center of the dropoff census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Широта центра района по переписи или коммунальной области для местности, где завершилась поездка, если код района скрыт в целях конфиденциальности. Это поле часто остается пустым для мест за пределами Чикаго.</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Centroid Longitude</th>\n",
       "      <td>The longitude of the center of the dropoff census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.</td>\n",
       "      <td>Number</td>\n",
       "      <td>Долгота центра района по переписи или коммунальной области для местности, где завершилась поездка, если код района скрыт в целях конфиденциальности. Это поле часто остается пустым для мест за пределами Чикаго.</td>\n",
       "      <td>Число</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Centroid Location</th>\n",
       "      <td>The location of the center of the dropoff census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.</td>\n",
       "      <td>Point</td>\n",
       "      <td>Местоположение центра района по переписи или коммунальной области для местности, где завершилась поездка, если код района скрыт в целях конфиденциальности. Это поле часто остается пустым для мест за пределами Чикаго.</td>\n",
       "      <td>Точка (координаты)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                              Description  \\\n",
       "Column Name                                                                                                                                                                                                                 \n",
       "Trip ID                                                                                                                                                                                 A unique identifier for the trip.   \n",
       "Taxi ID                                                                                                                                                                                 A unique identifier for the taxi.   \n",
       "Trip Start Timestamp                                                                                                                                             When the trip started rounded to the nearest 15 minutes.   \n",
       "Trip End Timestamp                                                                                                                                                 When the trip ended rounded to the nearest 15 minutes.   \n",
       "Trip Seconds                                                                                                                                                                                 Time of the trip in seconds.   \n",
       "Trip Miles                                                                                                                                                                                 Distance of the trip in miles.   \n",
       "Pickup Census Tract                                     The Census Tract where the trip began. For privacy  this Census Tract is not shown for some trips. This column often will be blank for locations outside Chicago.   \n",
       "Dropoff Census Tract                                    The Census Tract where the trip ended. For privacy  this Census Tract is not shown for some trips. This column often will be blank for locations outside Chicago.   \n",
       "Pickup Community Area                                                                                                   The Community Area where the trip began. This column will be blank for locations outside Chicago.   \n",
       "Dropoff Community Area                                                                                                  The Community Area where the trip ended. This column will be blank for locations outside Chicago.   \n",
       "Fare                                                                                                                                                                                               The fare for the trip.   \n",
       "Tips                                                                                                                                                      The tip for the trip. Cash tips generally will not be recorded.   \n",
       "Tolls                                                                                                                                                                                             The tolls for the trip.   \n",
       "Extras                                                                                                                                                                                        Extra charges for the trip.   \n",
       "Trip Total                                                                                                                                                      Total cost of the trip the total of the previous columns.   \n",
       "Payment Type                                                                                                                                                                                Type of payment for the trip.   \n",
       "Company                                                                                                                                                                                                 The taxi company.   \n",
       "Pickup Centroid Latitude      The latitude of the center of the pickup census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.   \n",
       "Pickup Centroid Longitude    The longitude of the center of the pickup census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.   \n",
       "Pickup Centroid Location      The location of the center of the pickup census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.   \n",
       "Dropoff Centroid Latitude    The latitude of the center of the dropoff census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.   \n",
       "Dropoff Centroid Longitude  The longitude of the center of the dropoff census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.   \n",
       "Dropoff Centroid Location    The location of the center of the dropoff census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.   \n",
       "\n",
       "                                   Type  \\\n",
       "Column Name                               \n",
       "Trip ID                      Plain Text   \n",
       "Taxi ID                      Plain Text   \n",
       "Trip Start Timestamp        Date & Time   \n",
       "Trip End Timestamp          Date & Time   \n",
       "Trip Seconds                     Number   \n",
       "Trip Miles                       Number   \n",
       "Pickup Census Tract          Plain Text   \n",
       "Dropoff Census Tract         Plain Text   \n",
       "Pickup Community Area            Number   \n",
       "Dropoff Community Area           Number   \n",
       "Fare                             Number   \n",
       "Tips                             Number   \n",
       "Tolls                            Number   \n",
       "Extras                           Number   \n",
       "Trip Total                       Number   \n",
       "Payment Type                 Plain Text   \n",
       "Company                      Plain Text   \n",
       "Pickup Centroid Latitude         Number   \n",
       "Pickup Centroid Longitude        Number   \n",
       "Pickup Centroid Location          Point   \n",
       "Dropoff Centroid Latitude        Number   \n",
       "Dropoff Centroid Longitude       Number   \n",
       "Dropoff Centroid Location         Point   \n",
       "\n",
       "                                                                                                                                                                                                                                             Описание  \\\n",
       "Column Name                                                                                                                                                                                                                                             \n",
       "Trip ID                                                                                                                                                                                                              Уникальный идентификатор поездки   \n",
       "Taxi ID                                                                                                                                                                                                                Уникальный идентификатор такси   \n",
       "Trip Start Timestamp                                                                                                                                                                           Время начала поездки округленное до ближайших 15 минут   \n",
       "Trip End Timestamp                                                                                                                                                                          Время окончания поездки округленное до ближайших 15 минут   \n",
       "Trip Seconds                                                                                                                                                                                                     Продолжительность поездки в секундах   \n",
       "Trip Miles                                                                                                                                                                                                                 Расстояние поездки в милях   \n",
       "Pickup Census Tract                                   Код района по переписи населения  где началась поездка. В целях конфиденциальности этот код не отображается для некоторых поездок. Это поле часто остается пустым для мест за пределами Чикаго.   \n",
       "Dropoff Census Tract                               Код района по переписи населения  где завершилась поездка. В целях конфиденциальности этот код не отображается для некоторых поездок. Это поле часто остается пустым для мест за пределами Чикаго.   \n",
       "Pickup Community Area                                                                                                                        Номер коммунальной области  где началась поездка. Это поле остается пустым для мест за пределами Чикаго.   \n",
       "Dropoff Community Area                                                                                                                    Номер коммунальной области  где завершилась поездка. Это поле остается пустым для мест за пределами Чикаго.   \n",
       "Fare                                                                                                                                                                                                                               Стоимость поездки.   \n",
       "Tips                                                                                                                                                                                 Сумма чаевых за поездку. Наличные чаевые обычно не записываются.   \n",
       "Tolls                                                                                                                                                                                             Сумма оплаты за проезд (платные дороги) за поездку.   \n",
       "Extras                                                                                                                                                                                                               Дополнительные сборы за поездку.   \n",
       "Trip Total                                                                                                                                                                                         Общая стоимость поездки сумма предыдущих столбцов.   \n",
       "Payment Type                                                                                                                                                                                                                   Способ оплаты поездки.   \n",
       "Company                                                                                                                                                                                                                      Название такси-компании.   \n",
       "Pickup Centroid Latitude                Широта центра района по переписи или коммунальной области для местности, где началась поездка, если код района скрыт в целях конфиденциальности. Это поле часто остается пустым для мест за пределами Чикаго.   \n",
       "Pickup Centroid Longitude              Долгота центра района по переписи или коммунальной области для местности, где началась поездка, если код района скрыт в целях конфиденциальности. Это поле часто остается пустым для мест за пределами Чикаго.   \n",
       "Pickup Centroid Location        Местоположение центра района по переписи или коммунальной области для местности, где началась поездка, если код района скрыт в целях конфиденциальности. Это поле часто остается пустым для мест за пределами Чикаго.   \n",
       "Dropoff Centroid Latitude            Широта центра района по переписи или коммунальной области для местности, где завершилась поездка, если код района скрыт в целях конфиденциальности. Это поле часто остается пустым для мест за пределами Чикаго.   \n",
       "Dropoff Centroid Longitude          Долгота центра района по переписи или коммунальной области для местности, где завершилась поездка, если код района скрыт в целях конфиденциальности. Это поле часто остается пустым для мест за пределами Чикаго.   \n",
       "Dropoff Centroid Location    Местоположение центра района по переписи или коммунальной области для местности, где завершилась поездка, если код района скрыт в целях конфиденциальности. Это поле часто остается пустым для мест за пределами Чикаго.   \n",
       "\n",
       "                                            Тип  Unnamed: 5  \n",
       "Column Name                                                  \n",
       "Trip ID                           Обычный текст  Ошибка:510  \n",
       "Taxi ID                           Обычный текст         NaN  \n",
       "Trip Start Timestamp               Дата и время         NaN  \n",
       "Trip End Timestamp                 Дата и время         NaN  \n",
       "Trip Seconds                              Число         NaN  \n",
       "Trip Miles                                Число         NaN  \n",
       "Pickup Census Tract               Обычный текст         NaN  \n",
       "Dropoff Census Tract              Обычный текст         NaN  \n",
       "Pickup Community Area                     Число         NaN  \n",
       "Dropoff Community Area                    Число         NaN  \n",
       "Fare                                      Число         NaN  \n",
       "Tips                                      Число         NaN  \n",
       "Tolls                                     Число         NaN  \n",
       "Extras                                    Число         NaN  \n",
       "Trip Total                                Число         NaN  \n",
       "Payment Type                      Обычный текст         NaN  \n",
       "Company                           Обычный текст         NaN  \n",
       "Pickup Centroid Latitude                  Число         NaN  \n",
       "Pickup Centroid Longitude                 Число         NaN  \n",
       "Pickup Centroid Location     Точка (координаты)         NaN  \n",
       "Dropoff Centroid Latitude                 Число         NaN  \n",
       "Dropoff Centroid Longitude                Число         NaN  \n",
       "Dropoff Centroid Location    Точка (координаты)         NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_descr = pd.read_csv('/work/data/table_descr.csv',sep=';',index_col='Column Name')\n",
    "\n",
    "table_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8208da04-693c-4c4f-91b7-f382890b98a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth')  # Сброс максимальной ширины столбцов к значению по умолчанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d676881-4c34-4bf1-8098-3c853e125eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# describe_result = taxi.describe().toPandas()\n",
    "\n",
    "# time cost i5 (7g), 4 workers (x) 2gb RAM \n",
    "# CPU times: user 352 ms, sys: 32.2 ms, total: 384 ms\n",
    "# Wall time: 2min 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d7e2066-b329-4f11-ba9b-8eda6aede617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file_path = '/work/data/2022_describe.csv'\n",
    "# describe_result.to_csv(output_file_path, index=False) #transpose()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f652d08-997c-49d2-a7e3-4f09f9f9e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = pd.read_csv('/work/data/2022_describe.csv',sep=',',index_col='summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24e16ba1-2885-4c30-8bc8-af87fcbb04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "drf = dr.copy()\n",
    "pds.get_df_formated(drf, '`',2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13926650-cf6a-4c20-a99a-2b47c7ae6dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>summary</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Trip ID</th>\n",
       "      <td>6`382`425</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>000000bb18</td>\n",
       "      <td>ffffff1aae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taxi ID</th>\n",
       "      <td>6`382`425</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0041f8f0c9</td>\n",
       "      <td>fff84aa08a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Start Timestamp</th>\n",
       "      <td>6`382`425</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>01/01/2022</td>\n",
       "      <td>12/31/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip End Timestamp</th>\n",
       "      <td>6`382`213</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>01/01/2022</td>\n",
       "      <td>12/31/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Seconds</th>\n",
       "      <td>6`380`960</td>\n",
       "      <td>1`198.21</td>\n",
       "      <td>1`895.66</td>\n",
       "      <td>0</td>\n",
       "      <td>86`341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Miles</th>\n",
       "      <td>6`382`369</td>\n",
       "      <td>6.19</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2`967.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Census Tract</th>\n",
       "      <td>2`623`831</td>\n",
       "      <td>17`031`468`160.38</td>\n",
       "      <td>368`945.90</td>\n",
       "      <td>17`031`010`100</td>\n",
       "      <td>17`031`980`100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Census Tract</th>\n",
       "      <td>2`675`331</td>\n",
       "      <td>17`031`411`846.86</td>\n",
       "      <td>345`773.49</td>\n",
       "      <td>17`031`010`100</td>\n",
       "      <td>17`031`980`100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Community Area</th>\n",
       "      <td>5`868`572</td>\n",
       "      <td>32.35</td>\n",
       "      <td>25.20</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Community Area</th>\n",
       "      <td>5`748`741</td>\n",
       "      <td>25.84</td>\n",
       "      <td>20.93</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>6`378`889</td>\n",
       "      <td>21.73</td>\n",
       "      <td>49.42</td>\n",
       "      <td>0</td>\n",
       "      <td>9`999.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tips</th>\n",
       "      <td>6`378`889</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tolls</th>\n",
       "      <td>6`378`889</td>\n",
       "      <td>0.02</td>\n",
       "      <td>7.66</td>\n",
       "      <td>0</td>\n",
       "      <td>6`666.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extras</th>\n",
       "      <td>6`378`889</td>\n",
       "      <td>2.16</td>\n",
       "      <td>21.75</td>\n",
       "      <td>0</td>\n",
       "      <td>8`888.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Total</th>\n",
       "      <td>6`378`889</td>\n",
       "      <td>26.83</td>\n",
       "      <td>56.96</td>\n",
       "      <td>0</td>\n",
       "      <td>9`999.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Payment Type</th>\n",
       "      <td>6`382`425</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <td>6`382`425</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>24 Seven T</td>\n",
       "      <td>U Taxicab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Centroid Latitude</th>\n",
       "      <td>5`870`874</td>\n",
       "      <td>41.90</td>\n",
       "      <td>0.06</td>\n",
       "      <td>41.65</td>\n",
       "      <td>42.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Centroid Longitude</th>\n",
       "      <td>5`870`874</td>\n",
       "      <td>-87.69</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-87.91</td>\n",
       "      <td>-87.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Centroid Location</th>\n",
       "      <td>5`870`874</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>POINT (-87</td>\n",
       "      <td>POINT (-87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Centroid Latitude</th>\n",
       "      <td>5`784`494</td>\n",
       "      <td>41.89</td>\n",
       "      <td>0.06</td>\n",
       "      <td>41.66</td>\n",
       "      <td>42.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Centroid Longitude</th>\n",
       "      <td>5`784`494</td>\n",
       "      <td>-87.66</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-87.91</td>\n",
       "      <td>-87.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Centroid  Location</th>\n",
       "      <td>5`784`494</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>POINT (-87</td>\n",
       "      <td>POINT (-87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "summary                         count               mean      stddev  \\\n",
       "Trip ID                     6`382`425                nan         nan   \n",
       "Taxi ID                     6`382`425                nan         nan   \n",
       "Trip Start Timestamp        6`382`425                nan         nan   \n",
       "Trip End Timestamp          6`382`213                nan         nan   \n",
       "Trip Seconds                6`380`960           1`198.21    1`895.66   \n",
       "Trip Miles                  6`382`369               6.19        8.00   \n",
       "Pickup Census Tract         2`623`831  17`031`468`160.38  368`945.90   \n",
       "Dropoff Census Tract        2`675`331  17`031`411`846.86  345`773.49   \n",
       "Pickup Community Area       5`868`572              32.35       25.20   \n",
       "Dropoff Community Area      5`748`741              25.84       20.93   \n",
       "Fare                        6`378`889              21.73       49.42   \n",
       "Tips                        6`378`889               2.75        4.08   \n",
       "Tolls                       6`378`889               0.02        7.66   \n",
       "Extras                      6`378`889               2.16       21.75   \n",
       "Trip Total                  6`378`889              26.83       56.96   \n",
       "Payment Type                6`382`425                nan         nan   \n",
       "Company                     6`382`425                nan         nan   \n",
       "Pickup Centroid Latitude    5`870`874              41.90        0.06   \n",
       "Pickup Centroid Longitude   5`870`874             -87.69        0.10   \n",
       "Pickup Centroid Location    5`870`874                nan         nan   \n",
       "Dropoff Centroid Latitude   5`784`494              41.89        0.06   \n",
       "Dropoff Centroid Longitude  5`784`494             -87.66        0.07   \n",
       "Dropoff Centroid  Location  5`784`494                nan         nan   \n",
       "\n",
       "summary                                min             max  \n",
       "Trip ID                         000000bb18      ffffff1aae  \n",
       "Taxi ID                         0041f8f0c9      fff84aa08a  \n",
       "Trip Start Timestamp            01/01/2022      12/31/2022  \n",
       "Trip End Timestamp              01/01/2022      12/31/2022  \n",
       "Trip Seconds                             0          86`341  \n",
       "Trip Miles                               0        2`967.54  \n",
       "Pickup Census Tract         17`031`010`100  17`031`980`100  \n",
       "Dropoff Census Tract        17`031`010`100  17`031`980`100  \n",
       "Pickup Community Area                    1              77  \n",
       "Dropoff Community Area                   1              77  \n",
       "Fare                                     0        9`999.75  \n",
       "Tips                                     0             496  \n",
       "Tolls                                    0        6`666.66  \n",
       "Extras                                   0        8`888.88  \n",
       "Trip Total                               0        9`999.75  \n",
       "Payment Type                          Cash         Unknown  \n",
       "Company                         24 Seven T       U Taxicab  \n",
       "Pickup Centroid Latitude             41.65           42.02  \n",
       "Pickup Centroid Longitude           -87.91          -87.53  \n",
       "Pickup Centroid Location        POINT (-87      POINT (-87  \n",
       "Dropoff Centroid Latitude            41.66           42.02  \n",
       "Dropoff Centroid Longitude          -87.91          -87.53  \n",
       "Dropoff Centroid  Location      POINT (-87      POINT (-87  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drf.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "586c2d7a-22b8-479d-92c2-d61433599aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = int(dr.transpose()['count'].astype(float).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "015947cc-1da4-4347-9adc-e5e7bb982e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6'382'425\n"
     ]
    }
   ],
   "source": [
    "print(pds.gvf(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e91bc49-f481-4d19-8362-3c28893676b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 ms, sys: 2.31 ms, total: 14.8 ms\n",
      "Wall time: 30.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "row_count = taxi.count()\n",
    "\n",
    "# time cost i5 (7g), 4 workers (x) 2gb RAM\n",
    "# CPU times: user 5.85 ms, sys: 871 µs, total: 6.72 ms\n",
    "# Wall time: 11.8 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23e8d3c0-1123-44d3-a755-b92ab844a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6'382'425\n"
     ]
    }
   ],
   "source": [
    "print(pds.gvf(row_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349004b9-3c2f-4b82-ac33-93f152871e38",
   "metadata": {},
   "source": [
    "### Пандас - cпособы преобразования object столбца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24fcab8a-05a7-49a6-91b8-8690649f359c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== 6'382'425\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>summary</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Trip ID</th>\n",
       "      <td>6`382`425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taxi ID</th>\n",
       "      <td>6`382`425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Start Timestamp</th>\n",
       "      <td>6`382`425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip End Timestamp</th>\n",
       "      <td>6`382`213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Seconds</th>\n",
       "      <td>6`380`960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Miles</th>\n",
       "      <td>6`382`369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>6`378`889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tips</th>\n",
       "      <td>6`378`889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tolls</th>\n",
       "      <td>6`378`889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extras</th>\n",
       "      <td>6`378`889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Total</th>\n",
       "      <td>6`378`889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Payment Type</th>\n",
       "      <td>6`382`425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <td>6`382`425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "summary                   count\n",
       "Trip ID               6`382`425\n",
       "Taxi ID               6`382`425\n",
       "Trip Start Timestamp  6`382`425\n",
       "Trip End Timestamp    6`382`213\n",
       "Trip Seconds          6`380`960\n",
       "Trip Miles            6`382`369\n",
       "Fare                  6`378`889\n",
       "Tips                  6`378`889\n",
       "Tolls                 6`378`889\n",
       "Extras                6`378`889\n",
       "Trip Total            6`378`889\n",
       "Payment Type          6`382`425\n",
       "Company               6`382`425"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# способ преобразования object столбца и\n",
    "# фильтрации по стоблцу с использование to_numeric\n",
    "drt = dr.transpose()\n",
    "drtf = drt[pd.to_numeric(drt['count'], errors='coerce') > row_count*0.99][['count']]\n",
    "# отображение отформатированных данных\n",
    "pds.get_df_formated(drtf,'`',2, 10)\n",
    "print('==================',pds.gvf(row_count))\n",
    "drtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99ee6d02-b7fd-450e-9e50-46c0eb674427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# способ преобразования object столбца и\n",
    "# фильтрации по стоблцу с использование astype\n",
    "# \n",
    "# можно сразу применять тип и фильтровать, но значения в поле не поменяют тип\n",
    "drtf1 = dr.transpose()[dr.transpose()['count'].astype(int) > -1][['count']]\n",
    "# \n",
    "# если нужны вычисления с полем, то надо создать дф\n",
    "drt1 = dr.transpose()\n",
    "drt1['count'] = drt1['count'].astype(float)\n",
    "drt1['not_null']=drt1['count']/row_count\n",
    "# drt1[['count','not_null']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d2d5a35-ea4d-4653-9d0d-e92bddaa07a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>summary</th>\n",
       "      <th>count</th>\n",
       "      <th>not_null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Trip ID</th>\n",
       "      <td>6`382`425</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taxi ID</th>\n",
       "      <td>6`382`425</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Start Timestamp</th>\n",
       "      <td>6`382`425</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip End Timestamp</th>\n",
       "      <td>6`382`213</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Seconds</th>\n",
       "      <td>6`380`960</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Miles</th>\n",
       "      <td>6`382`369</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Census Tract</th>\n",
       "      <td>2`623`831</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Census Tract</th>\n",
       "      <td>2`675`331</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Community Area</th>\n",
       "      <td>5`868`572</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Community Area</th>\n",
       "      <td>5`748`741</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>6`378`889</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tips</th>\n",
       "      <td>6`378`889</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tolls</th>\n",
       "      <td>6`378`889</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extras</th>\n",
       "      <td>6`378`889</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Total</th>\n",
       "      <td>6`378`889</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Payment Type</th>\n",
       "      <td>6`382`425</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <td>6`382`425</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Centroid Latitude</th>\n",
       "      <td>5`870`874</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Centroid Longitude</th>\n",
       "      <td>5`870`874</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Centroid Location</th>\n",
       "      <td>5`870`874</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Centroid Latitude</th>\n",
       "      <td>5`784`494</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Centroid Longitude</th>\n",
       "      <td>5`784`494</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Centroid  Location</th>\n",
       "      <td>5`784`494</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "summary                         count not_null\n",
       "Trip ID                     6`382`425        1\n",
       "Taxi ID                     6`382`425        1\n",
       "Trip Start Timestamp        6`382`425        1\n",
       "Trip End Timestamp          6`382`213     1.00\n",
       "Trip Seconds                6`380`960     1.00\n",
       "Trip Miles                  6`382`369     1.00\n",
       "Pickup Census Tract         2`623`831     0.41\n",
       "Dropoff Census Tract        2`675`331     0.42\n",
       "Pickup Community Area       5`868`572     0.92\n",
       "Dropoff Community Area      5`748`741     0.90\n",
       "Fare                        6`378`889     1.00\n",
       "Tips                        6`378`889     1.00\n",
       "Tolls                       6`378`889     1.00\n",
       "Extras                      6`378`889     1.00\n",
       "Trip Total                  6`378`889     1.00\n",
       "Payment Type                6`382`425        1\n",
       "Company                     6`382`425        1\n",
       "Pickup Centroid Latitude    5`870`874     0.92\n",
       "Pickup Centroid Longitude   5`870`874     0.92\n",
       "Pickup Centroid Location    5`870`874     0.92\n",
       "Dropoff Centroid Latitude   5`784`494     0.91\n",
       "Dropoff Centroid Longitude  5`784`494     0.91\n",
       "Dropoff Centroid  Location  5`784`494     0.91"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "==total rows count== 6'382'425\n",
      "================================\n",
      "=== columns count====== 23\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "drtf1 = drt1[drt1['count'] > -1][['count','not_null']]\n",
    "# display(drtf1)\n",
    "# отображение отформатированных данных\n",
    "pds.get_df_formated(drtf1,'`',2, 10)\n",
    "display(drtf1)\n",
    "print('================================')\n",
    "print('==total rows count==',pds.gvf(row_count))\n",
    "print('================================')\n",
    "print('=== columns count======',int(len(drtf1)))\n",
    "print('================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f8cd1-03b1-408d-89dd-b4406c9e48b9",
   "metadata": {},
   "source": [
    "### take. Отбор только значений элементов строк (из объектов Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff573e50-9992-45c6-8c58-eb227173cf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 ms, sys: 2.97 ms, total: 19.4 ms\n",
      "Wall time: 391 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Trip ID='bcfa19f2539021c054809d4c3993d226996ae095', Taxi ID='368ce5511598af2cc07efdb68067d381174fdac1d47a2837d021e647f4c8febb3d9bf330e155845b8217b9b451c29b1b9ecf5d55d4bfe6cfd5d79ffb49b1f334', Trip Start Timestamp='01/01/2022 12:00:00 AM', Trip End Timestamp='01/01/2022 12:00:00 AM', Trip Seconds=152, Trip Miles=0.1, Pickup Census Tract=None, Dropoff Census Tract=None, Pickup Community Area=None, Dropoff Community Area=None, Fare=3.75, Tips=0.0, Tolls=0.0, Extras=0.0, Trip Total=3.75, Payment Type='Cash', Company='Medallion Leasin', Pickup Centroid Latitude=None, Pickup Centroid Longitude=None, Pickup Centroid Location=None, Dropoff Centroid Latitude=None, Dropoff Centroid Longitude=None, Dropoff Centroid  Location=None)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taxi.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7f96315-2ddc-4673-b5b0-dd7176c99b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Trip ID='bcfa19f2539021c054809d4c3993d226996ae095', Taxi ID='368ce5511598af2cc07efdb68067d381174fdac1d47a2837d021e647f4c8febb3d9bf330e155845b8217b9b451c29b1b9ecf5d55d4bfe6cfd5d79ffb49b1f334', Trip Start Timestamp='01/01/2022 12:00:00 AM', Trip End Timestamp='01/01/2022 12:00:00 AM', Trip Seconds=152, Trip Miles=0.1, Pickup Census Tract=None, Dropoff Census Tract=None, Pickup Community Area=None, Dropoff Community Area=None, Fare=3.75, Tips=0.0, Tolls=0.0, Extras=0.0, Trip Total=3.75, Payment Type='Cash', Company='Medallion Leasin', Pickup Centroid Latitude=None, Pickup Centroid Longitude=None, Pickup Centroid Location=None, Dropoff Centroid Latitude=None, Dropoff Centroid Longitude=None, Dropoff Centroid  Location=None),\n",
       " Row(Trip ID='2aba69ff015f9ea8e7bff43cab7eddb228f34a12', Taxi ID='449fa4909552757130d09d98ebc7770e2dd94579036b0e1958f92577ffc9fd6deca97bc1f229d2700865ce12b54d454ba96f973077386210ccbe44bfb7aac317', Trip Start Timestamp='01/01/2022 12:00:00 AM', Trip End Timestamp='01/01/2022 12:30:00 AM', Trip Seconds=2360, Trip Miles=17.44, Pickup Census Tract=None, Dropoff Census Tract=None, Pickup Community Area=None, Dropoff Community Area=8, Fare=47.75, Tips=0.0, Tolls=0.0, Extras=5.0, Trip Total=52.75, Payment Type='Cash', Company='Flash Cab', Pickup Centroid Latitude=None, Pickup Centroid Longitude=None, Pickup Centroid Location=None, Dropoff Centroid Latitude=41.899602111, Dropoff Centroid Longitude=-87.633308037, Dropoff Centroid  Location='POINT (-87.6333080367 41.899602111)')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.take(3)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fac20bc0-6cfa-4baa-b7ed-8dd2487f575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.91 ms, sys: 6.43 ms, total: 16.3 ms\n",
      "Wall time: 422 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bcfa19f2539021c054809d4c3993d226996ae095',\n",
       " '2aba69ff015f9ea8e7bff43cab7eddb228f34a12']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Отбор значений для одного элемента объекта Row (одно поле)\n",
    "[row[ct.trip_i] for row in taxi.take(3)[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11538bf7-892e-4789-a4da-606eadd70df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Trip ID': 'bcfa19f2539021c054809d4c3993d226996ae095',\n",
       "  'Taxi ID': '368ce5511598af2cc07efdb68067d381174fdac1d47a2837d021e647f4c8febb3d9bf330e155845b8217b9b451c29b1b9ecf5d55d4bfe6cfd5d79ffb49b1f334'},\n",
       " {'Trip ID': '2aba69ff015f9ea8e7bff43cab7eddb228f34a12',\n",
       "  'Taxi ID': '449fa4909552757130d09d98ebc7770e2dd94579036b0e1958f92577ffc9fd6deca97bc1f229d2700865ce12b54d454ba96f973077386210ccbe44bfb7aac317'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# список словарей\n",
    "[{field: row[field] for field in [ct.trip_i, ct.taxi_i]} for row in taxi.take(3)[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6b9f342-351b-4830-baa2-b86331180b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bcfa19f2539021c054809d4c3993d226996ae095',\n",
       "  '2aba69ff015f9ea8e7bff43cab7eddb228f34a12'],\n",
       " ['368ce5511598af2cc07efdb68067d381174fdac1d47a2837d021e647f4c8febb3d9bf330e155845b8217b9b451c29b1b9ecf5d55d4bfe6cfd5d79ffb49b1f334',\n",
       "  '449fa4909552757130d09d98ebc7770e2dd94579036b0e1958f92577ffc9fd6deca97bc1f229d2700865ce12b54d454ba96f973077386210ccbe44bfb7aac317']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# список списков для нескольких полей, несколько проходов\n",
    "[[row[ct.trip_i] for row in taxi.take(3)[:2]], [row[ct.taxi_i] for row in taxi.take(3)[:2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e056516-2e6d-4ccd-b6e8-3ed8242712ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bcfa19f2539021c054809d4c3993d226996ae095',\n",
       "  '2aba69ff015f9ea8e7bff43cab7eddb228f34a12'),\n",
       " ('368ce5511598af2cc07efdb68067d381174fdac1d47a2837d021e647f4c8febb3d9bf330e155845b8217b9b451c29b1b9ecf5d55d4bfe6cfd5d79ffb49b1f334',\n",
       "  '449fa4909552757130d09d98ebc7770e2dd94579036b0e1958f92577ffc9fd6deca97bc1f229d2700865ce12b54d454ba96f973077386210ccbe44bfb7aac317')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# список кортежей, один проход\n",
    "list(zip(*[[row[ct.trip_i], row[ct.taxi_i]] for row in taxi.take(3)[:2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45706cf6-ac35-4b78-b730-3ca92aa31de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bcfa19f2539021c054809d4c3993d226996ae095',\n",
       "  '2aba69ff015f9ea8e7bff43cab7eddb228f34a12'],\n",
       " ['368ce5511598af2cc07efdb68067d381174fdac1d47a2837d021e647f4c8febb3d9bf330e155845b8217b9b451c29b1b9ecf5d55d4bfe6cfd5d79ffb49b1f334',\n",
       "  '449fa4909552757130d09d98ebc7770e2dd94579036b0e1958f92577ffc9fd6deca97bc1f229d2700865ce12b54d454ba96f973077386210ccbe44bfb7aac317']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# список списков - один проход\n",
    "[list(t) for t in zip(*[[row[ct.trip_i], row[ct.taxi_i]] for row in taxi.take(3)[:2]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43fabc44-f53b-42bc-a15b-5a64b22ccf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bcfa19f2539021c054809d4c3993d226996ae095',\n",
       "  '2aba69ff015f9ea8e7bff43cab7eddb228f34a12'],\n",
       " ['368ce5511598af2cc07efdb68067d381174fdac1d47a2837d021e647f4c8febb3d9bf330e155845b8217b9b451c29b1b9ecf5d55d4bfe6cfd5d79ffb49b1f334',\n",
       "  '449fa4909552757130d09d98ebc7770e2dd94579036b0e1958f92577ffc9fd6deca97bc1f229d2700865ce12b54d454ba96f973077386210ccbe44bfb7aac317']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# список списков - произвольный список полей один проход.\n",
    "fields = [ct.trip_i, ct.taxi_i]\n",
    "\n",
    "[list(t) for t in zip(*[[row[field] for field in fields] for row in taxi.take(3)[:2]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a91b3-d5ff-4bca-a271-e7a9ba8547d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Использование zip(*...) и генератора списка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797712a-6ec0-458e-bf4c-b13cace416a5",
   "metadata": {},
   "source": [
    "***Использование `zip(*...)` и генератора списка*** для преобразования кортежей в списки имеет минимальные накладные расходы. Эти операции довольно быстры, так как они выполняются в памяти и не требуют значительных вычислительных ресурсов.\n",
    "\n",
    "Вот краткий обзор того, как работают эти преобразования:\n",
    "\n",
    "**zip(*...)**: Это операция, которая транспонирует список списков. Она создает итератор, который группирует элементы из каждого из входных списков. Это делается за один проход по данным, что делает его очень эффективным.\n",
    "Генератор списка: Преобразование каждого кортежа в список с помощью генератора списка `[list(t) for t in ...]` также выполняется за один проход по данным и выполняет простое преобразование типов. Это эффективно и не требует значительных вычислительных затрат.  \n",
    "Для небольших наборов данных (несколько десятков или сотен элементов) накладные расходы будут практически незаметными. Для больших наборов данных, таких как миллионы элементов, накладные расходы могут стать более заметными, но по-прежнему будут минимальными по сравнению с другими более затратными операциями (например, ввод-вывод на диск или сложные вычисления).\n",
    "\n",
    "Чтобы продемонстрировать минимальные накладные расходы, давайте рассмотрим пример с измерением времени:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fefa124-a952-49fc-84ff-2564f6b584e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время выполнения:  5.546215295791626\n"
     ]
    }
   ],
   "source": [
    "# Пример данных\n",
    "rows = [{ct.trip_i: f'trip_id_{i}', ct.taxi_i: f'taxi_id_{i}'} for i in range(1000000)]\n",
    "\n",
    "# Измеряем время выполнения преобразования\n",
    "start_time = time.time()\n",
    "[list(t) for t in zip(*[[row[ct.trip_i], row[ct.taxi_i]] for row in rows])]\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Время выполнения: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea2478-becf-4abf-96dc-000c5377997f",
   "metadata": {},
   "source": [
    "В большинстве случаев такие преобразования в Python очень эффективны, и вам не стоит беспокоиться о них, если нет необходимости оптимизировать производительность до предела."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2715c-fc56-4ca8-b24d-6ec12a6c1e0f",
   "metadata": {},
   "source": [
    "### select. Отбор значений строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "997dae37-436e-40c2-91af-f9b5fc5e22e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Trip ID='bcfa19f2539021c054809d4c3993d226996ae095'),\n",
       " Row(Trip ID='2aba69ff015f9ea8e7bff43cab7eddb228f34a12'),\n",
       " Row(Trip ID='54d812a0b88f8f9707825261014b3563a0a60ace'),\n",
       " Row(Trip ID='7125b9e03a0f16c2dfb5eaf73ed057dc51eb68ef')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.select(ct.trip_i).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8cf065d4-c742-446e-822a-704b9b9c93f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Trip ID='2aba69ff015f9ea8e7bff43cab7eddb228f34a12'),\n",
       " Row(Trip ID='54d812a0b88f8f9707825261014b3563a0a60ace')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.select(ct.trip_i).take(4)[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86aca918-3439-4870-9fe6-f5cc2082cc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2aba69ff015f9ea8e7bff43cab7eddb228f34a12',\n",
       "  '54d812a0b88f8f9707825261014b3563a0a60ace'],\n",
       " ['449fa4909552757130d09d98ebc7770e2dd94579036b0e1958f92577ffc9fd6deca97bc1f229d2700865ce12b54d454ba96f973077386210ccbe44bfb7aac317',\n",
       "  'f98ae5e71fdda8806710af321dce58002146886c013f411be2cd336e0c224e78ed85626eeee462f0d1f5e6b29a030514b95f7b1b8ca59888a10b52dfe55ddf99']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# список списков - произвольный список полей один проход.\n",
    "fields = [ct.trip_i, ct.taxi_i]\n",
    "\n",
    "[list(t) for t in zip(*[[row[field] for field in fields] \\\n",
    "                        for row in taxi.select(ct.trip_i,ct.taxi_i).take(4)[1:3]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168cf2a9-6fa3-42f0-b31c-c6e2179ace43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Отбор строк через фильтрацию по номерам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2413aa-353f-4c51-8dfb-6336ae3183e5",
   "metadata": {},
   "source": [
    "#### monotonically_increasing_id для ДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "053069a6-5a99-4b38-886c-031d307674dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e969cb8-5063-42c2-97f2-021c6284e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33 ms, sys: 10.7 ms, total: 43.7 ms\n",
      "Wall time: 39.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Trip ID='2aba69ff015f9ea8e7bff43cab7eddb228f34a12'),\n",
       " Row(Trip ID='54d812a0b88f8f9707825261014b3563a0a60ace'),\n",
       " Row(Trip ID='7125b9e03a0f16c2dfb5eaf73ed057dc51eb68ef'),\n",
       " Row(Trip ID='f1a650ee419b4e52d766432e6f86eca3457bfb37'),\n",
       " Row(Trip ID='040caea96573c5743668b138011bc157c7825e86')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taxi.select(ct.trip_i).filter((monotonically_increasing_id() >= 1) \\\n",
    "                              & (monotonically_increasing_id() <= 4)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82909765-7709-407e-adfa-0d7be1d40307",
   "metadata": {},
   "source": [
    "#### zipWithIndex().filter для РДД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e5f383b-8cc5-4f13-9ca9-1fd706a34026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 134 ms, sys: 24.5 ms, total: 158 ms\n",
      "Wall time: 1min 56s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Trip ID='2aba69ff015f9ea8e7bff43cab7eddb228f34a12'),\n",
       " Row(Trip ID='54d812a0b88f8f9707825261014b3563a0a60ace'),\n",
       " Row(Trip ID='7125b9e03a0f16c2dfb5eaf73ed057dc51eb68ef'),\n",
       " Row(Trip ID='f1a650ee419b4e52d766432e6f86eca3457bfb37')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taxi.select(ct.trip_i).rdd.zipWithIndex().filter(lambda x: 1 <= x[1] <= 4).map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "409523ea-5513-4def-91da-53fd0bb73595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 ms, sys: 1.1 ms, total: 16.9 ms\n",
      "Wall time: 1.36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Trip ID='bcfa19f2539021c054809d4c3993d226996ae095'),\n",
       " Row(Trip ID='2aba69ff015f9ea8e7bff43cab7eddb228f34a12'),\n",
       " Row(Trip ID='54d812a0b88f8f9707825261014b3563a0a60ace'),\n",
       " Row(Trip ID='7125b9e03a0f16c2dfb5eaf73ed057dc51eb68ef')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taxi.select(ct.trip_i).rdd.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244952d5-e6fc-46fb-894f-724f7393524d",
   "metadata": {},
   "source": [
    "#### Причины неэффективности RDD Approach\n",
    "\n",
    "**Монолитический подход DataFrame API:**  \n",
    "`DataFrame API` в Apache Spark значительно оптимизирован за счет планировщика `Catalyst` и исполняющего механизма `Tungsten`. Эти оптимизации отсутствуют в `RDD API`, что делает операции на RDD в целом менее эффективными.  \n",
    "`DataFrame API` оптимизирован с использованием `Catalyst optimizer`. Он понимает логику операций и может оптимизировать выполнение, используя проекцию, фильтрацию, и другие оптимизации.\n",
    "В первом подходе, используя `monotonically_increasing_id, Spark` может оптимизировать фильтрацию на основе индексов и выполнять ее более эффективно.  \n",
    "**Отсутствие оптимизации в RDD API:**  \n",
    "`RDD API` не имеет уровня оптимизации, как `DataFrame API`. При использовании RDD, вы обязаны выполнять множество шагов вручную, что делает код менее эффективным.  \n",
    "В данном случае, `zipWithIndex` создает пару `(элемент, индекс)`, которая потом фильтруется и мапится обратно. Это добавляет значительные накладные расходы на распределенные вычисления и коммуникацию между узлами кластера.  \n",
    "**Сериализация и Десериализация:**  \n",
    "Операции с RDD включают сериализацию и десериализацию данных между узлами, что может увеличить время выполнения, особенно на больших наборах данных.\n",
    "\n",
    "**Как улучшить производительность RDD**  \n",
    "Хотя `DataFrame API` чаще всего предпочтительнее для большинства операций из-за своих оптимизаций, бывают случаи, когда RDD необходимы. Вот несколько советов, чтобы улучшить производительность RDD:  \n",
    "Используйте `Partitions Wisely`:  \n",
    "Убедитесь, что количество партиций соответствует размерам данных и ресурсам кластера.  \n",
    "`Persistence`:  \n",
    "Используйте методы persist() или cache() для данных, которые будут многократно использоваться.  \n",
    "`Avoid Shuffling`:  \n",
    "Минимизируйте количество операций, требующих перемещения данных между узлами (например, groupByKey, reduceByKey).\n",
    "\n",
    "**Добавление индексов:**  \n",
    "`zipWithIndex()` добавляет индексы **к каждому элементу** в RDD. Эта операция может потребовать **полной загрузки** данных и выполнения дополнительного шага для распределения индексов.\n",
    "**Фильтрация:**  \n",
    "`filter(lambda x: 1 <= x[1] <= 4)` фильтрует элементы на основе их индексов. Эта операция применима **к каждому элементу** после добавления индексов.\n",
    "\n",
    "Без `zipWithIndex()` не обойтись, если нужно отбирать элементы по их порядковому номеру, так как RDD по своей природе неупорядочен. Однако, если использовать take и затем отбирать нужные элементы, можно обойтись меньшими затратами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25030898-80f3-4629-a5b7-e96cc223d862",
   "metadata": {},
   "source": [
    "#### rdd.take(4)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d20bd044-6d1a-449f-9cf1-9a59a8c54059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.5 ms, sys: 253 µs, total: 31.7 ms\n",
      "Wall time: 741 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Trip ID='2aba69ff015f9ea8e7bff43cab7eddb228f34a12'),\n",
       " Row(Trip ID='54d812a0b88f8f9707825261014b3563a0a60ace'),\n",
       " Row(Trip ID='7125b9e03a0f16c2dfb5eaf73ed057dc51eb68ef')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taxi.select(ct.trip_i).rdd.take(4)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b830afc-8c13-4d0a-9d00-d3848bc66551",
   "metadata": {},
   "source": [
    "#### limit(5).collect()[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7250a83-d5ba-4783-94ce-d21bf2abea02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Trip ID='2aba69ff015f9ea8e7bff43cab7eddb228f34a12'),\n",
       " Row(Trip ID='54d812a0b88f8f9707825261014b3563a0a60ace'),\n",
       " Row(Trip ID='7125b9e03a0f16c2dfb5eaf73ed057dc51eb68ef'),\n",
       " Row(Trip ID='f1a650ee419b4e52d766432e6f86eca3457bfb37')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.select(ct.trip_i).limit(5).collect()[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132502eb-b694-4656-9c1d-21dc1a90b7a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DF API и SQL - cравнение скорости выполнения запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b44501f4-1fdb-45f8-bd07-ca63cc45291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.1 ms, sys: 19.7 ms, total: 47.8 ms\n",
      "Wall time: 43.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pck = taxi.select(ct.pickup_ct).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c05126a7-cd10-4332-9458-c859aaf8831b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.5 ms, sys: 2.79 ms, total: 47.3 ms\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "taxi.createOrReplaceTempView(\"taxis\")\n",
    "query = \"SELECT DISTINCT `Pickup Census Tract` FROM taxis\"\n",
    "result = spark.sql(query)\n",
    "pck2 = result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "abddf343-64ce-4a39-9f8c-ab767edaf918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:11<00:00, 38.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее время выполнения для первого блока кода: 38.27 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Количество испытаний\n",
    "num_trials = 5\n",
    "\n",
    "# Время выполнения для первого блока кода: сбор уникальных значений с помощью DataFrame API\n",
    "pck_times = []\n",
    "for _ in tqdm(range(num_trials)):\n",
    "    start_time = time.time()\n",
    "    pck = taxi.select('Pickup Census Tract').distinct().collect()\n",
    "    end_time = time.time()\n",
    "    pck_times.append(end_time - start_time)\n",
    "\n",
    "# Вывод среднего времени выполнения для первого блока кода\n",
    "print(\"Среднее время выполнения для первого блока кода: {:.2f} секунд\".format(sum(pck_times) / num_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3f624b3-8fd2-48b7-aa43-d402d3f885c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:12<00:00, 38.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее время выполнения для второго блока кода: 38.51 секунд\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Время выполнения для второго блока кода: сбор уникальных значений с помощью SQL запроса\n",
    "result_times = []\n",
    "for _ in tqdm(range(num_trials)):\n",
    "    start_time = time.time()\n",
    "    taxi.createOrReplaceTempView(\"taxis\")\n",
    "    result = spark.sql(\"SELECT DISTINCT `Pickup Census Tract` FROM taxis\")\n",
    "    pck2 = result.collect()\n",
    "    end_time = time.time()\n",
    "    result_times.append(end_time - start_time)\n",
    "\n",
    "# Вывод среднего времени выполнения для второго блока кода\n",
    "print(\"Среднее время выполнения для второго блока кода: {:.2f} секунд\".format(sum(result_times) / num_trials))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd44313b-1887-4fa6-b5dc-bbca4b85621e",
   "metadata": {},
   "source": [
    "**DF API по скорости сравнимо с SQL**. При этом SQL выполняется немного быстрее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e799873-dee5-4560-87a1-6b983c277386",
   "metadata": {},
   "source": [
    "### filter. Несколько условий."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8678ef-b93d-42ad-8b2d-b5c561e11790",
   "metadata": {},
   "source": [
    "#### Разная сортировка: orderBy(taxi[f2].asc(), taxi[f3].desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63549c1e-2d15-43f6-8ec4-7d08020b1575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 79.6 ms, sys: 12.1 ms, total: 91.7 ms\n",
      "Wall time: 47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f2 = 'Pickup Centroid Latitude'\n",
    "f3 = 'Pickup Centroid Longitude'\n",
    "geos_pck_not_null = taxi.select(f2, f3).filter(taxi[f2].isNotNull() &\n",
    "                taxi[f3].isNotNull()).distinct().orderBy(taxi[f2].asc(), taxi[f3].desc()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9455028a-f3ab-459c-aacb-4cb140e0c2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Pickup Centroid Latitude=41.651921576, Pickup Centroid Longitude=-87.564929171),\n",
       " Row(Pickup Centroid Latitude=41.660136051, Pickup Centroid Longitude=-87.60284764),\n",
       " Row(Pickup Centroid Latitude=41.663670652, Pickup Centroid Longitude=-87.540935513),\n",
       " Row(Pickup Centroid Latitude=41.663759731, Pickup Centroid Longitude=-87.637304085),\n",
       " Row(Pickup Centroid Latitude=41.665167734, Pickup Centroid Longitude=-87.535284961)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geos_pck_not_null[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a781bb1-f92b-400b-b09e-4015b23367d9",
   "metadata": {},
   "source": [
    "#### Несколько условий. isNull(). isNotNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bae0e17e-a42b-4b93-bdf0-77fc4a64419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.37 s, sys: 2.09 s, total: 10.5 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "geos_pck_not_null = taxi.select(f2, f3).filter(taxi[f2].isNull() &\n",
    "                taxi[f3].isNull()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6397549a-0d33-4f8d-b108-604921680901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 280 ms, sys: 8.11 ms, total: 288 ms\n",
      "Wall time: 421 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "geos_pck_not_null = taxi.select(f2, f3).filter(taxi[f2].isNull() & taxi[f3].isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e36bf6c1-c4cc-4f80-901f-73b6ea4fa03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511'551\n",
      "CPU times: user 14.6 ms, sys: 890 µs, total: 15.5 ms\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pds.gvf(geos_pck_not_null.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb8eaae9-bc54-40f8-8ec7-2bf6c30b1d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 ms, sys: 2.17 ms, total: 27.1 ms\n",
      "Wall time: 34.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taxi.select(f2, f3).filter(taxi[f2].isNull() &\n",
    "                taxi[f3].isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "758468b7-019f-4e08-959e-1cfb48fa4785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 ms, sys: 13.2 ms, total: 27.7 ms\n",
      "Wall time: 35.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taxi.select(f2, f3).filter(taxi[f2].isNotNull() &\n",
    "                taxi[f3].isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490069f-0973-48a7-aeea-4f7f9af966f5",
   "metadata": {},
   "source": [
    "#### taxi[ct.trip_m]>10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0108e596-867e-404e-94b9-3b5cc0903bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1777033"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.filter(taxi[ct.trip_m]>10).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "96364e7f-2cc2-49e6-ab7b-8f6f930d7660",
   "metadata": {},
   "outputs": [],
   "source": [
    "long = taxi.filter((taxi[ct.trip_m]>9.9) & (taxi[ct.trip_m]<10.1) \n",
    "                   & taxi[ct.pickup_cl].isNotNull() & taxi[ct.dropoff_cl].isNotNull() \n",
    "                   & taxi[ct.pickup_clon].isNotNull() & taxi[ct.dropoff_clon].isNotNull()\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c059e801-b167-416f-bbc9-8259813832e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20855"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f04e1c0-afbf-4363-a2a3-61a8178d731a",
   "metadata": {},
   "source": [
    "## withColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59edf16-9f7b-4314-b69d-323ee42c1eab",
   "metadata": {},
   "source": [
    "### вычисления на столбцах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f934cc9-fd48-40ed-9446-e610a710501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "long = long.withColumn('Distance',(\n",
    "                                ((taxi[ct.pickup_cl]-taxi[ct.dropoff_cl])**2 +\n",
    "                                (taxi[ct.pickup_clon]-taxi[ct.dropoff_clon])**2))**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "11398ee0-0610-4f9b-b108-39183a8ff066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 ms, sys: 20.7 ms, total: 31.8 ms\n",
      "Wall time: 392 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Pickup Centroid Latitude=41.901206994, Dropoff Centroid Latitude=41.901206994, Pickup Centroid Longitude=-87.676355989, Dropoff Centroid Longitude=-87.676355989, Distance=0.0),\n",
       " Row(Pickup Centroid Latitude=41.874005383, Dropoff Centroid Latitude=41.9867118, Pickup Centroid Longitude=-87.66351755, Dropoff Centroid Longitude=-87.663416405, Distance=0.11270646238477244),\n",
       " Row(Pickup Centroid Latitude=41.899602111, Dropoff Centroid Latitude=41.839086906, Pickup Centroid Longitude=-87.633308037, Dropoff Centroid Longitude=-87.714003807, Distance=0.10086573913913337),\n",
       " Row(Pickup Centroid Latitude=41.857183858, Dropoff Centroid Latitude=41.9867118, Pickup Centroid Longitude=-87.620334624, Dropoff Centroid Longitude=-87.663416405, Distance=0.1365046798204703),\n",
       " Row(Pickup Centroid Latitude=41.874005383, Dropoff Centroid Latitude=41.857183858, Pickup Centroid Longitude=-87.66351755, Dropoff Centroid Longitude=-87.620334624, Distance=0.046343595040167315),\n",
       " Row(Pickup Centroid Latitude=41.850266366, Dropoff Centroid Latitude=41.954027649, Pickup Centroid Longitude=-87.667569312, Dropoff Centroid Longitude=-87.763399032, Distance=0.1412435452864429),\n",
       " Row(Pickup Centroid Latitude=41.740205756, Dropoff Centroid Latitude=41.874005383, Pickup Centroid Longitude=-87.615969523, Dropoff Centroid Longitude=-87.66351755, Distance=0.14199702481717558),\n",
       " Row(Pickup Centroid Latitude=41.79259236, Dropoff Centroid Latitude=41.874005383, Pickup Centroid Longitude=-87.769615453, Dropoff Centroid Longitude=-87.66351755, Distance=0.1337342339679578),\n",
       " Row(Pickup Centroid Latitude=41.9867118, Dropoff Centroid Latitude=41.857183858, Pickup Centroid Longitude=-87.663416405, Dropoff Centroid Longitude=-87.620334624, Distance=0.1365046798204703),\n",
       " Row(Pickup Centroid Latitude=41.92276062, Dropoff Centroid Latitude=42.001571027, Pickup Centroid Longitude=-87.699155343, Dropoff Centroid Longitude=-87.695012589, Distance=0.07891921605166885)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "long.select(ct.pickup_cl,ct.dropoff_cl,ct.pickup_clon,ct.dropoff_clon,'Distance').take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac452b-de02-4ef1-8b91-9d0ca2b50bf0",
   "metadata": {},
   "source": [
    "### метод expr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61079bcb-cc2f-4dd5-b42c-a522c491d09b",
   "metadata": {},
   "source": [
    "в PySpark можно использовать метод `expr()` внутри `withColumn()`. Метод `expr()` позволяет вам писать SQL-подобные выражения для создания нового столбца. Это особенно полезно, когда выражение сложное и его легче записать в виде строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c0978-4801-4ea3-a784-6b8451ae5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример DataFrame\n",
    "data = [(\"Alice\", 10), (\"Bob\", 20), (\"Charlie\", 30)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# Добавление нового столбца с использованием expr\n",
    "df_with_new_column = df.withColumn(\"Age_Category\", expr(\n",
    "    \"CASE WHEN Age < 20 THEN 'Teenager' \" +\n",
    "    \"WHEN Age >= 20 AND Age < 30 THEN 'Young Adult' \" +\n",
    "    \"ELSE 'Adult' END\"\n",
    "))\n",
    ".withColumn(\"Age_In_Decades\", expr(\"round(float(Age/10), 2)\"))\n",
    "\n",
    "# Отображение результата\n",
    "df_with_new_column.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb04620-bf60-430f-903d-6e22189ac10b",
   "metadata": {},
   "source": [
    "### Метод .getItem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856f82f-d895-48c8-95c8-e78ce1302dbd",
   "metadata": {},
   "source": [
    "Метод `.getItem()` в PySpark используется для доступа к элементам в столбцах типа `ArrayType`, `MapType`, или `StructType`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87422afc-2ae2-4a0a-8fd8-b4419ec0c149",
   "metadata": {},
   "source": [
    "#### Доступ к элементу в массиве (списке)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660b4c9-8c08-4057-b89a-c0cc97bf05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример DataFrame с массивами\n",
    "data = [(\"Alice\", [1, 2, 3]), (\"Bob\", [4, 5, 6])]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Numbers\"])\n",
    "\n",
    "# Доступ к первому элементу массива\n",
    "df_with_first_number = df.withColumn(\"First_Number\", col(\"Numbers\").getItem(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d75c437-2dc5-446e-9afe-1da2833848ca",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+-----+---------+------------+\r\n",
    "|Nam e|  Numbers|First_Number|\r\n",
    "+-----+---------+------------+\r\n",
    "|Alice|[1, 2, 3]|           1|\r\n",
    "|  Bob|[4, 5, 6]|           4|\r\n",
    "-+----+---------+------------+\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526dd64-91d5-4ccc-807b-c09c6f0b5514",
   "metadata": {},
   "source": [
    "#### Доступ к элементу в словаре (хэшмэп)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf117a-8964-4535-b436-50830ffdc473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример DataFrame с картами\n",
    "data = [(\"Alice\", {\"math\": 85, \"science\": 90}), (\"Bob\", {\"math\": 78, \"science\": 88})]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Scores\"])\n",
    "\n",
    "# Доступ к значению по ключу\n",
    "df_with_math_score = df.withColumn(\"Math_Score\", col(\"Scores\").getItem(\"math\"))\n",
    "\n",
    "df_with_math_score.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0851b-a5ac-47d3-af67-61f3be9c0aee",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+-----+---------------------------+----------+\r\n",
    "| Name|                     Scores|Math_Score|\r\n",
    "+-----+---------------------------+----------+\r\n",
    "|Alice|{math -> 85, science -> 90}|        85|\r\n",
    "|  Bob|{math -> 78, science -> 88}|        78|\r\n",
    "+-----+---------------------------+----------+\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1953ef-0389-461e-8258-98047842fc5b",
   "metadata": {},
   "source": [
    "#### Доступ к элементу в структуре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da9640-af0e-464a-bc33-6fd6eaefbb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание схемы для поля Address\n",
    "address_schema = StructType([\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"ZIP\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Создание основной схемы\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Address\", address_schema, True)\n",
    "])\n",
    "\n",
    "# Пример данных\n",
    "data = [(\"Alice\", (\"New York\", 10001)), (\"Bob\", (\"San Francisco\", 94105))]\n",
    "\n",
    "# Создание DataFrame с использованием схемы\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Доступ к полю структуры и создание нового столбца\n",
    "df_with_city = df.withColumn(\"City\", col(\"Address\").getItem(\"City\"))\n",
    "df_with_city.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d04d26-3828-4d3b-8a68-ef1f97214c6b",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+----+-----------------------+-------------+\n",
    "|Name|Address                |City         |\n",
    "+----+-----------------------+-------------+\n",
    "|Alice|{New York, 10001}     |New York     |\n",
    "|Bob  |{San Francisco, 94105}|San Francisco|\n",
    "+----+-----------------------+-------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a21330-0508-4ed2-9653-99a70dd8e6ff",
   "metadata": {},
   "source": [
    "### .getItem() в select, filter, where, и groupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320dd37-2b41-4433-a357-01c74512c2bd",
   "metadata": {},
   "source": [
    "Метод `.getItem()` чаще всего используется в контексте `withColumn` для создания нового столбца на основе элементов массива, карты или структуры в существующем столбце. Однако его также можно использовать в других методах, таких как `select, filter, where, и groupBy`, когда требуется доступ к элементам сложных типов данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89bb87-4229-434c-86d4-b676edcede1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример DataFrame с массивами\n",
    "data = [(\"Alice\", [1, 2, 3]), (\"Bob\", [4, 5, 6]), (\"Charlie\", [1, 5, 9])]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Numbers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ed3f8-ebce-4565-85dc-b2c965289db5",
   "metadata": {},
   "source": [
    "#### Использование с withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de76995-7dd0-42ca-9a3f-04c65a5f1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_first_number = df.withColumn(\"First_Number\", col(\"Numbers\").getItem(0))\n",
    "df_with_first_number.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f0628-5ac4-4374-bf55-1866bd2b0941",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+-------+---------+------------+\r\n",
    "|   Name|  Numbers|First_Number|\r\n",
    "+-------+---------+------------+\r\n",
    "|  Alice|[1, 2, 3]|           1|\r\n",
    "|    Bob|[4, 5, 6]|           4|\r\n",
    "|Charlie|[1, 5, 9]|           1|\r\n",
    "+-------+---------+------------+\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06046bf7-1a95-4506-a6d9-0e8274c8bc93",
   "metadata": {},
   "source": [
    "#### Использование с select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88d500-317c-4929-973e-a32cb4928538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df.select(col(\"Name\"), col(\"Numbers\").getItem(0).alias(\"First_Number\"))\n",
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ef796-446c-4b7e-aa21-6141f47d569f",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+-------+------------+\n",
    "|   Name|First_Number|\n",
    "+-------+------------+\n",
    "|  Alice|           1|\n",
    "|    Bob|           4|\n",
    "|Charlie|           1|\n",
    "+-------+------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e27058-52a2-4dcb-8574-796f62c8b102",
   "metadata": {},
   "source": [
    "#### Использование с filter или where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec1ea3-7ece-4152-a70e-a86fd33cb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.filter(col(\"Numbers\").getItem(0) > 2)\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c3338-2d73-4cdc-9a5c-a24ada1071f9",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+----+---------+\r\n",
    "|Name|  Numbers|\r\n",
    "+----+---------+\r\n",
    "| Bob|[4, 5, 6]|\r\n",
    "+----+---------+\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8d2e8-6a1f-4493-8f6d-2810b31912fb",
   "metadata": {},
   "source": [
    "#### Использование с groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65961b1-459a-4e31-ab4b-004c402f1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupBy(col(\"Numbers\").getItem(0)).count()\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ca426-a1c8-46da-8ee8-377518c624c4",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+----------+-----+\r\n",
    "|Numbers[0]|count|\r\n",
    "+----------+-----+\r\n",
    "|         1|    2|\r\n",
    "|         4|    1|\r\n",
    "------+----+-----+\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13951a92-0a72-465f-adde-6cb61125c7b6",
   "metadata": {},
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81becedc-ac36-4ac7-9f97-697667a13e12",
   "metadata": {},
   "source": [
    "#### UDF для строк и столбцов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac5164-e33c-4f52-9016-97a5fcb57730",
   "metadata": {},
   "source": [
    "В PySpark, UDF (User Defined Function) применяется к значениям того ряда DataFrame, в котором она вызвана. UDF работает на уровне отдельных строк, и каждая строка передаётся в UDF по отдельности. Таким образом, UDF применяется только к данным текущего ряда, когда она вызывается, и не имеет прямого доступа к значениям других рядов.\n",
    "\n",
    "Если вы хотите использовать UDF, и вам нужно работать с временными рядами или выполнять операции, которые зависят от значений нескольких строк, вам придётся преобразовать данные таким образом, чтобы все необходимые значения были доступны внутри одной строки. Это может быть сделано с помощью оконных функций или других механизмов для агрегации данных.\n",
    "\n",
    "Преобразование столбца временных рядов в строку DataFrame — это не всегда оптимальное решение и может быть воспринято как хардкод, особенно если данные имеют сложную структуру или большой объём. Однако, есть способы, чтобы сделать это более гибко и масштабируемо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d7d92-a537-4a03-88de-e1bb1284a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"2021-01-01\", 100),\n",
    "    (\"2021-01-02\", 200),\n",
    "    (\"2021-01-03\", 300),\n",
    "    (\"2021-01-04\", 400)\n",
    "]\n",
    "\n",
    "columns = [\"date\", \"value\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc0838-d713-4a57-8ff1-0f6864a29535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение окна\n",
    "window_spec = Window.orderBy(\"date\")\n",
    "\n",
    "# Использование оконной функции lag для получения предыдущего значения\n",
    "df_with_lag = df.withColumn(\"previous_value\", lag(col(\"value\"), 1).over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943fd922-3125-47e0-895d-9183c87dbf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение UDF\n",
    "def my_udf(current_value, previous_value):\n",
    "    if previous_value is None:\n",
    "        return 0\n",
    "    return current_value - previous_value\n",
    "\n",
    "my_udf = udf(my_udf, IntegerType())\n",
    "\n",
    "# Применение UDF к DataFrame\n",
    "df_with_udf = df_with_lag.withColumn(\"difference\", my_udf(col(\"value\"), col(\"previous_value\")))\n",
    "\n",
    "df_with_udf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60aea45-7a01-4d35-abad-704e112f4d31",
   "metadata": {},
   "source": [
    "Вывод\n",
    "```yaml\n",
    "+----------+-----+--------------+----------+\n",
    "|      date|value|previous_value|difference|\n",
    "+----------+-----+--------------+----------+\n",
    "|2021-01-01|  100|          null|         0|\n",
    "|2021-01-02|  200|           100|       100|\n",
    "|2021-01-03|  300|           200|       100|\n",
    "|2021-01-04|  400|           300|       100|\n",
    "+----------+-----+--------------+----------+\n",
    "```\n",
    "\n",
    "Мы определили оконную спецификацию, чтобы упорядочить строки по дате.  \n",
    "С помощью оконной функции lag мы получили значение предыдущей строки и добавили его как новый столбец previous_value.  \n",
    "Определили UDF, которая вычисляет разность между текущим значением и предыдущим значением.  \n",
    "Применили UDF к DataFrame, добавив новый столбец difference, содержащий результаты вычислений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be02f7a-34ee-44ca-812b-9571acd6fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, [0.1, 0.2, 0.3, 0.4]),\n",
    "    (2, [0.2, 0.3, 0.4, 0.5]),\n",
    "    (3, [0.3, 0.4, 0.5, 0.6])\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"time_series\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91180d61-db18-44be-b9fd-453d7fdf82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def welch_udf(time_series):\n",
    "    # Преобразование списка в numpy массив\n",
    "    time_series_np = np.array(time_series)\n",
    "    # Применение функции welch\n",
    "    f, Pxx = welch(time_series_np)\n",
    "    # Возвращение среднего значения спектральной плотности мощности как пример\n",
    "    return float(np.mean(Pxx))\n",
    "\n",
    "# Регистрация UDF\n",
    "welch_udf = udf(welch_udf, FloatType())\n",
    "\n",
    "df_with_welch = df.withColumn(\"welch_result\", welch_udf(df[\"time_series\"]))\n",
    "df_with_welch.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c81a0-8422-4c96-a1f0-0ab2e1281178",
   "metadata": {},
   "source": [
    "Работа с временными рядами большого объёма, такими как, например, ежечасные данные за год для 700 районов города (что эквивалентно более чем 6 миллионам точек данных), может представлять значительные вычислительные и ресурсные сложности. Оборачивание этих временных рядов в строки и использование UDF может быть ограничивающим из-за:\n",
    "\n",
    "- Памяти: При работе с такими большими временными рядами может возникнуть значительное потребление памяти, так как каждая строка будет содержать массив данных за год.\n",
    "- Производительности: Использование UDF в PySpark на больших объёмах данных может быть медленным, особенно если UDF выполняет сложные вычисления."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f38f97-99cf-40ff-bf78-857a33268709",
   "metadata": {},
   "source": [
    "Возможные варианты решения:\n",
    "\n",
    "**Использование Оконных Функций и Группировка:**  \n",
    "- Вместо оборачивания данных в строки, можно использовать оконные функции для обработки временных рядов.  \n",
    "Например, можно разбить данные на более мелкие временные окна (недели или месяцы) и применять функции к этим окнам, чтобы снизить нагрузку на память и улучшить производительность.  \n",
    "\n",
    "**Пакетная Обработка и Декомпозиция:**  \n",
    "- Разделить данные на меньшие, более управляемые части.  \n",
    "- Параллельно обрабатывать каждую часть и затем агрегировать результаты.  \n",
    "- Это поможет уменьшить нагрузку на память и улучшить производительность.  \n",
    "\n",
    "**Оптимизация UDF:** \n",
    "- Убедитесь, что UDF оптимизирована для работы с большими объёмами данных.  \n",
    "- Используйте библиотеки, которые поддерживают векторизацию и многопоточность.  \n",
    "\n",
    "**Использование Специализированных Инструментов:**\n",
    "- Рассмотрите возможность использования специализированных библиотек для обработки временных рядов, таких как sktime или tsfresh, которые могут быть интегрированы с PySpark.  \n",
    "- Эти библиотеки могут предоставить более эффективные методы обработки временных рядов.  \n",
    "\n",
    "**Снижение Разрешения Данных:**  \n",
    "- Если допустимо, можно снизить временное разрешение данных (например, перейти от ежечасных данных к ежедневным).  \n",
    "- Это существенно уменьшит объём данных и упростит обработку.  \n",
    "\n",
    "**Распределённые Вычисления:**\n",
    "- Использование возможностей PySpark для распределённых вычислений.  \n",
    "- Убедитесь, что ваш кластер настроен правильно для обработки больших объёмов данных, включая достаточные ресурсы памяти и процессоров.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad354f-bbb3-4a32-aafa-4850ac2c4dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b37b28-849e-485b-9eae-04be6a564481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31f2f9db-2ce4-46a1-9303-4b4c319669c0",
   "metadata": {},
   "source": [
    "#### Пример UDF (с устаревшим способом указания типа UDF)\n",
    "для возврата значения функции норм.распределения вер-ти для 1М случайных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a26972ec-8994-4f5b-bb60-6e64bd7ba81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|                val|\n",
      "+---+-------------------+\n",
      "|  0|0.15316890012395712|\n",
      "|  1| 0.6708241117542864|\n",
      "|  2| 0.1710217543507765|\n",
      "+---+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/functions.py:399: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+\n",
      "| id|                val|        probability|\n",
      "+---+-------------------+-------------------+\n",
      "|  0|0.15316890012395712|0.39428988590544156|\n",
      "|  1| 0.6708241117542864| 0.3185610866119611|\n",
      "|  2| 0.1710217543507765| 0.3931505138903692|\n",
      "|  3| 0.3144878389031438| 0.3796939165449698|\n",
      "|  4| 0.5572331819593144| 0.3415733133118225|\n",
      "+---+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 498 ms, sys: 27.2 ms, total: 525 ms\n",
      "Wall time: 9.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "big_df = (\n",
    "    spark\n",
    "    .range(0, 1_000_000)\n",
    "    .withColumn('val',f.rand())\n",
    ")\n",
    "\n",
    "big_df.cache()\n",
    "big_df.show(3)\n",
    "\n",
    "@f.pandas_udf('double', f.PandasUDFType.SCALAR)\n",
    "def pandas_pdf(v):\n",
    "    return pd.Series(stats.norm.pdf(v))\n",
    "\n",
    "(\n",
    "    big_df\n",
    "    .withColumn('probability', pandas_pdf(big_df.val))\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d486427a-7183-4688-97f9-5f57bf93e353",
   "metadata": {},
   "source": [
    "Предупреждение  \n",
    "`/opt/conda/lib/python3.11/site-packages/pyspark/sql/pandas/functions.py:399: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\r\n",
    "  warnings.warn`  \n",
    "\n",
    "связано с устареванием способа указания типа UDF (User Defined Function) в PySpark. Сообщение указывает на то, что начиная с Python 3.6 и Spark 3.0, предпочтительно использовать аннотации типов вместо указания типа UDF через параметр f.PandasUDFType.\n",
    "\n",
    "Конкретно предупреждение означает, что метод указания типа UDF через f.PandasUDFType.SCALAR будет устаревшим в будущих версиях PySpark, и рекомендуется использовать аннотации типов для Pandas UDF. Это сделано для повышения ясности кода и его поддерживаемости"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b2b360-2d14-4cdc-922e-ca61138f0b5e",
   "metadata": {},
   "source": [
    "В `@f.pandas_udf('double', f.PandasUDFType.SCALAR)`\n",
    "- первый параметр `'double'` указывает тип возвращаемого значения ЮДФ,\n",
    "- второй параметр - это тип функции. Если возвращается один столбец (`.Series(...)`), используется `.PandasUDFType.SCALAR`, если возвращается несколько столбцов (pandas `DataFrame(...)`), `.PandasUDFType.GROUPED_MAP`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b36d823-080b-4716-b243-b1f2157dfaea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Пример UDF (аргумент - Series) с аннотацией типов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e34e0443-4ef2-4cda-b209-9013b9d72c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|               val|\n",
      "+---+------------------+\n",
      "|  0|0.2316482258404361|\n",
      "|  1|0.6999371995807026|\n",
      "|  2|0.3270694747541646|\n",
      "+---+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+------------------+-------------------+\n",
      "| id|               val|                pdf|\n",
      "+---+------------------+-------------------+\n",
      "|  0|0.2316482258404361|0.38838079762633254|\n",
      "|  1|0.6999371995807026|0.31226765982726706|\n",
      "|  2|0.3270694747541646| 0.3781645903738832|\n",
      "+---+------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 64.6 ms, sys: 25.9 ms, total: 90.5 ms\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "big_df = (\n",
    "    spark\n",
    "    .range(0, 1_000_000)\n",
    "    .withColumn('val', f.rand())\n",
    ")\n",
    "\n",
    "big_df.cache()\n",
    "big_df.show(3)\n",
    "\n",
    "@f.pandas_udf(DoubleType())\n",
    "def pandas_pdf(v: pd.Series) -> pd.Series:\n",
    "    return pd.Series(stats.norm.pdf(v))\n",
    "\n",
    "result = big_df.withColumn('pdf', pandas_pdf(f.col('val')))\n",
    "result.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3bbc7-9015-4c18-8478-e47a8ae347af",
   "metadata": {},
   "source": [
    "Здесь `DoubleType()` используется вместо `f.PandasUDFType.SCALAR`, и типы указаны непосредственно в аннотациях функции `(v: pd.Series) -> pd.Series`. Это улучшает читаемость кода и соответствует новым стандартам PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e067f2-4e7f-4479-badf-ef564dadbec5",
   "metadata": {},
   "source": [
    "В актуальном способе указания типов для PySpark UDF для определения UDF, которая возвращает несколько столбцов (pandas DataFrame), вы можете использовать аннотацию функции и указание типа возвращаемого значения при помощи PySpark StructType. В этом случае вам понадобится указать схему (schema), описывающую структуру возвращаемого DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff4ad3-c243-48ad-89ee-2ac33b1d3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "\n",
    "# Определяем схему возвращаемого DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"col1\", DoubleType(), True),\n",
    "    StructField(\"col2\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Определяем Pandas UDF\n",
    "@pandas_udf(schema)\n",
    "def my_udf(input_col: pd.Series) -> pd.DataFrame:\n",
    "    # Ваш код для обработки DataFrame\n",
    "    result = pd.DataFrame({\n",
    "        \"col1\": input_col * 2,\n",
    "        \"col2\": input_col.astype(str)\n",
    "    })\n",
    "    return result\n",
    "\n",
    "# Создаем DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [(1.0,), (2.0,), (3.0,)],\n",
    "    ['input_col']\n",
    ")\n",
    "\n",
    "# Применяем UDF к столбцу\n",
    "df.select(my_udf(df['input_col'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e70a56-c5a1-49e8-b2e0-9a9295f8768b",
   "metadata": {},
   "source": [
    "Pandas UDF в PySpark может принимать несколько pandas Series объектов в качестве аргументов. Для этого необходимо использовать декоратор @pandas_udf и определить схему выходного DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0e4fc-5ab5-4c02-a092-ea7ac440ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем схему выходного DataFrame\n",
    "output_schema = StructType([\n",
    "    StructField('output_col1', DoubleType()),\n",
    "    StructField('output_col2', DoubleType())\n",
    "])\n",
    "\n",
    "# Определяем Pandas UDF с использованием декоратора @pandas_udf\n",
    "@pandas_udf(output_schema)\n",
    "def my_udf(input_col1: pd.Series, input_col2: pd.Series) -> pd.DataFrame:\n",
    "    # Создаем DataFrame из двух входных Series\n",
    "    output_df = pd.DataFrame({\n",
    "        'output_col1': input_col1 + input_col2,\n",
    "        'output_col2': input_col1 * input_col2\n",
    "    })\n",
    "    return output_df\n",
    "\n",
    "# Создаем DataFrame для теста\n",
    "df = spark.createDataFrame(\n",
    "    [(1.0, 2.0), (3.0, 4.0), (5.0, 6.0)],\n",
    "    ['input_col1', 'input_col2']\n",
    ")\n",
    "\n",
    "# Применяем Pandas UDF к DataFrame\n",
    "result_df = df.select(my_udf(df['input_col1'], df['input_col2']))\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6e373-53b4-4039-8b08-ad4378a8acfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Пример UDF (аргумент - DataFrame) с аннотацией типов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba50e1c-06d1-4019-becc-b3b789f5a760",
   "metadata": {},
   "source": [
    "Вы можете передать в UDF аргумент в виде DataFrame, но это будет работать только с UDF типа GROUPED_MAP. В этом случае каждый входной DataFrame будет представлять собой группу данных, по которой будет выполняться UDF. Для этого типа UDF вам необходимо определить схему входного DataFrame и схему выходного DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99fded-d065-40d9-b3e5-12e9e548143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем схему входного DataFrame\n",
    "input_schema = StructType([\n",
    "    StructField(\"col1\", DoubleType(), True),\n",
    "    StructField(\"col2\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Определяем схему возвращаемого DataFrame\n",
    "output_schema = StructType([\n",
    "    StructField(\"col1\", DoubleType(), True),\n",
    "    StructField(\"col2\", DoubleType(), True),\n",
    "    StructField(\"col3\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Определяем Pandas UDF типа GROUPED_MAP\n",
    "@pandas_udf(output_schema, f.PandasUDFType.GROUPED_MAP)\n",
    "def my_udf(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Ваш код для обработки DataFrame\n",
    "    pdf[\"col3\"] = (pdf[\"col1\"] + pdf[\"col2\"]).astype(str)\n",
    "    return pdf\n",
    "\n",
    "# Создаем DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [(1.0, 2.0), (3.0, 4.0), (5.0, 6.0)],\n",
    "    ['col1', 'col2']\n",
    ")\n",
    "\n",
    "# Применяем UDF к DataFrame с использованием groupBy и apply\n",
    "# В данном случае мы используем groupBy().apply() даже без явного группирования\n",
    "df.groupBy().apply(my_udf).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bc7f6-16d8-4864-a392-d58fbdcceed8",
   "metadata": {},
   "source": [
    "Возможное предупреждение  \n",
    "`/opt/conda/lib/python3.11/site-packages/pyspark/sql/pandas/group_ops.py:103: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
    "  warnings.warn(`  \n",
    "говорит о том, что используется устаревший способ `df.groupBy().apply(my_udf).show()`. Pandas UDFs с использованием `applyInPandas` предоставляют более современный и предпочтительный способ обработки pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079987e-4705-42ab-b22b-9cbaf7df180e",
   "metadata": {},
   "source": [
    "Pandas UDF в режиме GROUPED_MAP не поддерживает передачу нескольких pandas DataFrame в качестве отдельных аргументов. Вместо этого вы должны объединить данные в один DataFrame до передачи в функцию. В режиме SCALAR Pandas UDF может принимать несколько Series в качестве аргументов.\n",
    "\n",
    "Для использования нескольких pandas DataFrame в Pandas UDF с декоратором @pandas_udf в режиме GROUPED_MAP, вам нужно объединить два DataFrame в один перед тем, как вызывать функцию. PySpark Pandas UDF с типом GROUPED_MAP ожидает один pandas DataFrame в качестве аргумента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12113131-d532-462d-8227-87f98f9cb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType\n",
    "\n",
    "# Определяем схему выходного DataFrame\n",
    "output_schema = StructType([\n",
    "    StructField('id', IntegerType()),\n",
    "    StructField('output_col1', DoubleType()),\n",
    "    StructField('output_col2', DoubleType())\n",
    "])\n",
    "\n",
    "# Определяем Pandas UDF с использованием декоратора @pandas_udf\n",
    "@pandas_udf(output_schema, f.PandasUDFType.GROUPED_MAP)\n",
    "def my_udf(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Создаем выходной DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        'id': pdf['id'],\n",
    "        'output_col1': pdf['input_col1'] + pdf['input_col2'],\n",
    "        'output_col2': pdf['input_col1'] * pdf['input_col2']\n",
    "    })\n",
    "    return output_df\n",
    "\n",
    "# Создаем два DataFrame для теста\n",
    "df1 = spark.createDataFrame(\n",
    "    [(1, 1.0), (2, 3.0), (3, 5.0)],\n",
    "    ['id', 'input_col1']\n",
    ")\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(1, 2.0), (2, 4.0), (3, 6.0)],\n",
    "    ['id', 'input_col2']\n",
    ")\n",
    "\n",
    "# Объединяем два DataFrame по столбцу 'id'\n",
    "df = df1.join(df2, on='id')\n",
    "\n",
    "# Применяем Pandas UDF к DataFrame с использованием groupBy\n",
    "result_df = df.groupBy('id').apply(my_udf)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c349bb5-a91a-4427-8c55-68a572ffb97b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Сравнение производительности векторизированой и не_ ЮДФ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf26fbc-5582-4bdc-bcc2-7afa4e7ab81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = (\n",
    "    spark\n",
    "    .range(0, 1_000_000)\n",
    "    .withColumn('val',f.rand())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cba0e8-b839-4f44-af70-45625e39f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pandas_pdf():\n",
    "    return (\n",
    "        big_df\n",
    "        .withColumn('probability', pandas_pdf(big_df.val))\n",
    "        .agg(f.count(f.col('probability')))\n",
    "        .show()\n",
    "    )\n",
    "\n",
    "%timeit -n 1 test_pandas_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438425a-b2c7-441e-9252-8f7941e011b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#row-by-row version with Python-JVM conversion\n",
    "@f.udf('double')\n",
    "def pdf(v):\n",
    "    return float(stats.norm.pdf(v))\n",
    "\n",
    "def test_pdf():\n",
    "    return (\n",
    "        big_df\n",
    "        .withColumn('probability', pdf(big_df.val))\n",
    "        .agg(f.count(f.col('probability')))\n",
    "        .show()\n",
    "    )\n",
    "\n",
    "%timeit -n 1 test_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0260961-d3f0-408f-8fcc-03e753fa5bc5",
   "metadata": {},
   "source": [
    "Результат для векторизированной ЮДФ - 903 ms, против не_ - 34.4 s\n",
    "\n",
    "Отличие примерно в 40 раз!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8363071c-8b2a-4a72-b9c0-9f66f9dab9d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Использование объектов pandas в Pandas UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0363cba-7f9c-445b-8325-214530329203",
   "metadata": {},
   "source": [
    "В **Pandas UDF** (User Defined Function) с использование декоратора `@pandas_udf` в Spark DataFrame API, функция pandas_pdf принимает один или несколько pandas Series (DataFrame) объектов и возвращает pandas Series (DataFrame) объект. В этом случае, pandas_pdf принимает Series v (который представляет собой значения из столбца val в DataFrame) и возвращает Series, содержащий значения, вычисленные с использованием функции stats.norm.pdf().\n",
    "\n",
    "Возвращая `pd.Series(stats.norm.pdf(v))`, мы создаем объект Pandas Series, который содержит значения, вычисленные с помощью функции `stats.norm.pdf()`, примененные к каждому элементу входного Series v. Это необходимо потому, что Spark использует PyArrow для обмена данными между JVM и Python, и PyArrow не может обрабатывать объекты `stats.norm.pdf(v)` напрямую, так как они не являются объектами pandas.\n",
    "\n",
    "Поэтому, оборачивая результат stats.norm.pdf(v) в pd.Series(), мы создаем объект pandas, который Spark может обработать, и возвращаем его из функции pandas_pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa056b4d-cf76-48a6-b6d3-88bd694310bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## .cache() .unpersist() - кэш памяти"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893e67b-1815-44c9-abcb-381652cd4b4b",
   "metadata": {},
   "source": [
    "Когда вы вызываете `big_df.cache()`, вы помещаете DataFrame big_df в **кэш памяти**. Это означает, что DataFrame будет кэширован в памяти и будет доступен для быстрого доступа к данным при последующих операциях.\n",
    "\n",
    "Однако, если у вас есть ограничения на использование памяти или если кэш становится необходимым для других операций, вы можете явно удалить кэш с помощью метода `unpersist()`.\n",
    "\n",
    "Это удалит кэш из памяти и освободит ресурсы, занимаемые DataFrame big_df. Если вы не удаляете кэш явно с помощью unpersist(), DataFrame останется в кэше и будет занимать память до тех пор, пока не будет удален Spark контекст или пока явно не будет вызвана операция `unpersist()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9dc903-7e1e-4bdf-9657-59d95cd471b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be90c3-fa49-4296-a82d-acf368a0e911",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Apache Arrow Memory & runtime switch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67575ea-37bf-4abb-8042-6669230189cf",
   "metadata": {},
   "source": [
    "До версии ПиСпарк 2.3 использование ЮДФ в Питоне проигрывало по производительности Скале и Джаве (из-за переключений времени выполнения `runtime` между Питон и JVM).\n",
    "\n",
    "С версии 2.3 Спарк начал использовать **Apache Arrow Memory** - общее пространство памяти для разных сред окружения, поэтому отпала необходимость и расходны на конвертацию объектов.\n",
    "\n",
    "Также Arrow хранит в памяти **столбчатые объекты** (columnar objects), что повышает производительность. Используя это, ПиСпарк отрефакторилился, в результате чего мы получили **векторизированные ЮДФ**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa731e-51c9-447b-8236-42593ab214fb",
   "metadata": {},
   "source": [
    "В аспектах программирования и операционных систем (в частности, процессов и потоков), термины \"runtime\" и \"switch runtime\" имеют разные значения. Давайте рассмотрим их подробнее:\n",
    "\n",
    "**Runtime**  \n",
    "**Программирование:**  \n",
    "`Runtime` (время выполнения) - это фаза, когда программа исполняется. В это время происходят все динамические операции, такие как выполнение кода, управление памятью, обработка исключений и т.д. Например, в Python есть интерпретатор, который выполняет код во время исполнения программы.\n",
    "`Runtime Environment` (среда выполнения) - это окружение, в котором исполняется программа. Оно включает в себя библиотеки, интерпретаторы (например, `Python Interpreter`) или виртуальные машины (например, `Java Virtual Machine`).  \n",
    "**Процессы и потоки ОС:**  \n",
    "`Runtime` в контексте ОС может также относиться к продолжительности времени, когда процесс или поток активен и выполняется на процессоре. Это время исполнения может включать в себя время, проведенное на процессоре, и время ожидания (например, ожидание ввода-вывода).\n",
    "\n",
    "**Switch Runtime**  \n",
    "**Программирование:**  \n",
    "`Switch Runtime` (переключение времени выполнения) в программировании часто означает смену среды выполнения для кода. Например, если вы хотите исполнять код, написанный на Python, в среде JVM, это может потребовать использование специального инструмента или транслятора, который позволит исполнять Python-код на Java Virtual Machine (например, Jython).  \n",
    "**Процессы и потоки ОС:**  \n",
    "`Context Switch` (переключение контекста) - это процесс, при котором операционная система переключается от выполнения одного процесса или потока к другому. Это необходимо для многозадачности, чтобы несколько процессов могли совместно использовать ресурсы процессора.\n",
    "В данном контексте, \"switch runtime\" может подразумевать переключение между разными процессами или потоками в ходе их выполнения. То есть операционная система переключает время выполнения между различными задачами для обеспечения параллелизма и эффективного использования процессора.\n",
    "\n",
    "**Примеры**  \n",
    "**Программирование:**  \n",
    "Вы используете Python для разработки, но вам нужно интегрировать код с существующим приложением на Java. Вы используете Jython для \"switch runtime\" и исполняете Python-код в JVM.\n",
    "**Процессы и потоки ОС:**  \n",
    "В многозадачной операционной системе несколько программ работают одновременно. Процессор переключается между задачами (процессами) каждые несколько миллисекунд, обеспечивая иллюзию параллельного выполнения. Это и есть \"context switch\".\n",
    "\n",
    "**Резюме**  \n",
    "`Runtime` (время выполнения): в программировании - фаза исполнения программы; в ОС - время, когда процесс или поток активно выполняется.\n",
    "`Switch Runtime` (переключение времени выполнения): в программировании - смена среды выполнения для кода; в ОС - переключение контекста между процессами или потоками для многозадачности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8222495-788b-4573-9395-7518855a028a",
   "metadata": {},
   "source": [
    "## Column объекты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e6744-53c7-4b0b-b1e6-88d279367a11",
   "metadata": {},
   "source": [
    "### Использование объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1144db76-a0f2-45e3-92dc-5bd238d9dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"John Doe\", 30), (\"Jane Smith\", 25), (\"Bob Johnson\", 35)]\n",
    "columns = [\"full name\", \"age\"]\n",
    "\n",
    "# Создание DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Обращение к полям через точку\n",
    "df.show()\n",
    "df.select(\"full name\").show()\n",
    "print(df[\"full name\"])\n",
    "print(df.age)\n",
    "df.age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451537f-708d-4cbd-8264-d8f4fac9a9bf",
   "metadata": {},
   "source": [
    "Для инструкции `df.age.show()` получим ошибку TypeError: 'Column' object is not callable. Обращение к столбцам через точку, как в df.age, может вызвать ошибку TypeError, так как это возвращает объект Column, который не является вызываемым (callable) для метода show().\n",
    "\n",
    "Объекты `df.age` и `df[\"full name\"]` в PySpark являются объектами типа Column. Column - это абстракция, представляющая столбец данных в DataFrame. Объект Column обеспечивает различные методы для выполнения операций на столбцах данных, таких как вычисления, фильтрация, агрегация и т. д.\n",
    "\n",
    "Чтобы использовать объект Column, вы можете использовать его методы для создания новых столбцов, применения функций к существующим столбцам и других операций с данными в DataFrame. Например, вы можете использовать объект Column для создания нового столбца, содержащего возраст пассажиров в годах путем деления столбца \"age\" на 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296fc7b-7c53-4714-8f2d-03b6224b49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"age_in_decades\", df.age/10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed8de4-3f06-47c9-9a16-01a03721db29",
   "metadata": {},
   "source": [
    "Или можете отфильтровать датафрейм по полю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9be782-509e-46e7-bbe3-0f83a9c8d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.age_in_decades > 3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5a8b6-a638-4ffb-8ce8-b4cac6f9d0ca",
   "metadata": {},
   "source": [
    "### Обращение к столбцам через точку"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4320adf-5c03-440c-9669-f5b700749ed7",
   "metadata": {},
   "source": [
    "Обращение к полям DataFrame в ПиСпарк через точку, если имена полей содержат несколько слов через пробел, невозможно - необходимо использовать квадратные скобки и строковые индексы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904bdb57-b87d-40c0-b436-0fc21a4e1124",
   "metadata": {},
   "source": [
    "### Функция col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5629fe-96c4-4b53-a298-ab1a6514b68c",
   "metadata": {},
   "source": [
    "Функция `col` в PySpark используется для указания на столбцы в DataFrame. Это удобно, когда вы хотите ссылаться на столбцы программным способом, особенно в выражениях и операциях DataFrame.\n",
    "\n",
    "```python\n",
    "df.select(col(\"columnName\"))\n",
    "\n",
    "```\n",
    "Здесь `col(\"columnName\")` то же самое, что и `df[\"columnName\"]` - объект Column\n",
    "\n",
    "В большинстве случаев инструкции `df.select(col(\"columnName\"))` и `df.select(\"columnName\")` ведут себя одинаково и возвращают один и тот же результат.\n",
    "\n",
    "С `col` вы можете легко создавать более **сложные выражения**.\n",
    "```python\n",
    "df.select((col(\"columnName\") + 1).alias(\"newColumnName\"))\n",
    "\n",
    "```\n",
    "\n",
    "При использовании `alias` (псевдонимов таблиц) `col` позволяет вам указывать **полное имя столбца**.\n",
    "```python\n",
    "df.alias(\"a\").select(col(\"a.columnName\"))\n",
    "\n",
    "```\n",
    "\n",
    "В сложных выражениях и join-ах `col` может сделать **код более читаемым** и понятным.\n",
    "```python\n",
    "df1.join(df2, col(\"df1.columnName\") == col(\"df2.columnName\"))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa86c7bd-ecac-4c25-b61f-098c83824f5e",
   "metadata": {},
   "source": [
    "**Использование `col` полезно, когда**:  \n",
    "- Вы работаете с более сложными выражениями.\n",
    "- Вы работаете с несколькими DataFrame и хотите избежать неоднозначностей в именах столбцов.\n",
    "- Вам нужно указать столбец программно, особенно в циклах или функциях.  \n",
    "\n",
    "**Когда можно использовать строки напрямую**:  \n",
    "Использование строк напрямую может быть удобным для простых операций выборки, когда вы выбираете один или несколько столбцов без сложных выражений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f860f8b-48db-4530-95a3-727b9729ff25",
   "metadata": {},
   "source": [
    "**Если имя столбца в DataFrame содержит пробелы**, вам нужно будет использовать метод col и обернуть имя столбца в обратные кавычки ( \\` ). Это необходимо для того, чтобы правильно идентифицировать такие имена столбцов в PySpark.  \n",
    "```python\n",
    "df1.join(df2, col(\"`df1.column Name`\") == col(\"df2.columnName\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f1666-13c8-4d58-9961-8bf6f2196f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример данных для df1 и df2\n",
    "data1 = [(\"John\", 28), (\"Doe\", 35)]\n",
    "columns1 = [\"column Name\", \"Age\"]\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "\n",
    "data2 = [(\"John\", \"New York\"), (\"Doe\", \"Los Angeles\")]\n",
    "columns2 = [\"columnName\", \"City\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Присвоение алиасов df1 и df2\n",
    "df1_alias = df1.alias(\"a\")\n",
    "df2_alias = df2.alias(\"b\")\n",
    "\n",
    "# Выполнение join с использованием col и алиасов\n",
    "result = df1_alias.join(df2_alias, col(\"a.`column Name`\") == col(\"b.columnName\"))\n",
    "\n",
    "# Выбор столбцов для отображения\n",
    "result.select(\n",
    "    col(\"a.`column Name`\").alias(\"column_Name\"), \n",
    "    col(\"b.City\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d038e-eb8c-4038-9c83-a845963dde86",
   "metadata": {},
   "source": [
    "Если вы используете **переменные для создания строкового имени столбца** (содержащего пробелы) и конкатенируете его, то вам не нужно дополнительно оборачивать имя столбца в обратные кавычки. PySpark корректно обработает такой синтаксис. Обратные кавычки необходимы только для строковых литералов, которые содержат пробелы или другие специальные символы.\n",
    "```python\n",
    "...\n",
    ".select(\n",
    "    f.col(\"b.\" + ct.pickup_ct), \n",
    "    f.col(\"b.\" + ct.pickup_cl), \n",
    "    f.col(\"b.\" + ct.pickup_clon)\n",
    ")\n",
    "```\n",
    "\n",
    "полный пример в разделе про джойны"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dee397-ee81-4c57-902d-f24632d7aa57",
   "metadata": {},
   "source": [
    "#### Динамическое использование "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd83bd-82cd-4c41-8dcf-26e2d9fb96d2",
   "metadata": {},
   "source": [
    "**Динамическое использование** функции col в PySpark позволяет вам строить имена столбцов на лету, используя переменные или выражения.\n",
    "Это особенно полезно, когда вы хотите написать универсальный код, который работает с различными именами столбцов в зависимости от условий или данных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d7ad92-06f9-4669-b7c4-861f9c6542ca",
   "metadata": {},
   "source": [
    "**Использование выражений**\n",
    "```python\n",
    "df.select(col(\"prefix_\" + column_name))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d49ccb-f2aa-492c-b713-5a75d253c187",
   "metadata": {},
   "source": [
    "**В циклах и функциях**  \n",
    "Пример для подсчета пропущенных значенийЖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9d38e-f8b2-44e9-bbc4-ad63e4107ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример данных\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Math\", \"A\"),\n",
    "    (\"Bob\", None, \"Physics\", \"B\"),\n",
    "    (\"Charlie\", 30, \"History\", \"\"),\n",
    "    (\"Diana\", 22, \"\", \"A\"),\n",
    "    (\"Eve\", None, \"Math\", \"C\"),\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"Subject\", \"Grade\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Применяем код для подсчета пустых значений, NULL и NaN\n",
    "df.select(\n",
    "    [count(when(col(c)=='') | col(c).isNull() | isnan(c), c).alias(c) for c in df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65dd7c-ebc6-439d-ac16-f177eca93eb1",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+-------+---+--------------+-----+\n",
    "|   Name|Age|       Subject|Grade|\n",
    "+-------+---+--------------+-----+\n",
    "|      0|  2|             1|    1|\n",
    "+-------+---+--------------+-----+--+\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dee91e-7a49-4b91-9fc8-32d88c3ddb0e",
   "metadata": {},
   "source": [
    "**Фильтрация по результатам агрегации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daab91e-2451-4d17-8ad0-e6166e8907cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример данных\n",
    "data = [(\"John\", \"Sales\"), (\"Doe\", \"HR\"), (\"Alice\", \"Sales\")]\n",
    "columns = [\"Name\", \"Department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Фильтрация по количеству записей в группе \"Department\" больше 1\n",
    "df.groupBy(\"Department\").count().filter(col(\"count\") > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792da62b-a0ac-42a6-81d0-7d6691ad57a4",
   "metadata": {},
   "source": [
    "**Добавление вычисляемого столбца и его использование для фильтрации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b52b655-f1a8-45ba-a709-d82ba2e39ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример данных\n",
    "data = [(\"John\", 28), (\"Doe\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Добавление столбца \"is_adult\" и фильтрация по нему\n",
    "df.withColumn(\"is_adult\", when(col(\"Age\") >= 18, \"Adult\").otherwise(\"Minor\")).filter(col(\"is_adult\") == \"Adult\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc45363-453f-42e3-b661-91358c161414",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Джойны"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f43df-214d-4475-bc07-ef247ba226ca",
   "metadata": {},
   "source": [
    "Подготовим данные для примеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b35a780-6223-4baf-b226-03ede84baf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctp - pickup census tract\n",
    "ctp = taxi.groupBy(ct.pickup_ct).count().orderBy(\"count\", ascending=False).cache()\n",
    "# ctd - dropoff census tract\n",
    "ctd = taxi.groupBy(ct.dropoff_ct).count().orderBy(\"count\", ascending=False).cache()н"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d59e3-bba4-47b1-9931-a49e2c2ba0fa",
   "metadata": {},
   "source": [
    "В Apache Spark доступны следующие типы джойнов:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4256cd5a-961f-4118-8c21-73e3756f830b",
   "metadata": {},
   "source": [
    "**inner:**  \n",
    "Возвращает строки, которые имеют совпадающие значения в обеих таблицах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8751f59-60cd-4fb4-bd0d-32304032e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_join = ctd.join(ctp, ctd[ct.dropoff_ct] == ctp[ct.pickup_ct], 'inner')\n",
    "inner_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c8a8d-dc32-4f97-a72d-1b0fff947c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join с проверкой совпадения по двум столбцам\n",
    "inner_join = ctd.join(ctp, (ctd[ct.dropoff_ct] == ctp[ct.pickup_ct]) & \\\n",
    "                      (ctd['count'] == ctp['count']), 'inner')\n",
    "inner_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31214cec-f376-4731-a153-8c11a4ba6902",
   "metadata": {},
   "source": [
    "**cross:**  \n",
    "Возвращает декартово произведение двух таблиц, где каждая строка первой таблицы соединяется с каждой строкой второй таблицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e176fc-7ca7-4b10-99d2-c9b560cf359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_join = ctd.crossJoin(ctp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ab69a-33cd-4941-ba25-938720f4ed40",
   "metadata": {},
   "source": [
    "**outer, full, full_outer:**  \n",
    "Возвращает все строки, когда есть совпадение в одной из таблиц. Включает строки, не имеющие совпадений из обеих таблиц.  \n",
    "Методы outer, full, и full_outer в контексте Apache Spark DataFrame join фактически являются синонимами и выполняют одинаковую операцию. Они все возвращают полный внешний джойн (full outer join), что означает, что они возвращают все строки из обеих таблиц, когда есть совпадение по ключу, или строки с NULL, если нет совпадения.  \n",
    "Три названия (outer, full, и full_outer) предоставляются для удобства и читаемости кода. Вот основные причины, почему может быть несколько названий для одной и той же операции:  \n",
    "- Читаемость и предпочтения разработчиков. В разных системах и инструментах SQL могут использоваться разные термины для одной и той же операции. Предоставляя несколько вариантов, Spark обеспечивает совместимость с разными системами и инструментами, делая код более переносимым.\n",
    "- Совместимость и переносимость. В некоторых контекстах или командах могут быть общепринятые стандарты или конвенции наименования. Spark предоставляет гибкость, чтобы разработчики могли следовать этим стандартам без необходимости адаптировать терминологию.\n",
    "- Общепринятые стандарты.  некоторых контекстах или командах могут быть общепринятые стандарты или конвенции наименования. Spark предоставляет гибкость, чтобы разработчики могли следовать этим стандартам без необходимости адаптировать терминологию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6255382-f1dd-4bd2-812a-2ffffc4a73ff",
   "metadata": {},
   "source": [
    "**Left, left_outer:** Возвращает все строки из левой таблицы и совпадающие строки из правой таблицы. Если нет совпадения, результаты правой таблицы будут NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640547c5-ba40-459f-b60f-baf1936b7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_join = ctd.join(ctp, ctd[ct.dropoff_ct] == ctp[ct.pickup_ct], 'left')\n",
    "left_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ffe39-cbf3-409f-87e6-3e98695a763b",
   "metadata": {},
   "source": [
    "**right, right_outer:** Возвращает все строки из правой таблицы и совпадающие строки из левой таблицы. Если нет совпадения, результаты левой таблицы будут NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e236c03-d4be-487d-954d-52019fdf1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_join = ctd.join(ctp, ctd[ct.dropoff_ct] == ctp[ct.pickup_ct], 'right')\n",
    "right_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc1373-698c-47ae-a03c-8ec66c141a04",
   "metadata": {},
   "source": [
    "**left_semi:** Возвращает только те строки из левой таблицы, для которых существуют совпадающие строки в правой таблице. В отличие от других типов джойнов, он не возвращает строки из правой таблицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0aa1b-0fc2-4642-9a6f-7dc9c598af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_semi_join = ctd.join(ctp, ctd[ct.dropoff_ct] == ctp[ct.pickup_ct], 'left_semi')\n",
    "left_semi_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553d5df-3b99-4e43-b892-0d3bd93381aa",
   "metadata": {},
   "source": [
    "**left_anti:** Возвращает только те строки из левой таблицы, для которых не существует совпадающих строк в правой таблице. Этот тип джойна используется для нахождения строк, которые существуют в левой таблице, но не имеют совпадений в правой таблице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62a281-a228-40f7-8b60-0b76f3bae055",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_anti_join = ctd.join(ctp, ctd[ct.dropoff_ct] == ctp[ct.pickup_ct], 'left_anti')\n",
    "left_anti_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f0e18-71a2-48d1-ba92-19d29fec687e",
   "metadata": {},
   "source": [
    "### Джойны: DF API vs SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64addd56-bec1-4094-9c2f-ecb1b17f46ab",
   "metadata": {},
   "source": [
    "Джойны с использованием АПИ датафрейма могут быть более громоздкими в плане синтаксиса из-за необходимости использования конструкций с псевдонимами (`alias`) и функций `col`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef88d1-21fd-4215-9416-56e17f2170f7",
   "metadata": {},
   "source": [
    "Подготовим таблицы для джойна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64f261-b0a0-4f96-ace8-59453794c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ДФ по районам посадки, содержащий тройки: код р-на, широта, долгота\n",
    "ctp_geo = taxi.groupBy(ct.pickup_ct,ct.pickup_cl,ct.pickup_clon).count().orderBy(\"count\", ascending=False)\n",
    "ctp_geo.cache()\n",
    "# Посмотрим повторяющиеся геоточки\n",
    "ctp_geo_dupl = ctp_geo.groupBy(ct.pickup_cl,ct.pickup_clon).count().filter(f.col('count') > 1).cache()\n",
    "ctp_geo_dupl.show()\n",
    "# Отберем отдельно те из них, которые имеют непустое значение для кода района.\n",
    "ctp_geo_nulls = ctp_geo.filter(ctp_geo[ct.pickup_cl].isNull() \\\n",
    "               & ctp_geo[ct.pickup_clon].isNull()\\\n",
    "               & ctp_geo[ct.pickup_ct].isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ca2d5-a133-452b-bc2e-79881ff4d38c",
   "metadata": {},
   "source": [
    "Свяжем таблицы, используя ДФ АПИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e92a5-ec5f-4eae-8eaa-d108a913c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cвяжем дубликаты с непустыми тройками (р-н, шир, долг)\n",
    "# Также посмотрим коды р-на, для повторяющихся непустых геоточек\n",
    "# Фильтрация и присвоение алиаса \"a\" для ctp_geo_dupl\n",
    "ctp_geo_dupl_filtered = ctp_geo_dupl.filter(\n",
    "    ctp_geo_dupl[ct.pickup_cl].isNotNull() & ctp_geo_dupl[ct.pickup_clon].isNotNull()\n",
    ").alias(\"a\")\n",
    "\n",
    "# Присвоение алиаса \"b\" для ctp_geo\n",
    "ctp_geo_alias = ctp_geo.alias(\"b\")\n",
    "\n",
    "# Выполнение join с использованием алиасов и квалифицированных имен столбцов\n",
    "result = ctp_geo_dupl_filtered.join(\n",
    "    ctp_geo_alias,\n",
    "    (f.col(\"a.\" + ct.pickup_cl) == f.col(\"b.\" + ct.pickup_cl)) & \n",
    "    (f.col(\"a.\" + ct.pickup_clon) == f.col(\"b.\" + ct.pickup_clon))\n",
    ").select(\n",
    "    f.col(\"b.\" + ct.pickup_ct), \n",
    "    f.col(\"b.\" + ct.pickup_cl), \n",
    "    f.col(\"b.\" + ct.pickup_clon)\n",
    ")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a08d1-15b0-4cda-b0bb-43b9a16e8ed6",
   "metadata": {},
   "source": [
    "Свяжем таблицы, используя SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca53798a-5375-44e6-98e7-2be2a7a7f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctp_geo_dupl.createOrReplaceTempView(\"ctp_geo_dupl\")\n",
    "ctp_geo.createOrReplaceTempView(\"ctp_geo\")\n",
    "\n",
    "sql_query = f\"\"\"\n",
    "SELECT b.`{ct.pickup_ct}`, b.`{ct.pickup_cl}`, b.`{ct.pickup_clon}`\n",
    "FROM ctp_geo_dupl a\n",
    "JOIN ctp_geo b\n",
    "ON a.`{ct.pickup_cl}` = b.`{ct.pickup_cl}` \n",
    "AND a.`{ct.pickup_clon}` = b.`{ct.pickup_clon}`\n",
    "WHERE a.`{ct.pickup_cl}` IS NOT NULL \n",
    "AND a.`{ct.pickup_clon}` IS NOT NULL\n",
    "\"\"\"\n",
    "result = spark.sql(sql_query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b43ac-22c6-402a-a5fa-1a921b9fc0ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## df.selectExpr(...) и spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d34c75-d1ef-4b06-b3fc-8121958d769e",
   "metadata": {},
   "source": [
    "Инструкции `df.selectExpr(...)` и `spark.sql(sql_query)` выполняют схожие функции по выборке данных, но отличаются по способу использования и контексту:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b20c9-9eb4-4753-be31-a1a3148f5eaf",
   "metadata": {},
   "source": [
    "### df.selectExpr(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8243f95-81ba-4684-bec9-12fef51ddd0e",
   "metadata": {},
   "source": [
    "`selectExpr` используется для выполнения выражений SQL на уровне DataFrame API. Это позволяет выполнять SQL-подобные выражения непосредственно на DataFrame без необходимости регистрации его как временной таблицы.\n",
    "```python\n",
    "df.selectExpr(\"col1\", \"col2\", \"col1 + col2 as sum_col\").show()\r\n",
    "```\n",
    "\n",
    "- Преимущества:  \n",
    "Прямое использование выражений SQL в контексте DataFrame.  \n",
    "Не требует создания временных таблиц.  \n",
    "Удобен для простых выборок и выражений.  \n",
    "\n",
    "- Недостатки:  \n",
    "Ограничен только выборкой данных и вычислением выражений в пределах одного DataFrame.  \n",
    "Не подходит для выполнения сложных SQL-запросов с участием нескольких таблиц (например, join).  `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3dd11-7485-41d3-87f7-e89eeb8b98c7",
   "metadata": {},
   "source": [
    "### spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a856f082-1928-4c2b-bdce-06b37e78c2c2",
   "metadata": {},
   "source": [
    "`spark.sql(sql_query)` используется для выполнения полного SQL-запроса в контексте `SparkSession`. Это позволяет использовать весь синтаксис SQL, включая JOIN, GROUP BY, ORDER BY и другие сложные операции.\n",
    "\n",
    "```python\n",
    "sql_query = \"\"\"\r\n",
    "SELECT col1, col2, col1 + col2 as sum_col\r\n",
    "FROM my_table\r\n",
    "WHERE col1 > 10\r\n",
    "\"\"\"\r\n",
    "spark.sql(sql_query).shw()\r\n",
    "\n",
    "-Преимущества:  \n",
    "Полная поддержка SQL-синтаксиса.  \n",
    "Подходит для сложных запросов, включающих несколько таблиц и сложные условия.  \n",
    "Более знаком многим пользователям, имеющим опыт работы с SQL.  \n",
    "\n",
    "- Недостатки:  \n",
    "Требует регистрации DataFrame как временной таблицы с помощью createOrReplaceTempView.  \n",
    "Может быть менее интуитивным для пользователей, предпочитающих работать с API DataFrame.  \n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a266a3c-da31-4660-aa14-a8e36a36860c",
   "metadata": {},
   "source": [
    "## Метод .alias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00885ca6-4e7e-4e58-8fa2-ba426ea01235",
   "metadata": {},
   "source": [
    "Метод .alias в PySpark используется для двух основных целей:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18478015-d0b3-434e-bd6d-d30efaaa053a",
   "metadata": {},
   "source": [
    "### DataFrame - Присвоение псевдонимов "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6cfdf-adc5-4e05-b2f5-3c5edad23c2a",
   "metadata": {},
   "source": [
    "Присвоение псевдонимов DataFrame позволяет удобно работать с именами таблиц, особенно при выполнении операций соединения (join), когда требуется различать столбцы из разных DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5c723-8fef-424a-bc11-fe34a3922e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataFrame\n",
    "data1 = [(\"Alice\", 10), (\"Bob\", 20)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df1 = spark.createDataFrame(data1, columns)\n",
    "\n",
    "data2 = [(\"Alice\", \"F\"), (\"Bob\", \"M\")]\n",
    "columns = [\"name\", \"gender\"]\n",
    "df2 = spark.createDataFrame(data2, columns)\n",
    "\n",
    "# Присвоение псевдонимов DataFrame\n",
    "df1_alias = df1.alias(\"a\")\n",
    "df2_alias = df2.alias(\"b\")\n",
    "\n",
    "# Выполнение join с использованием псевдонимов\n",
    "result = df1_alias.join(\n",
    "    df2_alias, df1_alias[\"name\"] == df2_alias[\"name\"]\n",
    ").select(\"a.name\", \"a.age\", \"b.gender\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea98fc6-796a-45cb-9465-2603b033c7c5",
   "metadata": {},
   "source": [
    "### Столбцы - Присвоение псевдонимов "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1398635-6811-4adc-b955-e002a7769026",
   "metadata": {},
   "source": [
    "Присвоение псевдонимов столбцам позволяет переименовывать столбцы в результате выборки, что полезно для улучшения читаемости или для избежания конфликтов имен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff5ce9-d07d-489b-b729-c9e954b5f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataFrame\n",
    "data = [(\"Alice\", 10), (\"Bob\", 20)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Присвоение псевдонимов столбцам\n",
    "df_with_alias = df.select(df[\"name\"].alias(\"student_name\"), df[\"age\"].alias(\"student_age\"))\n",
    "df_with_alias.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9865837-2284-4157-8455-5fbbc3f37158",
   "metadata": {},
   "source": [
    "## when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbc459-4e75-4016-97d6-a3b89cdcf979",
   "metadata": {},
   "source": [
    "Функция when в PySpark используется для создания условных выражений в DataFrame. Она аналогична конструкции CASE WHEN в SQL и позволяет выполнять операции на основе определенных условий.\n",
    "\n",
    "Синтаксис"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df332d87-9a16-4996-a018-d3aca8eee4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "    when(condition, value).otherwise(default_value)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19df46b-7a29-4553-9b96-88757c17cff2",
   "metadata": {},
   "source": [
    "**Простое условие**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d2c3a-c8c3-4bcf-9de9-466a87352107",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Alice\", 25),\n",
    "    (\"Bob\", 22),\n",
    "    (\"Charlie\", 30),\n",
    "    (\"Diana\", 22),\n",
    "    (\"Eve\", 28),\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Добавляем новый столбец \"Age_25_or_Above\"\n",
    "df = df.withColumn(\"Age_25_or_Above\", when(col(\"Age\") >= 25, \"Yes\").otherwise(\"No\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae5a99-df01-4de3-8fd5-d4679128afcb",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+-------+---+--------------+\n",
    "|   Name|Age|Age_25_or_Above|\n",
    "+-------+---+--------------+\n",
    "|  Alice| 25|           Yes|\n",
    "|    Bob| 22|            No|\n",
    "|Charlie| 30|           Yes|\n",
    "|  Diana| 22|            No|\n",
    "|    Eve| 28|           Yes|\n",
    "+-------+---+--------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce5412-d2ca-4ca2-ada3-78c8ce693009",
   "metadata": {},
   "source": [
    "**Несколько условий**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6a4a3d-be44-4369-a3cd-d01d2e59f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"Age_Category\",\n",
    "    when(col(\"Age\") < 20, \"Teenager\")\n",
    "    .when((col(\"Age\") >= 20) & (col(\"Age\") < 30), \"Young Adult\")\n",
    "    .otherwise(\"Adult\")\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c84108-d2c3-4124-b669-e4d43ca55010",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+-------+---+--------------+-------------+\n",
    "|   Name|Age|Age_25_or_Above|Age_Category|\n",
    "+-------+---+--------------+-------------+\n",
    "|  Alice| 25|           Yes|        Adult|\n",
    "|    Bob| 22|            No|  Young Adult|\n",
    "|Charlie| 30|           Yes|        Adult|\n",
    "|  Diana| 22|            No|  Young Adult|\n",
    "|    Eve| 28|           Yes|  Young Adult|\n",
    "+-------+---+--------------+-------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab371b-ec64-4220-89fc-6bd7579910f9",
   "metadata": {},
   "source": [
    "Вы можете цепочкой использовать несколько условий `when` и завершить цепочку `otherwise`, чтобы указать значение по умолчанию, если ни одно из условий не выполнено."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12b01c-53d0-4da5-8ab3-e0e91b76fbfb",
   "metadata": {},
   "source": [
    "**Использование withColumn и when для обработки NULL значений**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849719cd-505f-4da2-b28d-1a761f5ec578",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Alice\", 25, \"Math\"),\n",
    "    (\"Bob\", None, \"Physics\"),\n",
    "    (\"Charlie\", 30, None),\n",
    "    (\"Diana\", 22, \"History\"),\n",
    "    (\"Eve\", None, \"Math\"),\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"Subject\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Заменяем NULL значения в столбце \"Age\" на 0\n",
    "df = df.withColumn(\"Age\", when(col(\"Age\").isNull(), 0).otherwise(col(\"Age\")))\n",
    "\n",
    "# Заменяем NULL значения в столбце \"Subject\" на \"Unknown\"\n",
    "df = df.withColumn(\"Subject\", when(col(\"Subject\").isNull(), \"Unknown\").otherwise(col(\"Subject\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d04a94-3837-45d6-b052-cd456d145334",
   "metadata": {},
   "source": [
    "```markdown\n",
    "+-------+---+-------+\n",
    "|   Name|Age|Subject|\n",
    "+-------+---+-------+\n",
    "|  Alice| 25|   Math|\n",
    "|    Bob|  0|Physics|\n",
    "|Charlie| 30|Unknown|\n",
    "|  Diana| 22|History|\n",
    "|    Eve|  0|   Math|\n",
    "+-------+---+-------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5188583f-15ea-4c16-a27f-43458c5a7c0a",
   "metadata": {},
   "source": [
    "## .orderBy и .sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715368e-11fc-41da-b457-9580292c03c1",
   "metadata": {},
   "source": [
    "Методы .orderBy и .sort в PySpark выполняют одинаковую функцию: они сортируют DataFrame по заданным столбцам. Эти методы являются синонимами и предоставляют одинаковые результаты. Они могут принимать как строковые имена столбцов, так и объекты Column. Оба метода также поддерживают сортировку в порядке возрастания и убывания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb24a74-fc3e-479c-9881-8f128c850280",
   "metadata": {},
   "source": [
    "Методы .orderBy и .sort предоставляют одинаковую функциональность, но существуют оба для удобства и читаемости кода. Разные пользователи могут предпочитать разные имена методов, но результат их применения будет одинаковым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f2486-e27a-4473-91ba-feb4f58c3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример данных\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "# Создание DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0764dc-d193-4d5b-906b-c152f68496c7",
   "metadata": {},
   "source": [
    "### orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644886d-acc5-4b4e-98ce-1c84343a1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сортировка по столбцу Age в порядке возрастания\n",
    "df_ordered = df.orderBy(\"Age\")\n",
    "df_ordered.show()\n",
    "\n",
    "# Сортировка по столбцу Age в порядке убывания\n",
    "df_ordered_desc = df.orderBy(df[\"Age\"].desc())\n",
    "df_ordered_desc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d05665-58d6-448b-85ad-b487f073b53d",
   "metadata": {},
   "source": [
    "### sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59556e08-2ec5-491c-a127-b9b22beafba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сортировка по столбцу Age в порядке возрастания\n",
    "df_sorted = df.sort(\"Age\")\n",
    "df_sorted.show()\n",
    "\n",
    "# Сортировка по столбцу Age в порядке убывания\n",
    "df_sorted_desc = df.sort(df[\"Age\"].desc())\n",
    "df_sorted_desc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49483d-ff4e-40a2-83d9-eb336764bc85",
   "metadata": {},
   "source": [
    "### Сортировка по нескольким столбцам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a7f36-55a4-4e26-92bb-bf34d2c42bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сортировка по столбцу Age в порядке возрастания, затем по столбцу Name в порядке убывания\n",
    "df_ordered_multi = df.orderBy(df[\"Age\"].asc(), df[\"Name\"].desc())\n",
    "df_ordered_multi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1a116-d2ee-4a1e-a511-6c9ad39735a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сортировка по столбцу Age в порядке возрастания, затем по столбцу Name в порядке убывания\n",
    "df_sorted_multi = df.sort(df[\"Age\"].asc(), df[\"Name\"].desc())\n",
    "df_sorted_multi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fcf58-a866-4c2a-accb-81b6ce61ce96",
   "metadata": {},
   "source": [
    "### Пример с DataFrame со структурными данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d15439-3541-47de-8cee-709efc6f7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Alice\", (\"New York\", 10001)),\n",
    "    (\"Bob\", (\"San Francisco\", 94105)),\n",
    "    (\"Cathy\", (\"Los Angeles\", 90001))\n",
    "]\n",
    "\n",
    "schema = [\"Name\", \"Address\"]\n",
    "address_schema = StructType([StructField(\"City\", StringType(), True), StructField(\"ZIP\", IntegerType(), True)])\n",
    "df = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0321f-5e10-4188-a39f-6937f66740dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сортировка по столбцу ZIP в порядке возрастания\n",
    "df_sorted_by_zip = df.orderBy(col(\"Address.ZIP\").asc())\n",
    "df_sorted_by_zip.show(truncate=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abfef28-b840-40ba-b64a-e3f4aa00c126",
   "metadata": {},
   "source": [
    "### asc_nulls_first, asc_nulls_last, desc_nulls_first, и desc_nulls_last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d97994-7994-43aa-ac38-a6d0aa6714dd",
   "metadata": {},
   "source": [
    "Методы asc_nulls_first, asc_nulls_last, desc_nulls_first, и desc_nulls_last используются для указания порядка сортировки с учетом null значений:\n",
    "\n",
    "- asc_nulls_first: сортирует в порядке возрастания, помещая null значения в начало.\n",
    "- asc_nulls_last: сортирует в порядке возрастания, помещая null значения в конец.\n",
    "- desc_nulls_first: сортирует в порядке убывания, помещая null значения в начало.\n",
    "- desc_nulls_last: сортирует в порядке убывания, помещая null значения в конец."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58396c9b-07b2-45ab-bc4b-5f1a9b879b46",
   "metadata": {},
   "source": [
    "## Оконные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c58ce-ced6-4392-bf5d-c68d37d08f41",
   "metadata": {},
   "source": [
    "В `Apache Spark`, объект `Window` представляет собой спецификацию окна, которая определяет какие строки из DataFrame должны быть использованы для вычисления операций, таких как агрегации, ранжирования, или вычисления оконных функций. Оконные функции работают с группой строк (окно), которые определяются с помощью спецификации окна.\n",
    "\n",
    "Основные компоненты спецификации окна включают:\n",
    "- **PartitionBy**: определяет как разделить строки DataFrame на группы (партиции). Функции окна будут вычисляться независимо для каждой партиции.  \n",
    "- **OrderBy**: определяет порядок сортировки строк внутри каждой партиции. Это необходимо для определения последовательности строк в окне.  \n",
    "- **RowsBetween**, **RangeBetween**: определяют, какие строки в пределах каждой партиции будут включены в окно, используемое для вычислений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07bc1c8-f8bf-4bb3-aa5c-cafa8902b297",
   "metadata": {},
   "source": [
    "### Вычисление суммы по группам с использованием окна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e29c14-3bae-49e2-909f-3a8192b86c37",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Пример данных\n",
    "data = [(\"Alice\", 1), (\"Bob\", 3), (\"Alice\", 2), (\"Bob\", 5), (\"Alice\", 4)]\n",
    "\n",
    "# Создание DataFrame\n",
    "df = spark.createDataFrame(data, [\"name\", \"score\"])\n",
    "\n",
    "# Определение спецификации окна\n",
    "window_spec = Window.partitionBy(\"name\").orderBy(\"score\")\n",
    "\n",
    "# Пример использования оконной функции (например, вычисление суммы score по каждому имени, упорядоченному по score)\n",
    "df.withColumn(\"sum_score\", f.sum(\"score\").over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71599b-720a-4358-9281-5e42e141e204",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+-----+-----+---------+\n",
    "| name|score|sum_score|\n",
    "+-----+-----+---------+\n",
    "|Alice|    1|        1|\n",
    "|Alice|    2|        3|\n",
    "|Alice|    4|        7|\n",
    "|  Bob|    3|        3|\n",
    "|  Bob|    5|        8|\n",
    "+-----+-----+---------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5841c-a928-47ac-b1bc-0bf5e4637734",
   "metadata": {},
   "source": [
    "В этом примере `Window.partitionBy(\"name\").orderBy(\"score\")` создает спецификацию окна, которая разделяет строки DataFrame на партиции по столбцу \"name\" и упорядочивает строки в каждой партиции по столбцу \"score\". Функция `f.sum(\"score\").over(window_spec)` применяет оконную функцию sum к столбцу \"score\" в рамках каждого окна, определенного спецификацией window_spec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f76c2-a93f-49d5-8241-3241c258d0ee",
   "metadata": {},
   "source": [
    "Аналог в SQL\n",
    "```SQL\n",
    "SELECT\n",
    "    name,\n",
    "    score,\n",
    "    SUM(score) OVER (PARTITION BY name ORDER BY score) AS sum_score\n",
    "FROM\n",
    "    table_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb15c5-ee6b-49ef-94c9-3f78c4c14cbf",
   "metadata": {},
   "source": [
    "### Вычисление ранга для каждого пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00496e4f-f55f-4bb0-8f69-cf747521e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример данных\n",
    "data = [(\"Alice\", 100), (\"Bob\", 80), (\"Alice\", 120), (\"Bob\", 90), (\"Alice\", 110)]\n",
    "\n",
    "# Создание DataFrame\n",
    "df = spark.createDataFrame(data, [\"name\", \"score\"])\n",
    "\n",
    "# Определение спецификации окна для упорядочивания по убыванию score и разбиения по name\n",
    "window_spec = Window.partitionBy(\"name\").orderBy(df[\"score\"].desc())\n",
    "\n",
    "# Пример использования оконной функции для вычисления ранга (rank) в пределах каждого пользователя\n",
    "df.withColumn(\"rank\", f.rank().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd8d66-0c23-4f9b-af71-a148b4e7a965",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+-----+-----+----+\n",
    "| name|score|rank|\n",
    "+-----+-----+----+\n",
    "|Alice|  120|   1|\n",
    "|Alice|  110|   2|\n",
    "|Alice|  100|   3|\n",
    "|  Bob|   90|   1|\n",
    "|  Bob|   80|   2|\n",
    "+-----+-----+----+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf17af1-e536-4cca-bb0a-728ea2bda7a4",
   "metadata": {},
   "source": [
    "Аналог в SQL\n",
    "```SQL\n",
    "SELECT\n",
    "    name,\n",
    "    score,\n",
    "    RANK() OVER (PARTITION BY name ORDER BY score DESC) AS rank\n",
    "FROM\n",
    "    table_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4c367-1649-4ccc-9727-fd49e64a368c",
   "metadata": {},
   "source": [
    "### Порядковый номер для результата отбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d414112-c720-48e4-9004-bcd496e19b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `ctp_geo_notnull_ct_null` and `ctd_geo_notnull_ct_null` are the DataFrames\n",
    "\n",
    "# Union and assign sequential numbers in one step\n",
    "combined_df = ctp_geo_notnull_ct_null.select(\n",
    "    f.col(ct.pickup_ct).alias(\"census_tract\"),\n",
    "    f.col(ct.pickup_cl).alias(\"latitude\"),\n",
    "    f.col(ct.pickup_clon).alias(\"longitude\")\n",
    ").union(\n",
    "    ctd_geo_notnull_ct_null.select(\n",
    "        f.col(ct.dropoff_ct).alias(\"census_tract\"),\n",
    "        f.col(ct.dropoff_cl).alias(\"latitude\"),\n",
    "        f.col(ct.dropoff_clon).alias(\"longitude\")\n",
    "    )\n",
    ").distinct().withColumn(\"census_tract\", f.row_number().over(Window.orderBy(f.lit(1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6c24c-e7a3-48b9-8591-aa3349051f2a",
   "metadata": {},
   "source": [
    "## Типы данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0692c32-01c4-409f-aaa0-bb5802e8737f",
   "metadata": {},
   "source": [
    "В PySpark используются следующие основные типы данных:\n",
    "\n",
    "**Numeric Types (Числовые типы):**\n",
    "- `ByteType`: Один байт, целое число от -128 до 127.\n",
    "- `ShortType`: Два байта, целое число от -32,768 до 32,767.\n",
    "- `IntegerType`: Четыре байта, целое число от -2,147,483,648 до 2,147,483,647.\n",
    "- `LongType`: Восемь байт, целое число от -9,223,372,036,854,775,808 до 9,223,372,036,854,775,807.\n",
    "- `FloatType`: Четыре байта, число с плавающей точкой одинарной точности.\n",
    "- `DoubleType`: Восемь байт, число с плавающей точкой двойной точности.\n",
    "- `DecimalType`: Десятичное число произвольной точности.\n",
    "\n",
    "**String Types (Строковые типы):**\n",
    "- `StringType`: Текстовая строка.\n",
    "\n",
    "**Binary Types (Бинарные типы):**\n",
    "- `BinaryType`: Массив байтов.\n",
    "\n",
    "**Boolean Type (Булев тип):**\n",
    "- `BooleanType`: Логическое значение (true или false).\n",
    "\n",
    "**Datetime Types (Типы даты и времени):**\n",
    "- `DateType`: Дата без времени.\n",
    "- `TimestampType`: Дата и время с точностью до микросекунд.\n",
    "\n",
    "**Complex Types (Комплексные типы):**\n",
    "- `ArrayType`: Массив значений одного типа.\n",
    "- `MapType`: Набор пар \"ключ-значение\", где ключи и значения могут быть разных типов.\n",
    "- `StructType`: Структурированный тип, аналогичный строкам в таблице, содержащий несколько полей.\n",
    "\n",
    "**Null Type (Тип Null):**\n",
    "- `NullType`: Тип для значений NULL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a725ed1-e52d-455a-8cd1-ea414c6e9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# как получить тип конкретного поля.\n",
    "df.schema[\"byte_col\"].dataType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486cfdf5-1f4d-48d4-bfce-3404493346ff",
   "metadata": {},
   "source": [
    "### Числовые типы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6514401-d706-4419-a86f-b128a5029cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ByteType, ShortType, IntegerType,\n",
    "                              LongType, FloatType, DoubleType,\n",
    "                              DecimalType, StructType, StructField\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"byte_col\", ByteType(), True),\n",
    "    StructField(\"short_col\", ShortType(), True),\n",
    "    StructField(\"int_col\", IntegerType(), True),\n",
    "    StructField(\"long_col\", LongType(), True),\n",
    "    StructField(\"float_col\", FloatType(), True),\n",
    "    StructField(\"double_col\", DoubleType(), True),\n",
    "    StructField(\"decimal_col\", DecimalType(10, 2), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, 2, 3, 4, 5.0, 6.0, 7.0)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dcf95d-8cbe-46e2-8616-767b7ea5cd97",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+--------+---------+-------+-------+---------+----------+-----------+\n",
    "|byte_col|short_col|int_col|long_col|float_col|double_col|decimal_col|\n",
    "+--------+---------+-------+-------+---------+----------+-----------+\n",
    "|       1|        2|      3|      4|      5.0|       6.0|        7.0|\n",
    "+--------+---------+-------+-------+---------+----------+-----------+\n",
    "```как получить тип конкретного поля."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f48e2-0eaf-4346-a2b1-1f5a8641334a",
   "metadata": {},
   "source": [
    "### Типы даты и времени"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccdf64-0976-4c85-89d5-c1329fad7a19",
   "metadata": {},
   "source": [
    "#### Пример полей даты-времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6295a-e19b-475c-9763-e84fd291fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, TimestampType, StructType, StructField\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date_col\", DateType(), True),\n",
    "    StructField(\"timestamp_col\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"2024-07-07\", \"2024-07-07 12:34:56\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fafd22-0397-4ca9-83f3-094e8cb7698c",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+----------+-------------------+\n",
    "|  date_col|      timestamp_col|\n",
    "+----------+-------------------+\n",
    "|2024-07-07|2024-07-07 12:34:56|\n",
    "+----------+-------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6081b36-a872-4eb7-8f1a-7eb965bd8f5e",
   "metadata": {},
   "source": [
    "#### Основные функции для дат"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be3bdb-94fb-4e3c-bbc4-eba11e1ac272",
   "metadata": {},
   "source": [
    "Вот несколько основных функций PySpark для работы с датами и временем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c28c5-47f6-4cf9-a136-fbad02ffe7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "# 1. Добавление или вычитание дней\n",
    "df = df.withColumn(\"date_add\", f.date_add(\"date_col\", 10))\n",
    "df = df.withColumn(\"date_sub\", f.date_sub(\"date_col\", 10))\n",
    "\n",
    "# 2. Вычисление разницы между датами\n",
    "df = df.withColumn(\"datediff\", f.datediff(f.to_date(f.lit(\"2024-08-01\")), \"date_col\"))\n",
    "\n",
    "# 3. День недели (1 - понедельник, 7 - воскресенье)\n",
    "df = df.withColumn(\"dayofweek\", f.dayofweek(\"date_col\"))\n",
    "\n",
    "# 4. Извлечение года, месяца, дня и т.д.\n",
    "df = df.withColumn(\"year\", f.year(\"date_col\"))\n",
    "df = df.withColumn(\"month\", f.month(\"date_col\"))\n",
    "df = df.withColumn(\"day\", f.dayofmonth(\"date_col\"))\n",
    "\n",
    "# 5. Форматирование даты в строку\n",
    "df = df.withColumn(\"date_format\", f.date_format(\"date_col\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# 6. Преобразование строки в дату (если формат известен)\n",
    "df = df.withColumn(\"to_date\", f.to_date(\"date_string_col\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# 7. Получение текущего времени\n",
    "df = df.withColumn(\"current_timestamp\", f.current_timestamp())\n",
    "\n",
    "# 8. Перевод времени из одной временной зоны в другую\n",
    "df = df.withColumn(\"from_utc_timestamp\", f.from_utc_timestamp(\"timestamp_col\", \"GMT\"))\n",
    "\n",
    "# 9. Проверка на високосный год\n",
    "df = df.withColumn(\"is_leap_year\", f.year(\"date_col\").cast(\"int\").rdd.map(lambda x: x[0]).map(lambda x: (x % 4 == 0 and x % 100 != 0) or (x % 400 == 0)))\n",
    "\n",
    "# 10. Поиск первой даты после текущей\n",
    "df = df.withColumn(\"next_day\", f.next_day(f.current_date(), \"monday\"))\n",
    "\n",
    "# 11. Подсчет количества дней между двумя датами\n",
    "df = df.withColumn(\"datediff\", f.datediff(f.to_date(f.lit(\"2024-08-01\")), \"date_col\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ae131-9627-4463-bdd2-08374e6fb072",
   "metadata": {},
   "source": [
    "#### Выделение части времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23afee9-44fa-48c3-b4d0-5594abd909f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, TimestampType, StructType, StructField\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date_col\", DateType(), True),\n",
    "    StructField(\"timestamp_col\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"2024-07-07\", \"2024-07-07 12:34:56\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df = df.withColumn(\"year\", f.year(\"timestamp_col\"))\n",
    "df = df.withColumn(\"month\", f.month(\"timestamp_col\"))\n",
    "df = df.withColumn(\"day\", f.dayofmonth(\"timestamp_col\"))\n",
    "df = df.withColumn(\"hour\", f.hour(\"timestamp_col\"))\n",
    "df = df.withColumn(\"minute\", f.minute(\"timestamp_col\"))\n",
    "df = df.withColumn(\"second\", f.second(\"timestamp_col\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aefa615-c161-4e31-b97c-26b558400ee1",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+----------+-------------------+----+-----+---+----+------+------+\n",
    "|  date_col|      timestamp_col|year|month|day|hour|minute|second|\n",
    "+----------+-------------------+----+-----+---+----+------+------+\n",
    "|2024-07-07|2024-07-07 12:34:56|2024|    7|  7|  12|    34|    56|\n",
    "+----------+-------------------+----+-----+---+----+------+------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2d836-dc7e-498c-8b91-0df70d7f7e8a",
   "metadata": {},
   "source": [
    "#### Сдвиг add sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4e70e-ca42-4434-af1a-48a969c029b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, TimestampType, StructType, StructField\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date_col\", DateType(), True),\n",
    "    StructField(\"timestamp_col\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"2024-07-07\", \"2024-07-07 12:34:56\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df = df.withColumn(\"date_add\", f.date_add(\"timestamp_col\", 10))\n",
    "df = df.withColumn(\"date_sub\", f.date_sub(\"timestamp_col\", 10))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c233a0a-86f0-4016-9801-24c26a6fdae3",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+----------+-------------------+-------------------+-------------------+\n",
    "|  date_col|      timestamp_col|           date_add|           date_sub|\n",
    "+----------+-------------------+-------------------+-------------------+\n",
    "|2024-07-07|2024-07-07 12:34:56|2024-07-17 12:34:56|2024-06-27 12:34:56|\n",
    "+----------+-------------------+-------------------+-------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10962922-f14b-4690-8cf2-3718f7c4c2cf",
   "metadata": {},
   "source": [
    "#### Разница datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a250b-0e53-4561-afc3-1b19643505a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, TimestampType, StructType, StructField\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date_col\", DateType(), True),\n",
    "    StructField(\"timestamp_col\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"2024-07-07\", \"2024-07-07 12:34:56\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df = df.withColumn(\"datediff_current_date\", f.datediff(f.current_date(), \"date_col\"))\n",
    "df = df.withColumn(\"datediff_date_col\", f.datediff(f.to_date(f.lit(\"2024-08-01\")), \"date_col\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebbf79b-d8a4-4702-98a1-7c479d491dba",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+----------+-------------------+---------------------+------------------+\n",
    "|  date_col|      timestamp_col|datediff_current_date|datediff_date_col |\n",
    "+----------+-------------------+---------------------+------------------+\n",
    "|2024-07-07|2024-07-07 12:34:56|                  361|                25|\n",
    "+----------+-------------------+---------------------+------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72061d2-644c-44b1-9228-5b6003d176c8",
   "metadata": {},
   "source": [
    "#### Символы формата"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648526c6-ea6d-4c6c-a4bc-64907af41104",
   "metadata": {},
   "source": [
    "В PySpark можно использовать следующие символы формата для аргумента format в функции f.to_timestamp:\n",
    "\n",
    "**Год:**\n",
    "- **y или yy:** 2-значный год (например, 21 для 2021).\n",
    "- **yyyy:** 4-значный год (например, 2021).\n",
    "\n",
    "**Месяц:**\n",
    "- **M или MM:** Месяц как число без ведущего нуля (1-12).\n",
    "- **MMM или MMMM:** Название месяца (янв, январь и т.д.).\n",
    "\n",
    "**День:**\n",
    "- **d или dd:** День месяца как число без ведущего нуля (1-31).\n",
    "\n",
    "**Час:**\n",
    "- **H или HH:** Часы в 24-часовом формате без ведущего нуля (0-23).\n",
    "- **h или hh:** Часы в 12-часовом формате с ведущим нулем (01-12).\n",
    "\n",
    "**Минуты:**\n",
    "- **m или mm:** Минуты без ведущего нуля (0-59).\n",
    "\n",
    "**Секунды:**\n",
    "- **s или ss:** Секунды без ведущего нуля (0-59).\n",
    "\n",
    "**Фракции секунд:**\n",
    "- **S или SSS:** Фракции секунды.\n",
    "\n",
    "**AM/PM:**\n",
    "- **a:** AM или PM.\n",
    "\n",
    "**Примеры:**\n",
    "\n",
    "- \"**yyyy-MM-dd HH:mm:ss:** Дата и время в формате год-месяц-день час:минута:секунда.\"\n",
    "- \"**yyyy-MM-dd:** Только дата в формате год-месяц-день.\"\n",
    "- \"**MM/dd/yyyy:** Дата в формате месяц/день/год.\"\n",
    "- \"**MMM dd, yyyy HH:mm:ss:** Дата и время с использованием аббревиатуры месяца, например, янв 01, 2021 12:34:56.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b037f-6a13-42be-bf4b-f4c6f9fe46c1",
   "metadata": {},
   "source": [
    "#### Форматирование даты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85e494-5430-4896-9c69-68ef70254ccf",
   "metadata": {},
   "source": [
    "Для форматирования дат в PySpark основными функциями являются `f.date_format` для преобразования даты в строку и `f.to_timestamp` для преобразования строки в формат даты/времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58ff6c-e22f-46b2-bc0b-09a69c2bcc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, TimestampType, StructType, StructField\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date_col\", DateType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"2024-07-07\"),\n",
    "    (\"2024-08-15\"),\n",
    "    (\"2023-05-10\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Форматирование даты в строку\n",
    "df = df.withColumn(\"date_format\", f.date_format(\"date_col\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Преобразование строки в Timestamp\n",
    "df = df.withColumn(\"to_timestamp\", f.to_timestamp(\"date_col\"))\n",
    "\n",
    "# Другие возможные сценарии форматирования даты\n",
    "df = df.withColumn(\"month_day\", f.date_format(\"date_col\", \"MM-dd\"))\n",
    "df = df.withColumn(\"week_day\", f.date_format(\"date_col\", \"E\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb3b72-c721-4310-b22a-6c35a5f8dfbe",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+----------+------------+-------------------+----------+--------+\n",
    "|  date_col| date_format|      to_timestamp| month_day|week_day|\n",
    "+----------+------------+-------------------+----------+--------+\n",
    "|2024-07-07|  2024-07-07|2024-07-07 00:00:00|     07-07|     Sat|\n",
    "|2024-08-15|  2024-08-15|2024-08-15 00:00:00|     08-15|     Thu|\n",
    "|2023-05-10|  2023-05-10|2023-05-10 00:00:00|     05-10|     Wed|\n",
    "+----------+------------+-------------------+----------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d159e-17ab-46fe-b3ee-4087f4b953c5",
   "metadata": {},
   "source": [
    "##### f.to_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7161c3da-e025-4c33-8b4c-1710be73a5a3",
   "metadata": {},
   "source": [
    "Функция f.to_timestamp(col, format=None) в PySpark используется для преобразования столбца или выражения колонки в тип данных TimestampType.  \n",
    "\n",
    "Если формат не указан, PySpark будет использовать формат по умолчанию, что может привести к некорректным результатам, если формат данных отличается от ожидаемого.  \n",
    "При использовании функции to_timestamp необходимо учитывать, что строки, которые не могут быть интерпретированы как даты в указанном формате, будут приводить к значению NULL в соответствующем столбце.\n",
    "\n",
    "**Аргументы:**\n",
    "- col: Колонка или выражение колонки, которое необходимо преобразовать в тип TimestampType.\n",
    "- format (опционально): Строка, определяющая формат входной строки. Если не указан, PySpark попытается автоматически определить формат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47919b-d54b-4549-ab59-ec42c6b9e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем DataFrame с колонкой date_col типа StringType\n",
    "data = [(\"2024-07-07\",), (\"2024-08-15\",), (\"2023-05-10\",)]\n",
    "df = spark.createDataFrame(data, [\"date_col\"])\n",
    "\n",
    "# Преобразуем колонку date_col в тип TimestampType\n",
    "df = df.withColumn(\"to_timestamp\", f.to_timestamp(\"date_col\"))\n",
    "df = df.withColumn(\"to_timestamp_custom\", f.to_timestamp(\"date_col\", \"yyyy-MM-dd\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb108b-81eb-4fe9-b12e-e5a20fe41858",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+----------+-------------------+-------------------+\n",
    "|  date_col|      to_timestamp |to_timestamp_custom|\n",
    "+----------+-------------------+-------------------+\n",
    "|2024-07-07|2024-07-07 00:00:00|2024-07-07 00:00:00|\n",
    "|2024-08-15|2024-08-15 00:00:00|2024-08-15 00:00:00|\n",
    "|2023-05-10|2023-05-10 00:00:00|2023-05-10 00:00:00|\n",
    "+----------+-------------------+-------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d609b-74c6-44ba-9c8e-87087fd5ff3f",
   "metadata": {},
   "source": [
    "##### f.date_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869f7bb-2023-44dc-a32d-a9b6edb4eb97",
   "metadata": {},
   "source": [
    "Функция `f.date_format(col, format)` в PySpark используется для форматирования столбца даты или времени в строку с заданным форматом.  \n",
    "\n",
    "Функция `date_format` позволяет указать любой формат, совместимый с `Java SimpleDateFormat`, что обеспечивает гибкость в форматировании дат и времени в PySpark.\n",
    "При использовании формата следует учитывать, что некорректный формат может привести к ошибкам или непредсказуемым результатам.\n",
    "\n",
    "**Аргументы:**\n",
    "\n",
    "- col: Колонка или выражение колонки, содержащее дату или время, которое необходимо отформатировать.\n",
    "- format: Строка, определяющая формат вывода. Формат должен соответствовать спецификациям Java SimpleDateFormat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e97289-4957-4517-91c3-2808574c14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем DataFrame с колонкой date_col типа DateType\n",
    "data = [(\"2024-07-07\",), (\"2024-08-15\",), (\"2023-05-10\",)]\n",
    "df = spark.createDataFrame(data, [\"date_col\"]).withColumn(\"date_col\", f.col(\"date_col\").cast(DateType()))\n",
    "\n",
    "# Преобразуем колонку date_col в строку с заданным форматом\n",
    "df = df.withColumn(\"formatted_date\", f.date_format(\"date_col\", \"yyyy-MM-dd\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333764a4-3bd9-43b3-b49e-06fe186c8c84",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+----------+---------------+\n",
    "|  date_col| formatted_date|\n",
    "+----------+---------------+\n",
    "|2024-07-07|     2024-07-07|\n",
    "|2024-08-15|     2024-08-15|\n",
    "|2023-05-10|     2023-05-10|\n",
    "+----------+---------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fc7e4-d63e-451e-8d99-3905ff9a38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"formatted_date\", f.date_format(\"date_col\", \"MM/dd/yyyy\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7095b72-5e7a-437e-af58-dd3ed23c169c",
   "metadata": {},
   "source": [
    "```yaml\n",
    "+----------+---------------+\n",
    "|  date_col| formatted_date|\n",
    "+----------+---------------+\n",
    "|2024-07-07|     07/07/2024|\n",
    "|2024-08-15|     08/15/2024|\n",
    "|2023-05-10|     05/10/2023|\n",
    "+----------+---------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f18c26-ebd4-4e0c-9af5-63f0caadc65b",
   "metadata": {},
   "source": [
    "## Сохранение в файл"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7f84c-a0ba-472e-8392-e7caf98d3048",
   "metadata": {},
   "source": [
    "### Один или несколько файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf526b-1838-46b7-85da-98834588c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить DataFrame taxi в формате CSV в директории /work/data\n",
    "taxi.write.csv(\"/work/data/taxi.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b03bf2-3abc-4192-93eb-37e6f444dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# перезапись\n",
    "taxi.write.csv(\"/work/data/taxi.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535b3da-880b-465b-be99-0b0920a39487",
   "metadata": {},
   "source": [
    "Когда вы используете метод `write.csv()` в Spark, он создает несколько файлов в формате CSV, так как каждый раздел (partition) DataFrame сохраняется в отдельный файл.  \n",
    "Используйте `coalesce(1)` для сохранения в единый файл CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fb238-1975-495c-a7f5-99894c65fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение DataFrame в единый файл CSV\n",
    "taxi.coalesce(1).write.csv(\"/work/data/taxi.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dffff4-2cf3-4c93-a355-4879d7a7034b",
   "metadata": {},
   "source": [
    "Есть несколько важных преимуществ и различий между сохранением DataFrame в CSV с сохранением каждого раздела в отдельный файл и с использованием метода `coalesce(1)` для сохранения в один файл:\n",
    "\n",
    "**Параллелизм и производительность:**\n",
    "- Сохранение каждого раздела в отдельный файл CSV в Spark делает процесс сохранения параллельным. Каждый раздел обрабатывается и сохраняется независимо, что может улучшить производительность на больших объемах данных.\n",
    "- Использование `coalesce(1)` приводит к тому, что все данные будут перемещены и записаны на одном узле, что может быть менее эффективно для больших наборов данных из-за ограничений ресурсов одного узла.\n",
    "\n",
    "**Управление файлами:**\n",
    "- При сохранении каждого раздела в отдельный файл у вас будет много файлов, что может быть неудобно для управления и обработки в дальнейшем.\n",
    "- С использованием `coalesce(1)` вы получите только один файл, что облегчает управление результатами и последующую обработку.\n",
    "\n",
    "**Использование ресурсов:**\n",
    "- Сохранение каждого раздела в отдельный файл может быть более эффективным с точки зрения распределения нагрузки на узлы кластера Spark и использования ресурсов.\n",
    "- Однако использование `coalesce(1)` может быть полезным в случаях, когда вам действительно нужен единый файл и вы готовы потерпеть некоторое ухудшение производительности из-за работы на одном узле."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5719d251-2a91-4053-8f24-e4ce378fa398",
   "metadata": {},
   "source": [
    "Cоздается папка `taxis.csv` со следующими файлами:  \n",
    "`._SUCCESS.crc`     1кб  \n",
    "Это скрытый файл, который указывает на успешное завершение процесса сохранения данных. Он содержит информацию о том, что операция сохранения была выполнена без ошибок.  \n",
    "\n",
    "`.part-00000-45c80058-7aeb-4e94-82a8-570964c6aceb-c000.csv.crc`       20.3мб  \n",
    "Этот файл .crc является контрольной суммой (CRC) для соответствующего CSV файла part-00000-45c80058-7aeb-4e94-82a8-570964c6aceb-c000.csv. Он используется для проверки целостности данных в CSV файле.\n",
    "\n",
    "`_SUCCESS`        0кб  \n",
    "Этот файл пустой и также служит индикатором успешного завершения операции сохранения. Он может быть создан в случае использования определенных форматов записи в Spark.\n",
    "\n",
    "`part-00000-45c80058-7aeb-4e94-82a8-570964c6aceb-c000.csv`        2.5гб  \n",
    "Это основной CSV файл, который содержит данные вашего DataFrame. В данном случае, это единственный файл из-за использования `coalesce(1)`, который объединяет все разделы DataFrame в один файл.\n",
    "\n",
    "Дополнительные файлы (.crc и _SUCCESS) важны для контроля целостности данных и индикации успешного выполнения операции. Они могут быть полезны при автоматизации и мониторинге процессов обработки данных в Spark.  \n",
    "\n",
    "Можно перенести и использовать только основной CSV файл `part-00000-45c80058-7aeb-4e94-82a8-570964c6aceb-c000.csv`, который содержит фактические данные DataFrame. Остальные файлы, такие как `.crc` файлы и `_SUCCESS`, являются вспомогательными и могут быть удалены или не использованы при переносе данных на другой диск.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a51d9a-ac33-41d6-bca5-5fdb1b8b8ad8",
   "metadata": {},
   "source": [
    "### Объединение двух больших DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435ad31e-a924-43ed-b6c7-99e59fcd27f4",
   "metadata": {},
   "source": [
    "Объединение двух больших DataFrame (taxi_22 и taxi_23), созданных из двух разных файлов, и сохранение объединенного DataFrame в один файл, а затем загрузка его обратно, может иметь несколько преимуществ и недостатков, которые стоит учитывать:\n",
    "**Преимущества сохранения и загрузки объединенного DataFrame:**  \n",
    "\n",
    "- Уменьшение числа операций чтения/записи: Если вы часто объединяете эти два DataFrame или используете их в нескольких операциях, сохранение объединенного DataFrame в файл может уменьшить количество операций чтения и объединения, что может ускорить последующие операции.\n",
    "\n",
    "- Улучшение параллелизма: При загрузке данных из одного файла DataFrame может быть загружен параллельно, что может улучшить общую производительность, особенно на распределенных кластерах.\n",
    "\n",
    "**Недостатки сохранения и загрузки объединенного DataFrame:**  \n",
    "\n",
    "- Дополнительное время на сохранение и загрузку: Сохранение и загрузка данных занимают время и ресурсы. В зависимости от размера данных и доступных ресурсов это может быть значительным временем.\n",
    "\n",
    "- Размер файла: Объединенный файл может быть очень большим, особенно если данные изначально были большими. Это может повлиять на время сохранения и загрузки, а также на общую производительность при работе с файлом.\n",
    "\n",
    "**Альтернативные подходы:** \n",
    "\n",
    "- Оптимизация операций в памяти: PySpark предоставляет мощные инструменты для обработки данных в памяти. Иногда лучше оставаться в памяти и оптимизировать выполнение операций без необходимости сохранения и загрузки на диск.\n",
    "\n",
    "- Использование форматов хранения данных: Выбор эффективного формата хранения данных, такого как Parquet или ORC, может значительно улучшить производительность операций чтения и записи.\n",
    "\n",
    "**Решение:**\n",
    "\n",
    "- Если вы часто используете объединенные данные и хотите снизить время выполнения операций в будущем, сохранение в файл может быть полезным.\n",
    "- Если операции с данными выполняются редко или если время сохранения и загрузки в файл критично для общей производительности, может быть лучше остаться в памяти и оптимизировать операции в этом контексте.\n",
    "\n",
    "Важно провести тестирование производительности для оценки влияния каждого из подходов на ваше конкретное приложение и окружение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef55ded5-a881-4f0e-bca2-8a70a87efb2b",
   "metadata": {},
   "source": [
    "## Репартиции"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a807e-930e-44a6-b0ae-9f93ee61c71a",
   "metadata": {},
   "source": [
    "### Варианты репартиций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29aa1e3-a0c6-446c-bf8f-94b9bc6ed183",
   "metadata": {},
   "source": [
    "Объединяем два дф писпарк taxi_22 taxi_23.  \n",
    "Репартиция может помочь ускорить выполнение операции объединения, особенно если исходные DataFrame имеют различное количество партиций или если они не были репартиционированы оптимально.  \n",
    "Если вы знаете, что исходные DataFrame уже репартиционированы, но с разным количеством партиций, можно выполнить репартицию до объединения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9c1e4-f4a8-48dc-b72b-fc61bc398d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_22 = taxi_22.repartition(taxi_23.rdd.getNumPartitions())\n",
    "combined_df = taxi_22.union(taxi_23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1264c9-789a-45f2-89dd-bc8857d463af",
   "metadata": {},
   "source": [
    "Это уравновесит число партиций в обоих DataFrame перед объединением."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ab261d-251d-41d7-8d1b-e25f99318247",
   "metadata": {},
   "source": [
    "Репартиция основанная на категориях в столбцах может быть полезной, если вы хотите оптимизировать распределение данных в DataFrame в соответствии с определенными категориями. Это может ускорить выполнение операций, таких как соединения или агрегации, которые зависят от значений этих категорий.\n",
    "\n",
    "В PySpark для выполнения репартиции на основе значений в столбце вы можете использовать метод partitionBy вместе с функцией partitionBy или методом repartition.\n",
    "\n",
    "**Использование partitionBy с DataFrameWriter:**  \n",
    "Если вы пишете DataFrame в файлы (например, в паркет или в формате CSV), вы можете использовать partitionBy для автоматической репартиции данных по значению столбца:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03764839-8fc0-41ed-ac5b-e274dc62f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"category_column\").parquet(\"path_to_save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a62338-8ccf-448f-bbf5-bf254e530254",
   "metadata": {},
   "source": [
    "Это разделит данные на подкаталоги в зависимости от уникальных значений в столбце category_column.\n",
    "\n",
    "**Использование repartition с col:**  \n",
    "Если вы хотите репартиционировать DataFrame на основе значений в столбце category_column, можно сделать так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f614a256-e293-47c5-88f7-260829c8d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 10  # Желаемое количество партиций\n",
    "\n",
    "# Репартиционирование DataFrame на основе значения столбца\n",
    "df_repartitioned = df.repartition(num_partitions, col(\"category_column\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc3491c-f4e4-4847-887b-e8aeaa114a01",
   "metadata": {},
   "source": [
    "В этом случае данные будут распределены по партициям на основе значений в столбце category_column. Это может быть полезно, если операции часто фильтруются или группируются по этому столбцу.\n",
    "\n",
    "Выбор числа партиций (num_partitions) зависит от объема данных и характеристик кластера. Чем больше уникальных значений в столбце category_column, тем больше партиций может потребоваться для равномерного распределения данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d16ee-4136-4c3f-930d-a88f1026a955",
   "metadata": {},
   "source": [
    "В PySpark вы можете выполнять репартицию как по одному столбцу, так и по комбинации столбцов. Вот как это можно сделать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb6106-38dd-41fa-a848-0bb6b2494863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Репартиционирование по комбинации столбцов\n",
    "df_repartitioned = df.repartition(\"category_column1\", \"category_column2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b700874-8f15-48db-84ab-aeb420949b0e",
   "metadata": {},
   "source": [
    "В этом случае данные будут распределены по партициям на основе уникальных комбинаций значений в столбцах category_column1 и category_column2.  \n",
    "\n",
    "**Использование col функции для репартиции**  \n",
    "Если вам нужно выполнить репартицию на основе значений, вычисленных на основе выражений, вы можете использовать функцию `col` для создания вычисляемого столбца:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46912d08-f9a3-4358-be79-2074c22869bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример репартиции на основе комбинации столбцов с использованием col\n",
    "df_repartitioned = df.repartition(10, col(\"category_column1\") % 5, col(\"category_column2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da566732-d94d-415e-90b5-9f08e76db09f",
   "metadata": {},
   "source": [
    "Этот пример показывает репартицию на основе комбинации category_column1 и category_column2, где выражение col(\"category_column1\") % 5 используется для вычисления номера партиции по модулю 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f318b-42ac-4e6f-829f-0dd8a2d38203",
   "metadata": {},
   "source": [
    "### Учет несбалансированности данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce091173-f371-400a-9538-eeca3269f864",
   "metadata": {},
   "source": [
    "Партицируем по столбцу `area` в котором есть 77 значений. Но количество строк несбалансировано по этим значениям.  \n",
    "\n",
    "Поскольку количество строк для каждого значения area не сбалансировано, репартиционирование по этому столбцу может привести к тому, что некоторые партиции будут значительно больше, чем другие. Это может повлиять на равномерность обработки данных и распределение нагрузки на кластере.\n",
    "\n",
    "Если вам важно, чтобы данные были равномерно распределены по партициям, можно попробовать следующий подход:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f474a561-fe48-4a22-895c-c1feb4dac55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем количество партиций (можно выбрать число, основываясь на размере кластера и объеме данных)\n",
    "num_partitions = 100\n",
    "\n",
    "# Репартиционирование по столбцу 'area'\n",
    "df_repartitioned = df.repartition(num_partitions, col(\"area\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c76d1a-dcab-4c30-b749-b3f39a7c5560",
   "metadata": {},
   "source": [
    "При предположении, что значение 'area' = 1 содержит 10% строк с дисбалансом, строки для 'area' = 1 будут разнесены по разным партициям, потому что каждое уникальное значение столбца 'area' в данном случае будет связано с определенной партицией на основе хэш-функции.\n",
    "\n",
    "Конкретно, если для 'area' = 1 существует значительное количество строк, они будут распределены по нескольким партициям из 100 доступных. Это происходит потому, что репартиционирование по хэш-функции столбца 'area' пытается равномерно распределить данные по указанному числу партиций (в данном случае 100).\n",
    "\n",
    "Примерно 10% строк, соответствующих 'area' = 1, будут разнесены по 10% от общего числа партиций (т.е. примерно 10 партиций из 100). Это помогает улучшить параллелизм обработки данных при выполнении операций, таких как соединения и агрегации.\n",
    "\n",
    "Таким образом, хотя строки с 'area' = 1 будут распределены по нескольким партициям, они останутся локализованными в тех партициях, которые определяет хэш-функция для данного уникального значения 'area'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192626e3-6ad7-4889-a958-3704289674ea",
   "metadata": {},
   "source": [
    "После выполнения репартиционирования важно провести тестирование производительности. Вы можете оценить, как распределены данные по партициям, и измерить эффективность операций, таких как фильтрация, соединения и агрегации. Если вы замечаете, что некоторые партиции слишком большие или маленькие, можно будет попробовать изменить количество партиций или метод репартиционирования."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598dcd47-5c33-4cbf-829b-4970fc7f2b98",
   "metadata": {},
   "source": [
    "### Подходы к партицированию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8365937-995a-44b4-9b70-b07e05cc77f9",
   "metadata": {},
   "source": [
    "#### Исходное количество партиций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f1150-18a7-47d2-b775-6a1882e0dfc2",
   "metadata": {},
   "source": [
    "При чтении данных из источника, такого как файлы или базы данных, PySpark автоматически определяет количество партиций на основе параметров по умолчанию или конфигурационных настроек. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52a01c-6275-4df3-a112-b44372602a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение данных из файла с использованием параметра numPartitions\n",
    "df = spark.read.parquet(\"path_to_file\", numPartitions=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488eba2-11df-43ee-a0eb-27b5dcbcd060",
   "metadata": {},
   "source": [
    "В этом примере numPartitions=100 указывает PySpark использовать 100 партиций при чтении данных из файла. Однако фактическое количество партиций может зависеть от различных факторов, таких как размер данных и настройки кластера."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9e65d-b676-4fa3-8f43-bb0e25fbc572",
   "metadata": {},
   "source": [
    "#### Репартиционирование на основе размера данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829ca5f-78ec-4601-b416-82ac50440a1e",
   "metadata": {},
   "source": [
    "Если у вас уже есть DataFrame и вы хотите управлять количеством партиций вручную, можно использовать метод `.repartition()` с указанием желаемого числа партиций:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332c111-0d25-44ad-b20c-caac660e9d75",
   "metadata": {},
   "source": [
    "df_repartitioned = df.repartition(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932ee76-0b0a-470a-aab3-198a20a12246",
   "metadata": {},
   "source": [
    "В этом случае DataFrame будет репартиционирован на 100 партиций. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317f2b4-6bfb-4ef2-83ed-8082f4c621bf",
   "metadata": {},
   "source": [
    "#### Репартиционирование на основе размера кластера и ресурсов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afac23-8ddf-4750-890b-f7bfcaf1b24d",
   "metadata": {},
   "source": [
    "Определение количества партиций также может зависеть от доступных ресурсов кластера. Например, если у вас есть большой кластер с множеством ядер и вы хотите максимально использовать параллелизм, можно репартиционировать DataFrame на большее количество партиций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e0384-2260-4311-8062-d2a8c6fff258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение количества ядер в кластере\n",
    "num_cores = spark.sparkContext.defaultParallelism\n",
    "\n",
    "# Репартиционирование на основе числа ядер\n",
    "df_repartitioned = df.repartition(num_cores * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47925447-969f-424c-aab0-0156e25e21f6",
   "metadata": {},
   "source": [
    "Это увеличит число партиций в зависимости от числа доступных ядер в кластере, что поможет максимизировать параллелизм и улучшить производительность операций."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0802da-f4e6-4207-b807-646c9beb105d",
   "metadata": {},
   "source": [
    "#### Репартиционирование на основе характеристик данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e076885-138f-4fc9-8f97-6dc135735a73",
   "metadata": {},
   "source": [
    "Если данные имеют ключевые столбцы для фильтрации или соединения, вы можете репартиционировать DataFrame на основе этих столбцов, чтобы улучшить производительность запросов. Например, репартиционирование на основе значений ключевого столбца может помочь сократить перемещение данных между узлами кластера при выполнении операций соединения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b98f78-90db-44be-8503-9af0ec315e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Репартиционирование на основе ключевого столбца\n",
    "df_repartitioned = df.repartition(100, col(\"key_column\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881517a-8643-4eab-bacb-4861ed53ce1b",
   "metadata": {},
   "source": [
    "Это распределит данные по 100 партициям на основе уникальных значений в столбце key_column, что может улучшить производительность операций, зависящих от этого столбца."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e05982-e960-4ce8-9e53-d640f3f26967",
   "metadata": {},
   "source": [
    "### Пример расчета партиций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b633ac-fd82-4cdd-b00a-38be406678cb",
   "metadata": {},
   "source": [
    "Допустим кластер на локальной машине с 4-мя ядрами. Датасет 6.5М строк.  \n",
    "\n",
    "**Использование размера блока данных** \n",
    "\n",
    "В Spark размер блока данных (block size) по умолчанию равен 128 МБ. Оптимальное количество партиций может быть рассчитано на основе размера вашего датасета и размера блока данных. Для этого можно использовать следующую формулу:  \n",
    "\n",
    "`num_partitions=⌈(block_size/dataset_size​)×parallelism_factor⌉`\n",
    "\n",
    "Где:\n",
    "\n",
    "- dataset_size — размер вашего датасета в байтах.\n",
    "- block_size — размер блока данных, по умолчанию 128 МБ (указывается в байтах).\n",
    "- parallelism_factor — коэффициент параллелизма, обычно равен количеству ядер.\n",
    "\n",
    "Например, если размер вашего датасета составляет 6.5 миллионов строк и вы используете блок данных по умолчанию в 128 МБ (134'217'728 байт), то:\n",
    "\n",
    "`dataset_size=6.5×10^6×average_row_size_in_bytes`\n",
    "\n",
    "При этом `average_row_size_in_bytes` — средний размер строки в байтах.\n",
    "\n",
    "**Примерная оценка:**  \n",
    "Давайте примем следующие допущения для расчета:\n",
    "\n",
    "- Средний размер строки в вашем датасете составляет, например, 200 байт.\n",
    "- Размер блока данных (block size) равен 128 МБ (134'217'728 байт).\n",
    "- Ваш датасет содержит 6.5 миллионов строк.\n",
    "\n",
    "Тогда размер вашего датасета будет приблизительно:\n",
    "`dataset_size=6.5×10^6×200=1.3×10^9` байт  \n",
    "\n",
    "`num_partitions=⌈(1.3×10^9/134'217'728)×4⌉`\n",
    "\n",
    "`num_partitions=⌈9.7⌉≈10`\n",
    "\n",
    "Таким образом, начальное количество партиций можно выбрать около 10 и провести дальнейшее тестирование производительности для определения наилучшего значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982605ad-73ed-47c1-852e-38086efa3e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
